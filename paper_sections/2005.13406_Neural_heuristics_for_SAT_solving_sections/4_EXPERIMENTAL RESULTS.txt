Dataset and training details. To train and evaluate the models we use a class of SAT problems \(SR(n)\) introduced and described in detail in [SLB \(^{+18}\) ]. It is parametrized only by \(n\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \(SR(n)\) samples has two labels (see Section 3): sat indicating whether the formula \(\Phi\) is satisfiable and policy indicating for each literal \(l\) whether \(\Phi \wedge l\) is satisfiable. We generate each of those numbers by running MiniSat 2.2 [ES03]. Sample random \(SR(30)\) formulas are solved by MiniSAT 2.2 in 0.007 seconds, while \(SR(110)\) takes 0.137 second and \(SR(150)\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism.  

Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2.   

<table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178±0.672</td><td>0.084±0.004</td><td>0.050±0.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024±0.555</td><td>0.233±0.017</td><td>0.105±0.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010±0.482</td><td>0.266±0.033</td><td>0.110±0.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227±0.127</td><td>0.319±0.007</td><td>0.123±0.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table>  

Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \(SR(n)\) samples. JW- OS proved to be the best on average classes of problems: \(SR(50)\) and \(SR(70)\) , whereas neural guidance- based algorithms proved to be the best on large problems: \(SR(90)\) and \(SR(110)\) .  

<center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \(x\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \(SR(x)\) formulas. The \(y\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center>  

Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \(SR(50)\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \(0.3^{2}\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).
<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center>  

Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \(SR(30)\) , level 20 attention degraded the model performance. For \(SR(50)\) , level 40 the metrics with and without attention stayed within the standard deviation of each other.  

Reproducibility. For each set of hyperparameters (e.g. \(SR(30)\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on TensorFlow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access MiniSat through PySAT interface [IMM18].  

<center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center>  

5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.