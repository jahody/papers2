We use a message- passing graph- based neural network architecture similar to NeuroSAT introduced in [SLB\(^+\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula.  

<center>Figure 2: Left: A graph representation of formula \((A \vee \neg C \vee B) \wedge (\neg B \vee C)\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \((CCL)\) , connection between literals and their negations \((CLL)\) , literal embeddings from previous iteration \((LE_{t-1})\) , and clause embeddings from previous iteration \((CE_{t-1})\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center>  

We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \(V\) (and a vector \(K\) if needed) based on its embedding, to every connected node. \(V\) and \(K\) are generated with a three- layer MLP with LeakyReLU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with LeakyReLU activation after each hidden layer and sigmoid activation after the last layer.  

We explore two different aggregation methods. The first is the average of received \(V\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \(V\) , we send two vectors, \(V\) and \(K\) . Receiving node generates one vector \(Q\) based on its embedding, and the result of aggregation is \(\sum_{i} V_{i} \text{ sigmoid } (K_{i} \cdot Q)\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \(K\) and \(Q\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\(^+\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.
Like NeuroSAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration.