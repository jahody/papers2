[SLB+18] proposed the NeuroSAT message- passing network architecture for SAT- solving that generates the assignment of variables directly in the graph. This is an important inspiration for our work, although in contrast to [SLB+18] we use a message- passing network for guidance of a backtracking- based algorithm instead. A similar graph representation, but more general in order to accommodate for higher- order logic is used in FormulaNet presented in [TWTD17]. To the best of our knowledge the FormulaNet architecture was never used for neural guidance. In [SLB+18, WTWD17] formulas are represented as graphs and a general approach to neural networks and graphs, including the attention mechanism, can be found in [BHB+18, VCC+17]. The PossibleWorldNet architecture described in [ESA+18] is based on the TreeNN architecture, with an additional idea of checking multiple possible worlds. We consider the exploration of possible worlds as an alternative to structured backtracking- based search algorithms like DPLL and CDCL. It is worth noting that PossibleWorldNet could be modified to use a message- passing architecture while keeping the exploration of possible worlds. Another application of TreeNN for proof synthesis in propositional logic was proposed in [SS18]. EqNet [ACKS16] solves a more
general problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \(a \wedge \neg a\) ). However, the formulas solved by EqNet have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\(^+\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\(^+\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers.