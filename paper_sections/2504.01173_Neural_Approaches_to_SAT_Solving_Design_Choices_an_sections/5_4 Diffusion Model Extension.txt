As we mentioned in Section 3.4.4, one can use the GNN as a function approximator \(f_{\theta}(\cdot)\) inside a diffusion model. This enables another way of scaling the test- time compute. We adapt the diffusion model used by Sun et al. Sun and Yang [2023] where the function approximator is trained to predict the ground truth solution \(\mathbf{x_0} = f_{\theta}(\mathbf{x_t},t)\) conditioned on a sample \(\mathbf{x_t}\) at time \(t\) . The predicted assignment is then used to obtain a sample at time \(t - 1\) and this process is repeated again \(\mathbf{x_0} = f_{\theta}(\mathbf{x_{t - 1}},t - 1)\) until we reach \(t = 0\) . One application of the function approximator together with the sampling is called a diffusion step. The number of diffusion steps \(T\) used for inference is a parameter which can be adapted after the model was already
Table 4: Performance of a model trained on SR40 (VCG+RNN with closest assignment supervision) when tested across various benchmarks with a maximum of 100 message-passing iterations and early stopping. The model maintains reasonable performance on SR100 ( \(74.2\%\) decision accuracy) but degrades on larger instances. "UNSAT Instances (gap \(= =1\) )" shows the percentage of UNSAT instances where the model achieved a gap of 1, which is always optimal for SR datasets but not always achievable for 3SAT instances. "Steps" columns indicate average/median iterations required to reach solutions, demonstrating the model's efficiency.   

<table><tr><td>Dataset</td><td>Decision Accuracy</td><td>SAT Instances Solved</td><td>UNSAT Instances (gap == 1)</td><td>SAT Steps (Avg/Med)</td><td>UNSAT Steps (Avg/Med)</td></tr><tr><td>SR40</td><td>89.5%</td><td>79.0%</td><td>95.6%</td><td>16.17/13.0</td><td>13.33/10.0</td></tr><tr><td>SR100</td><td>74.2%</td><td>48.3%</td><td>91.3%</td><td>24.93/21.0</td><td>23.47/19.0</td></tr><tr><td>SR200</td><td>58.5%</td><td>17.0%</td><td>64.5%</td><td>32.44/30.0</td><td>33.98/28.0</td></tr><tr><td>SR400</td><td>51.5%</td><td>3.0%</td><td>16.0%</td><td>41.33/34.0</td><td>52.06/44.5</td></tr><tr><td>3SAT100</td><td>74.8%</td><td>52.8%</td><td>64.0%</td><td>25.63/22.0</td><td>29.66/24.0</td></tr><tr><td>3SAT200</td><td>54.0%</td><td>17.1%</td><td>22.5%</td><td>33.21/25.0</td><td>35.10/31.5</td></tr></table>  

Table 5: Performance of a model trained on SR100 (VCG+RNN with closest assignment supervision) when tested on SR benchmarks with a maximum of 100 message-passing iterations and early stopping. Given that the SR40-trained model achieved only \(3\%\) SAT accuracy on SR400 (see Table 4), we focused on evaluating how training on larger instances improves scaling. The results show dramatic improvement on larger benchmarks ( \(36.5\%\) vs \(3\%\) on SR400), demonstrating that training on larger problems significantly enhances generalization capacity. The "Steps" metrics confirm the SR100-trained model requires fewer iterations on larger problems (e.g., 30.68 vs 41.33 average iterations for SAT instances on SR400).   

<table><tr><td>Dataset</td><td>Decision Accuracy</td><td>SAT Instances Solved</td><td>UNSAT Instances (gap == 1)</td><td>SAT Steps (Avg/Med)</td><td>UNSAT Steps (Avg/Med)</td></tr><tr><td>SR40</td><td>90.6%</td><td>81.2%</td><td>90.0%</td><td>14.57/12.0</td><td>16.21/12.0</td></tr><tr><td>SR100</td><td>79.3%</td><td>58.7%</td><td>85.7%</td><td>22.16/18.0</td><td>22.64/19.0</td></tr><tr><td>SR200</td><td>83.0%</td><td>68.2%</td><td>60.2%</td><td>22.29/20.0</td><td>27.12/22.0</td></tr><tr><td>SR400</td><td>68.2%</td><td>36.5%</td><td>78.0%</td><td>30.68/26.0</td><td>33.13/28.0</td></tr></table>
<center>Figure 2: Percentage of SAT instances solved as message passing iterations increase for a model trained on SR40 with SAT+UNSAT closest assignment supervision. Left: Performance on SR100, showing rapid initial improvement. Right: Comparison across benchmarks, demonstrating effectiveness decreases with problem size but benefits from additional iterations, highlighting the recurrent architecture's inference-time scaling capability. </center>  

<center>Figure 3: Average gap (unsatisfied clauses) reduction with increasing message passing iterations for a model trained on SR40 with SAT+UNSAT closest assignment supervision. Left: Comparison across benchmarks showing extremely rapid gap reduction in early iterations for all problem sizes, with all benchmarks achieving remarkably low average gaps despite varying SAT-solving performance. Right: Individual instance trajectories revealing different convergence patterns between SAT and UNSAT instances, with occasional fluctuations suggesting potential benefit from monitoring solution quality during inference. </center>  

trained and therefore, in this setting we have two types of iterations. One is the number of message- passing iterations and the second is the number of diffusion steps. In Table 6 we report the tradeoff between the number of message- passing steps (referred to as GNN_Steps) and the number of diffusion steps. The reported numbers correspond to the dataset SR100 with 100 variables in each problem. The model was trained on the SR40 distribution and the tested combinations use around 300 iterations in total distributed between the two types of steps.  

The experiments revealed a consistent trend: increasing the number of message- passing steps is generally more important for improving metrics such as Accuracy and Avg. Gap.