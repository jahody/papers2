There are several obvious supervision objectives and prediction tasks which can be used to train the model. The original NeuroSAT model was trained to predict the satisfiability status of a given formula using binary cross- entropy. Later, several authors tried different training tasks and objectives which have been summarized in a review paper by Li et al. Li et al. [2023]. We reimplement these objective and task for our setup and also introduce a novel training objective which in certain settings results in significant improvements of the model performance. These objective are briefly described below.  

Satisfiability Classification This is the task which was used by Selsam et al. [2018] for training the original NeuroSAT architecture. The model is trained to predict whether the formula is satisfiable or not through graph- level embedding aggregation using global mean pooling. The loss is computed by binary cross- entropy between the prediction \(\hat{y}\) and ground truth \(y\in \{0,1\}\) .. \(\mathcal{L}_{\mathrm{sat}} = -(y\log \hat{y} +(1 - y)\log (1 - \hat{y}))\)  

Unsupervised Training For unsupervised training, we define the loss using clause validity Ozolins et al. [2022], where \(\hat{x}_{i}\) represents the model's predicted continuous probability of a variable being true:  

\[V_{c}(\hat{x}) = 1 - \prod_{i\in c^{+}}(1 - \hat{x}_{i})\prod_{i\in c^{-}}\hat{x}_{i},\quad \mathcal{L}_{\phi}(\hat{x}) = -\sum_{c\in \phi}\log (V_{c}(\hat{x})), \quad (12)\]  

where \(c^{+}\) and \(c^{- }\) are the sets of variables that occur in clause \(c\) in positive and negative form respectively. This loss reaches its minimum only when the prediction \(\hat{x}\) is a satisfying assignment. We note that alternative unsupervised formulations exist Amizadeh et al. [2018], and comprehensive evaluations reported by Li et al. Li et al. [2023] suggest that these two different
approaches perform similarly in practice. Another training option would be to directly optimize a convex loss function derived from SDP relaxation, but this approach is limited because SDP formulations work well for MAX- 2- SAT and can be extended to MAX- 3- SAT, but become increasingly difficult to formulate for general MaxSAT problems with larger clauses.  

Assignment Prediction For satisfiable formulas, we can train the model to predict the satisfiable variable assignments directly. We tried to use either mean squared error or cross- entropy loss between the predicted assignments and the ground truth assignments: \(\mathcal{L}_{\mathrm{assign}}^{\mathrm{MSE}} =\) \(\| \hat{a} - x\|_{2}^{2}\) and \(\mathcal{L}_{\mathrm{assign}}^{\mathrm{CE}} = - \sum_{i}x_{i}\log \hat{x}_{i} + (1 - x_{i})\log (1 - \hat{x}_{i})\) where \(x\) is the ground truth assignment and \(\hat{a},\hat{x}\) are the predicted assignments which differ by application of softmax (i.e. \(\hat{a}\) are just logits without a softmax applied).  

Closest Assignment Training One problem with assignment prediction is that satisfiable formulas can have a lot of solution and the network is penalized even if it predicts satisfiable solution which differs from the one which is used as a ground truth. We therefore introduce a novel supervision method which uses a MaxSAT solver to always compute the solution which is closest to the solution predicted by the model. We then update then model with respect to this solution. In Section 5.2, we show that this method works particularly well when the solution space is large.  

For each formula in a batch, a valid assignments that minimize the Hamming distance to the model's current predictions is found by the RC2 MaxSAT solver. For satisfiable formulas it finds an assignment that satisfies all clauses while being closest to current prediction. For unsatisfiable formulas, it finds an assignment that maximizes the number of satisfied clauses while minimizing distance to prediction.  

This approach allows the model to explore different regions of the solution space while maintaining valid solutions for SAT instances or optimal partial solutions for UNSAT instances. The supervision signal adapts to the model's current state rather than forcing it toward a single pre- determined assignment. The disadvantage of this method is that the computation of the loss is slower then with the precomputed solution. This could be solved by pre- computing solutions or by using an approximate MaxSAT solver.  

SAT- Only Instance Filtering After initially training with both satisfiable and unsatisfiable instances, we experimented with formula- type specialization by restricting training to only satisfiable instances. In Table 3, we show that this filtering can lead to higher accuracy of the trained model.