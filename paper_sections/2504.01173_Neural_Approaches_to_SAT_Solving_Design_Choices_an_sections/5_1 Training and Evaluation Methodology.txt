For training, we generate 50,000 instances: 25,000 pairs for SR and 50,000 instances for 3- SAT. We annotate each dataset by the maximum number of variables appearing in the training formulas. For SR, we test two variations, SR40 for which the training examples are sampled with 3- 40 variables and SR100 for which the training examples contain 10- 100 variables. For 3- SAT, the training samples contain 10- 100 variables (3SAT100). The SR dataset is well suited for training SAT/UNSAT prediction models due to its design that prevents learning from superficial features, making it harder for models to exploit statistical shortcuts rather than learning true logical reasoning. We also create versions of training data which contain only satisfiable instances (denoted SAT only). The size of these datasets is half of the original datasets (i.e. 25000 examples). To evaluate generalization, we validate exclusively on problems with exactly the maximum number of variables in each category, therefore SR40 for evaluation means that the problems have always exactly 40 variables (not a range of 3- 40), SR100 test contains only problems with exactly 100 variables, and so on.1  

Table 1 summarizes the key statistics of our evaluation datasets.  

Table 1: Statistics of benchmark test sets. SAT% indicates the percentage of satisfiable instances in each dataset. Avg. Gap represents the average number of unsatisfied clauses when using random variable assignments. SAT Gap and UNSAT Gap show this metric separated by instance satisfiability. SR datasets are generated using the SR generator with the indicated number of variables (e.g., SR40 contains instances with 40 variables), while 3SAT datasets contain instances near the phase transition point with the specified number of variables. All datasets maintain a balanced distribution of satisfiable and unsatisfiable instances.  

<table><tr><td>Dataset</td><td>SAT%</td><td>Avg. Gap</td><td>SAT Gap</td><td>UNSAT Gap</td><td>Avg. Clauses</td></tr><tr><td>SR40</td><td>50.0%</td><td>21.29</td><td>21.59</td><td>20.99</td><td>228.40</td></tr><tr><td>SR100</td><td>50.0%</td><td>51.31</td><td>50.64</td><td>51.98</td><td>547.49</td></tr><tr><td>SR200</td><td>50.0%</td><td>100.31</td><td>101.03</td><td>99.59</td><td>1083.81</td></tr><tr><td>SR400</td><td>50.0%</td><td>198.74</td><td>198.53</td><td>198.95</td><td>2152.32</td></tr><tr><td>3SAT100</td><td>53.5%</td><td>52.78</td><td>53.00</td><td>52.54</td><td>426.00</td></tr><tr><td>3SAT200</td><td>55.5%</td><td>107.65</td><td>107.45</td><td>107.90</td><td>852.00</td></tr></table>  

The Gap metric represents the average number of unsatisfied clauses when using random
variable assignments. This metric has the same definition for both SAT and UNSAT instances; it simply counts how many clauses remain unsatisfied with random assignments on average. Larger gaps indicate more challenging problems where random guessing performs poorly.