Table 2 presents a comprehensive comparison of different architectural configurations trained exclusively on the SR40 dataset. This comparison includes different graph representations (LiteralClause Graph vs. Variable- Clause Graph), update functions (RNN vs. LSTM), and supervision approaches (SAT/UNSAT classification, assignment supervision, and unsupervised objective training), all evaluated on instances with 40 variables. All models were evaluated using Exponential Moving Average (EMA) of parameters during validation only, as detailed in A.1.2, which helps reduce fluctuations in validation metrics and provide more reliable model selection. Importantly, curriculum learning ( A.1.1) was employed only for training models with SAT/UNSAT classification objectives, as it proved unnecessary for models trained with assignment prediction or unsupervised learning approaches.  

Graph Representation Impact: Our results demonstrate that Literal- Clause Graph (LCG) and Variable- Clause Graph (VCG) representations exhibit different strengths. VCG shows better performance for assignment- based training with RNN updates, achieving a SAT accuracy of 68.8% compared to 48.6% for LCG. Additionally, VCG's more compact representation (using one node per variable rather than two for positive and negative literals) provides computational advantages for larger formulas, making it our preferred choice for scaling to more complex problems.
Message Passing Mechanism: While LSTM- based message passing shows advantages in some configurations, particularly for unsupervised training, we found that RNN- based approaches offer a better balance of performance and interpretability for assignment- based training. RNN updates with VCG representation achieved higher results for finding satisfying assignments, with \(68.8\%\) SAT accuracy and \(84.4\%\) decision accuracy. The simpler RNN structure also facilitates better analysis of the model's internal reasoning process. However, we found training RNN- based models for SAT/UNSAT classification particularly challenging, with LSTM being more stable for this specific task.  

Supervision Approach: Our experiments reveal distinct advantages for different supervision approaches:  

1. Assignment-based supervision shows better performance for finding satisfying assignments, especially with VCG+RNN configuration (68.8% SAT accuracy, 84.4% decision accuracy).  

2. Unsupervised learning achieves the lowest average gaps across configurations (as low as 0.91 for VCG+RNN and 0.84 for VCG+LSTM). This makes unsupervised training useful for applications where minimizing unsatisfied clauses is the priority.  

3. SAT/UNSAT classification training, while challenging with RNN, enables an interesting property: models trained only for classification develop an implicit ability to separate embeddings for positive and negative literals. This separation allows for retrieving satisfying assignments through clustering techniques, despite the model not being explicitly trained for assignment prediction.  

Based on the results reported in Table 2, we identify the VCG+RNN+Assignment configuration as our most effective approach, offering a good balance between assignment accuracy and computational efficiency. This configuration forms the foundation for our further experiments and analysis in subsequent sections.  

Assignment Training Refinements: Table 3 highlights the impact of a novel training method we introduce, here called "closest assignment", with the VCG+RNN configuration across multiple datasets. This method computes assignments that minimize Hamming distance to the model's current predictions, showing improvements over training with precalculated assignments, especially for formulas with more variables. For SR100, using the closest assignment approach reduces the average gap from 3.81 to 1.43 for SAT+UNSAT training and improves SAT accuracy from 44.8% to 53.2%.  

This improvement correlates with the number of possible solutions in the benchmarks (SR10- 100 has a median of 16 solutions per formula compared to SR3- 40's median of 7), supporting our hypothesis that for formulas with larger solution spaces, guiding the model with dynamically selected assignments that align with its current predictions yields better generalization than using fixed predetermined assignments.  

The computational challenges of calculating closest assignments during training are noteworthy, particularly for larger benchmarks like 3SAT+UNSAT, where this approach became impractical and we therefore omit this experiment and leave the last row of Table 3 empty. It also highlights an opportunity for future work on more efficient approximation methods for finding near- optimal assignments.  

Training Data Composition: Our results also indicate that training exclusively on SAT instances (SAT only) improves performance for finding satisfying assignments. For SR40, this approach with closest assignment training achieves our highest SAT accuracy of 76% and decision accuracy of 88%. However, models trained on both SAT and UNSAT instances (SAT+UNSAT)
with closest assignment supervision demonstrate better gap minimization, achieving an average gap of 0.98 versus 2.68 for SAT- only training on SR40.

Table 2: Performance comparison of GNN architectures for SAT solving on the SR40 dataset.The table compares Literal-Clause Graph (LCG) and Variable-Clause Graph (VCG) repre-sentations, RNN and LSTM update mechanisms, and different training objectives. Metrics include average gap (number of unsatisfied clauses) across all instances and separated by sat-isfiability status (lower is better), SAT accuracy (percentage of satisfiable instances solved by finding assignment), and decision accuracy (percentage of correct satisfiability predictions). No-table findings include: unsupervised training consistently achieves lowest gaps; VCG+RNN with assignment prediction shows highest SAT accuracy (68.8%); and RNN-based models with SAT/UNSAT classification proved challenging to train effectively (indicated by dashes). As-terisks (*) indicate results obtained through clustering of node embeddings rather than direct prediction. This model combination was particularly hard to train in our setup. We found that both for VCG and LCG RNN is very sensitive to hyper-parameter selection. As the model failed to get generalized in our final unified experimental setup we do not include this result (close to random performance) now.

<table><tr><td>Graph</td><td>Update</td><td>Loss Function</td><td>Avg. Gap↓</td><td>Gap on SAT↓</td><td>Gap on UNSAT↓</td><td>SAT Acc.↑</td><td>Dec. Acc.↑</td></tr><tr><td rowspan="6">LCG</td><td rowspan="3">RNN</td><td>SAT/UNSAT</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Assignment</td><td>1.83</td><td>1.25</td><td>2.41</td><td>48.6%</td><td>72.8%</td></tr><tr><td>Unsup</td><td>0.93</td><td>0.59</td><td>1.26</td><td>51.4%</td><td>75.7%</td></tr><tr><td rowspan="3">LSTM</td><td>SAT/UNSAT</td><td>1.96*</td><td>1.27*</td><td>2.62*</td><td>59.2%</td><td>83.9/79.6%</td></tr><tr><td>Assignment</td><td>1.82</td><td>1.06</td><td>2.58</td><td>56.8%</td><td>78.4%</td></tr><tr><td>Unsup</td><td>0.81</td><td>0.45</td><td>1.16</td><td>62%</td><td>81%</td></tr><tr><td rowspan="6">VCG</td><td rowspan="3">RNN</td><td>SAT/UNSAT</td><td>3.62*</td><td>1.9*</td><td>5.34*</td><td>56.6%</td><td>80/78.3%</td></tr><tr><td>Assignment</td><td>1.95</td><td>0.8</td><td>3.05</td><td>68.8%</td><td>84.4%</td></tr><tr><td>Unsup</td><td>0.91</td><td>0.58</td><td>1.23</td><td>51.6%</td><td>75.8%</td></tr><tr><td rowspan="3">LSTM</td><td>SAT/UNSAT</td><td>2.33*</td><td>1.57</td><td>3.08</td><td>52.2%</td><td>81.9/76.1%</td></tr><tr><td>Assignment</td><td>2.05</td><td>0.96</td><td>3.14</td><td>66.4%</td><td>83.2%</td></tr><tr><td>Unsup</td><td>0.84</td><td>0.51</td><td>1.17</td><td>56.4%</td><td>78.2%</td></tr></table>

# 5.3 Test-time Scaling

A key property of our recurrent GNN architecture for SAT solving is the ability to adjust computational effort at inference time. Unlike standard GNNs with fixed number of layers,the weight-shared recurrent design enables flexible scaling through additional iterations and resampling.