We also report an interesting finding which allows to simplify the function approximator used in the diffusion model. Notice that in the expression \(\mathbf{x}_0 = f_\theta (\mathbf{x}_t, t)\) it is also conditioned on the timestep \(t\) . This conditioning is dictated by the theory of diffusion models Nakkiran et al. [2024] and most of the models, including the one by Sun et al. Sun and Yang [2023] blindly follow this design choice. In our experiments, we found out that this conditioning is not needed and that the model sometimes works even better without it. Therefore, in all reported results, the
<center>Figure 4: Performance heatmaps for a model trained on SR40 with SAT+UNSAT closest assignment supervision, showing how metrics improve with both increased iterations (columns) and resampling attempts (rows). Testing on SR40 (top), SR100 (middle), and 3SAT100 (bottom) demonstrates significant gains from both scaling dimensionsâ€”e.g., SR40 decision accuracy improves from 84% (1 sample, 25 iterations) to 93% (5 samples, 125 iterations). This two-dimensional inference-time scaling capability is consistent across benchmarks but with decreasing returns on larger problems. </center>  

model is trained to predict the solution \(\mathbf{x_0}\) only from the sample at timestep \(t\) ( \(\mathbf{x_0} = f_{\theta}(\mathbf{x_t})\) ). Concurrently to us, this fact was also discovered by Sun et al. Sun et al. [2025] (the same surname is a coincidence) and it's possible that many of the reported experiments which blindly use this conditioning would result in better values without it.  

In this simplified setup, the training examples \((\mathbf{x_0},\mathbf{x_t})\) are sampled by taking a solution of a formula \((\mathbf{x_0})\) , sampling a random \(t\) from the diffusion schedule and obtaining a corrupted version of the solution at time \(t\) ( \(\mathbf{x_t}\) ). The model is trained to predict \(\mathbf{x_0}\) from \(\mathbf{x_t}\) . The GNN is the same as in the case of assignment prediction except that it also contains a learnable embedding layer which embeds the Boolean values in the assignment \(\mathbf{x_t}\) into a vector space to obtain the initial embeddings of variables (or literals) for the first pass of message- passing.  

The only difference from the model trained for assignment prediction is therefore that the initial embeddings are not sampled randomly but obtained by embedding the perturbed assignment \(\mathbf{x_t}\) . This also means that during test time, these two approaches differ only by rounding, i.e. running the model trained for assignment prediction for 100 steps and after every 20 steps rounding the variable embeddings to vectors representing True and False is same as running the diffusion model for 5 diffusion steps where each step has 20 message- passing iterations.
Table 6: Performance Metrics for Different GNN and Diffusion Step Configurations. Variable names shown in parentheses in the original data source are omitted here for brevity.   

<table><tr><td>GNN Steps</td><td>Diffusion Steps</td><td>Avg. Gap</td><td>Dec. Acc. (%)</td><td>Single-Step Avg. Gap</td><td>Single-Step Dec. Acc (%)</td></tr><tr><td>20</td><td>15</td><td>1.09</td><td>68.7</td><td>3.26</td><td>60.2</td></tr><tr><td>23</td><td>13</td><td>1.05</td><td>70.4</td><td>2.80</td><td>63.1</td></tr><tr><td>27</td><td>11</td><td>0.96</td><td>73.6</td><td>2.44</td><td>64.7</td></tr><tr><td>31</td><td>9</td><td>0.98</td><td>73.8</td><td>2.11</td><td>68.2</td></tr><tr><td>35</td><td>8</td><td>0.93</td><td>75.4</td><td>1.96</td><td>69.5</td></tr><tr><td>38</td><td>7</td><td>0.92</td><td>76.3</td><td>1.83</td><td>70.2</td></tr><tr><td>42</td><td>7</td><td>0.90</td><td>76.7</td><td>1.70</td><td>71.6</td></tr><tr><td>46</td><td>6</td><td>0.95</td><td>76.8</td><td>1.64</td><td>72.1</td></tr><tr><td>50</td><td>6</td><td>0.94</td><td>76.2</td><td>1.52</td><td>73.0</td></tr></table>