By tracking clause satisfaction across iterations, we observe that GNNs solve SAT problems through progressive local refinement. The gap (number of unsatisfied clauses) decreases following a trajectory typical of iterative optimization methods: rapid initial improvement followed by gradual refinement.  

This behavior supports the interpretation that GNNs implicitly learn to perform continuous optimization in a high- dimensional space similar to SDP relaxations for SAT. The effectiveness of additional message passing iterations during inference further strengthens this connection. A difference from the SDP relaxation is that the objective function which the GNN implicitely optimizes is non- convex because we observed that it can get stuck in local optima or converge to different solutions when initialized multiple times by different random embeddings.  

Figure 3 illustrates how the average gap decreases with increasing iterations. The trajectory suggests a rapid improvement phase followed by more gradual refinement. Individual instance trajectories reveal that while most instances show steady improvement toward optimal solutions, some exhibit fluctuations, particularly unsatisfiable instances. This observation supports the potential value of early stopping techniques, as in rare cases, the gap at later iterations might be higher than a previously achieved minimum gap.  

The bi- level optimization perspective—where message passing performs an inner optimization loop (finding variable assignments) guided by network parameters optimized at the outer level (during training)—helps explain the network's ability to generalize to novel problem instances and larger problems than those seen during training. In Section 7, we discuss more details about a possibility of manual derivation of the GNN equations from and explicit objective function.