We conducted a comprehensive evaluation that compares different architectural choices and supervision methods. Our evaluation focuses on five key performance metrics:  

- Average Gap: The average number of unsatisfied clauses across all test instances. Lower values indicate better performance, with 0 representing perfect satisfaction (i.e., no unsatisfied clauses) on satisfiable instances. For unsatisfiable instances, this metric reflects how close the model gets to minimizing unsatisfied clauses.- Gap on SAT: The average number of unsatisfied clauses computed only over satisfiable instances.- Gap on UNSAT: The average number of unsatisfied clauses computed only over unsatisfiable instances.- SAT Accuracy: The percentage of satisfiable instances for which the model correctly finds a satisfying assignment, computed only over satisfiable instances.- Decision Accuracy: The percentage of instances for which the model correctly predicts whether the formula is satisfiable. Since our approach does not formally refute unsatisfiable instances, we classify an instance as unsatisfiable when the model fails to find a satisfying assignment. This means unsatisfiable instances are always classified correctly under this assumption. This applies specifically in the case of assignment-based evaluation.