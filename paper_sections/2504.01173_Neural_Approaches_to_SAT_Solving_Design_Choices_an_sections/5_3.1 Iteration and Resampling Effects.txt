Figure 2 demonstrates how increasing message-passing iterations improves the percentage of solved SAT instances. Similarly, Figure 3 shows how the average gap decreases across iterations for various benchmarks. The heat maps in Figure 4 provide a comprehensive view of how performance metrics improve with both increased iterations and resampling attempts.

For the model trained on SR40, several observations are notable:

·Iteration benefits: Increasing iterations from 25 to 125 consistently improves all metrics across benchmarks.

·Resampling effects: Multiple inference attempts with different random initializations of node feature vectors further enhance performance. For SR40, decision accuracy improves from 84% with one sample to 93% with five samples at 125 iterations.
Table 3: Performance analysis of VCG+RNN with assignment prediction across different datasets and training methodologies. Our novel "Closest" supervision method (which dynamically selects assignments closest to current model predictions) consistently outperforms training with precalculated assignments. For SR40, SAT-only training with closest assignment supervision achieves the highest SAT accuracy (76%), while SAT+UNSAT training with closest assignment supervision yields the lowest average gap (0.98). The missing data for 3SAT100 with SAT+UNSAT closest supervision is due to prohibitive computational costs. Bold values indicate best results per dataset.  

<table><tr><td>Dataset</td><td>Training Mode</td><td>Assignment Type</td><td>Avg. Gap ↓</td><td>Gap on SAT ↓</td><td>Gap on UNSAT ↓</td><td>SAT Acc. ↑</td><td>Dec. Acc. ↑</td></tr><tr><td rowspan="4">SR40</td><td>SAT only</td><td>Precalculated</td><td>2.93</td><td>1.11</td><td>4.75</td><td>68.2 %</td><td>84.1 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>2.68</td><td>0.88</td><td>4.48</td><td>76 %</td><td>88 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>1.95</td><td>0.8</td><td>3.05</td><td>68.8 %</td><td>84.4 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>0.98</td><td>0.48</td><td>1.49</td><td>71.2 %</td><td>85.6 %</td></tr><tr><td rowspan="4">SR100</td><td>SAT only</td><td>Precalculated</td><td>4.42</td><td>2.36</td><td>6.48</td><td>47.4 %</td><td>73.7 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>3.57</td><td>1.67</td><td>5.48</td><td>59.6 %</td><td>79.8 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>3.81</td><td>2.34</td><td>5.28</td><td>44.8 %</td><td>72.4 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>1.43</td><td>0.92</td><td>1.94</td><td>53.2 %</td><td>76.6 %</td></tr><tr><td rowspan="4">3SAT100</td><td>SAT only</td><td>Precalculated</td><td>5.93</td><td>3.40</td><td>9.27</td><td>25.7 %</td><td>57.2 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>5.23</td><td>2.33</td><td>9.11</td><td>48.4 %</td><td>70 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>4.22</td><td>2.84</td><td>6.00</td><td>23.9 %</td><td>55.8 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></table>  

- Cross-distribution applicability: The model trained on SR40 maintains reasonable effectiveness on SR100 and 3SAT100, though with expected performance decrease. This aligns with findings from Li et al. Li et al. [2023], who demonstrated that models trained on SR distributions generally transfer well to other SAT problem structures.