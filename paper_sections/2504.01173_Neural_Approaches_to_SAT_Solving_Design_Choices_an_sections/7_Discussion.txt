In this section, we discuss the limitations of our work along with an outlook for future research. The primary limitation of the methods presented here is that they are not competitive with state- of- the- art SAT solvers on benchmarks derived from real- world problems. Current SAT solvers can handle formulas with millions of variables, which is not feasible for the GNN in its current form. However, as mentioned in the introduction, our motivation for studying these models is to better understand the reasoning capabilities of neural networks in a simplified context.  

The test- time scaling experiments clearly demonstrate that the GNNs can successfully generalize beyond their training distribution and do not merely learn superficial statistical patterns. The qualitative results presented in Section 6 further suggest that it is possible to fully understand the mechanisms by which the GNN solves a given formula. Figure 3 illustrates that the trained GNN functions as an implicit MaxSAT solver, incrementally maximizing the number
<center>Figure 5: Evolution of variable embeddings during message passing iterations for a satisfiable SR40 instance. The visualization shows 2D projections at different stages (Initial through Iteration 25), colored k-means algorithm in each iteration (green/red). Initially random, embeddings gradually organize into two distinct clusters often corresponding to optimal variable assignments. This clustering behavior was observed across different model architectures and training objectivesâ€”notably, even models trained solely for SAT/UNSAT classification (without explicit assignment supervision) develop this embedding separation. This phenomenon supports our interpretation that GNNs implicitly perform continuous optimization similar to SDP relaxation for SAT problems. </center>
of satisfied clauses at each step. These local updates occur in continuous space and can therefore be viewed as gradient updates with respect to an implicit objective function measuring clause satisfaction. Variables are also represented in a high- dimensional vector space, similar to semi- definite programming as explained in B.  

From this perspective, Equations 6, 7, and 11 can be interpreted as a gradient descent algorithm searching for an optimal assignment over a high- dimensional unit sphere (due to unit normalization), while the final classification layer corresponds to a rounding step to Boolean values. In future work, we aim to manually derive these equations from a trained GNN using a primal- dual approach, interleaving gradient updates of primal and dual variables associated with constraints. We believe that by utilizing suitable proximal operators and an appropriate metric in the relaxed solution space, the GNN can be effectively interpreted as a primal- dual algorithm optimizing a continuous relaxation of the MaxSAT objective in a high- dimensional space. This points out to another major advantage of using the RNN update function because its simple form is suitable for such derivation.  

Deriving equations for such algorithms applicable to arbitrary combinatorial optimization problems would be highly beneficial in practice, allowing these equations to be parameterized by learnable matrices and fine- tuned for specific problem distributions. Such data- driven solvers would be analogous to physics- informed neural networks Cai et al. [2021], where substantial domain knowledge is embedded within the model, followed by fine- tuning to approximate the dynamics of a particular physical system. This approach results in fast numerical solvers tailored to specific domains. We believe that the development of data- drive numerical solvers represents an exciting future direction for combinatorial optimization research. To make these numerical solvers practical, it will still be necessary to integrate them into more complex systems, where they would function as guessing or bounding heuristic.  

Another limitation of our work is that the model was tested exclusively on random problems. This decision is justified by the findings of Li et al. Li et al. [2023], who demonstrated that models trained on random problem instances exhibit superior generalization to other distributions. Since Li et al. already provided experimental results demonstrating the transferability of models across different problem distributions, we chose not to repeat those experiments here.