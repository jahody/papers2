Our research builds directly upon NeuroSAT Selsam et al. [2018], which introduced the first end- to- end neural approach for SAT solving using a recurrent message- passing architecture. While we maintain the core iterative design of NeuroSAT (allowing variable numbers of message- passing iterations through weight sharing), we explore simplified variants using RNNs and LSTMs and incorporate techniques like curriculum learning to improve training efficiency.  

Several other works have explored different directions in neural SAT solving. Li et al. Li et al. [2023] developed G4SATBench to benchmark various GNN architectures (GCN, GGNN, GIN) across different graph representations and supervision objectives. Unlike their broader exploration across architecture types, our work focuses on the recurrent message- passing paradigm from NeuroSAT and investigates how different training objectives and graph representations affect performance within this specific framework. We also mention the work by Warde et al. Warde- Farley et al. [2023] who developed a recurrent architecture based on a Restricted Boltzmann Machine.  

Hybrid approaches that integrate neural networks with traditional solvers include NeuroCore by Selsam and Bjorner Selsam and Bjorner [2019], which uses neural predictions to guide variable branching in CDCL solvers. Similarly, Wang et al. Wang et al. [2021] proposed NeuroComb to enhance CDCL solvers through GNN- based identification of important variables and clauses.
These approaches differ from our end- to- end model but demonstrate alternative applications of neural methods to SAT solving.  

The connection between neural networks and continuous relaxations is particularly relevant to our work. Kyrillidis et al. Kyrillidis et al. [2020] introduced FourierSAT, which transforms Boolean SAT problems into continuous optimization using the Walsh- Fourier transform. This approach provides a theoretical foundation for understanding how neural networks might implicitly convert discrete search problems into continuous optimization. Similar technique was introduced by Hosny et al. Hosny and Reda [2024] who develop GPU- accelerated approaches for MaxSAT problems. Hula et al. Hula et al. [2024] and Yau et al. Yau et al. [2024] explore the connection between GNNs and semidefinite programming relaxations, demonstrating empirically and theoretically that message- passing can implement gradient- based optimization of SDP relaxations.  

In the broader domain of combinatorial optimization, Sun et al. Sun and Yang [2023] used diffusion models based on GNNs to solver problems such as traveling salesman.