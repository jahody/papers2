Table 2: Conditional probability prediction performance (MAE) under varying condition counts and circuit sizes.   

<table><tr><td>#Condition Nodes</td><td>#Nodes=928</td><td>#Nodes=6428</td><td>#Nodes=17796</td></tr><tr><td>1</td><td>0.0243</td><td>0.0372</td><td>0.0457</td></tr><tr><td>2</td><td>0.0267</td><td>0.0401</td><td>0.0496</td></tr><tr><td>5</td><td>0.0314</td><td>0.0479</td><td>0.0532</td></tr></table>  

We train the model on small circuits and evaluate its conditional probability prediction accuracy on both small and large- scale benchmarks. In the single- condition setting, the average L1 loss on the validation set reaches as low as 0.0331. To illustrate generalizability, we select representative circuits of varying sizes. As shown in Table 2, the model maintains low prediction error across all scales, with L1 loss as low as 0.0243 for small circuits and remaining below 0.05 even for circuits with over 17K nodes. As the number of conditions increases from 1 to 5, prediction error rises slightly yet remains within acceptable bounds, demonstrating strong robustness. Importantly, inference overhead under multi- condition settings is negligible, thanks to the lightweight design of the aggregator functions of and gates.