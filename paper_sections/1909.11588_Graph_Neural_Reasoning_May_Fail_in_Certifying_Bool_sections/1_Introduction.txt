Logical reasoning problems span from simple propositional logic to complex predicate logic and high- order logic, with known theoretical complexities from NP- completeness [3] to semi- decidable and undecidable [2]. Testing the ability and limitation of machine learning tools on logical reasoning problems leads to a fundamental understanding of the boundary of learnability and robust AI, helping to address interesting questions in decision procedures in logic, program analysis, and verification as defined in the programming language community.  

There have been arrays of successes in learning propositional logic reasoning [1, 12], which focus on Boolean satisfiability (SAT) problems as defined below. A Boolean logic formula is an expression composed of Boolean constants ( \(\top\) : true, \(\bot\) : false), Boolean variables \((x_{i})\) , and propositional connectives such as \(\wedge\) , \(\vee\) , \(\neg\) (for example \((x_{1} \vee \neg x_{2}) \wedge (\neg x_{1} \vee x_{2})\) ). The SAT problem asks if a given Boolean formula can be satisfied (evaluated to \(\top\) ) by assigning proper Boolean values to the literal variables. A crucial feature of the logical reasoning domain (as is visible in the SAT problem) is that the inputs are often structural, where logical connections between entities (variables in SAT problems) are the key information.  

SAT and its variant problems are almost NP- complete or even more complicated in the complexity. The fact motivates the emergence of sub- optimal heuristic that trades off the solver performance to rapid reasoning. In terms of the fast inference process, deep learning models are favored as learnable heuristic solvers [1, 12, 16]. Among them Graph Neural Networks (GNNs) have grasped amount of attentions, since the message- passing process delivers the transparency to interpret the inference within GNNs, thus, revealing the black box behind neural logical reasoning in the failure instances.  

However, it should be noticed that logical decision procedures is more complex that just reading the formulas correctly. It is unclear if GNN embeddings (from simple message- passing) contain all the information needed to reason about complex logical questions on top of the graph structures
derived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP- complete problems [1, 12], whereas no evidences have been reported for solving semi- decidable predicate logic problems via GNN. The significant difficulty to prove the problems is the requirement of comprehensive reasoning over a search space, since a complete proof includes SAT and UNSAT (i.e., Boolean unsatisfiability).  

Perhaps disappointingly, this work presents some theoretical evidences that support a pessimistic conjecture: GNNs do not simulate the complete solver for UNSAT. Specifically, we discover that the neural reasoning procedure learned by GNNs does simulate the algorithms that may allow a CNF formula changing over iterations. Those complete SAT- solvers, e.g., DPLL and CDCL, are almost common in the operation that adaptively alters the original Boolean formula that eases the reasoning process. So GNNs do not learn to simulate their behaviors. Instead, we prove that by appropriately defining a specific structure of GNN that a parametrized GNN may learn, the local search heuristic in WalkSAT can be simulated by GNN. Towards these results, we believe that GNN can not solve UNSAT in existing logical reasoning problems.