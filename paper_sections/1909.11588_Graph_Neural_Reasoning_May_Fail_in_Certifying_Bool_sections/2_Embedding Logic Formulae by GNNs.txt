Preliminary: Graph Neural Networks (GNNs). GNNs refer to the neural architectures devised to learn the embeddings of nodes and graphs via message- passing. Resembling the generic definition in [14], they consist of two successive operators to propagate the messages and evolve the embeddings over iterations:  

\[m_{v}^{(k)} = \mathrm{Aggregate}^{(k)}\left(\{h_{u}^{(k - 1)}:u\in \mathcal{N}(v)\} \right),\quad h_{v}^{(k)} = \mathrm{Combine}^{(k)}\left(h_{v}^{(k - 1)},m_{v}^{(k)}\right) \quad (1)\]  

where \(h_{v}^{(k)}\) denotes the hidden state (embedding) of node \(v\) in the \(k^{t h}\) iteration, and \(\mathcal{N}(v)\) denotes the neighbors of node \(v\) . In each iteration, the Aggregate \((k)(\cdot)\) aggregates hidden states from node \(v\) 's neighbors \(\{h_{u}^{(k - 1)}:u\in \mathcal{N}(v)\}\) to produce the new message (i.e., \(m_{v}^{(k)}\) ) for node \(v\) . Combine \((k)(\cdot ,\cdot)\) updates the embedding of \(v\) in terms of its previous state and its current message. After a specific number of iterations (e.g., \(K\) in our discussion), the embeddings should capture the global relational information of the nodes, which can be fed into other neural network modules for specific tasks.  

Significant successes about GNNs have been witnessed in relational reasoning [6, 17, 20], where an instance could be departed into multiple objects then encoded by a series of features with their relation. It typically suits representation in Eq. 1. Whereas in logical reasoning, a Boolean formula is in Conjunctive Normal Form (CNF) that consists of literal and clause items. In term of the independence among literals in CNF (so do clauses), [12] embeds a formula into a bipartite graph, where the nodes denote the clauses and literals that are disjoint, respectively. In this principle, given a literal \(v\) as a node, all the nodes of clauses that contains the literal are routinely treated as \(v\) 's neighbors, vice and versa for the node of each clause. We assume \(\Phi\) is a logic formula in CNF, i.e., a set of clauses, and \(\Psi (v)\in \Phi\) denote one of clauses within the logic formula \(\Phi\) that contains literal \(v\) . Derived from Eq. 1, GNNs for logical reasoning can be further specified by  

\[\begin{array}{r l} & {m_{v}^{(k)} = \mathrm{Aggregate}_{L}^{(k)}\Big(\{h_{\Psi^{-1}}^{(k - 1)}:\Psi (v)\in \Phi \} \Big),\quad h_{v}^{(k)} = \mathrm{Combine}_{L}^{(k)}\Big(h_{v}^{(k - 1)},h_{v^{-1}}^{(k - 1)},m_{v}^{(k)}\Big),}\\ & {m_{\Psi^{(v)}}^{(k)} = \mathrm{Aggregate}_{C}^{(k)}\Big(\{h_{u}^{(k - 1)}:u\in \Psi (v)\} \Big),\quad h_{\Psi^{(v)}}^{(k)} = \mathrm{Combine}_{C}^{(k)}\Big(h_{\Psi^{(v)}}^{(k - 1)},m_{\Psi^{(v)}}^{(k)}\Big),}\\ & {\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad (2)} \end{array} \quad (2)\]  

where \(h_{v}^{(k)}\) and \(h_{\Psi^{(v)}}^{(k)}\) denote embeddings of the literal \(v\) and the clause \(\Psi (v)\) in the \(k^{t h}\) iteration \((h_{v^{- v}}^{(k)}\) denotes the embedding of the negation of \(v\) ); \(m_{v}^{(k)}\) and \(m_{\Psi (v)}^{(k)}\) refer to their propagated messages. Since the value of a Boolean formula is determined by the value assignment of the literal variables, Eq. 2 solely requires the final- state literal embeddings \(\{h_{u}^{(K)},u\in L\}\) to predict the logical reasoning result. More specifically, we use \(L\) and \(C\) to denote a literal set and a clause set ( \(L\) and \(C\) may be different for each CNF formula), then \(\Psi (v)\) is a clause and \(\Psi (v)\) denotes a clause including the literal \(v\in L\) .  

Note that the graph embeddings for SAT [7] and 2QBF [7] are generally represented by Eq.2. Hence our further analysis is based on Eq.2.