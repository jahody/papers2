To create the augmented datasets, we leverage CaDiCaL (Fleury & Heisinger, 2020) to generate a DART proof (Wetzler et al., 2014) for each SAT instance, which tracks the clause learning procedure and records all the learned clauses during the solving process. These learned clauses are then added to each instance, with a maximum limit of 1,000 clauses. For experiments on augmented datasets, we keep all training settings identical to those used for the original datasets.  

For contrastive pretraining experiments, we treat each original formula and its augmented counterpart as a positive pair and all other instances in a mini- batch as negative pairs. We use an MLP projection to map the graph embedding \(z_{i}\) of each formula to \(m_{i}\) and employ the SimCLR's contrastive loss (Chen et al., 2020), where the loss function for a positive pair of examples \((i,j)\) in a mini- batch of size \(2N\) is defined as:  

\[\mathcal{L}_{i,j} = -\log \frac{\exp(\sin(m_{i},m_{j}) / \tau)}{\sum_{k = 1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\sin(m_{i},m_{k}) / \tau)}. \quad (8)\]
Here, \(\mathbb{1}_{[k \neq i]}\) is an indicator function that evaluates to 1 if \(k \neq i\) , \(\tau\) is a temperature parameter, and \(\text{sim}(\cdot , \cdot)\) is the similarity function defined as \(\text{sim}(m_i, m_j) = m_i^\top m_j / \|m_i\| \|m_j\|\) . The final loss is the average over all positive pairs. In our experiments, we set the temperature parameter to 0.5 and utilize a learning rate of \(10^{- 4}\) with a weight decay of \(10^{- 8}\) . The pretraining process is performed for a total of 100 epochs. Once the pretraining is completed, we keep the GNN model and remove the projection head for downstream tasks.  

For experiments involving random initialization, we utilize Kaiming Initialization (He et al., 2015) to initialize all literal/variable and clause embeddings during both training and testing. For the predicted assignments, we utilize 2- clustering decoding to construct two possible assignment predictions for NeuroSAT\* at each iteration. When calculating the number of flipped variables and unsatisfiable clauses for NeuroSAT\*, we only consider the better assignment prediction of the two at each iteration, which is the one that satisfies more clauses. All other experimental settings remain the same as in the benchmarking evaluation.