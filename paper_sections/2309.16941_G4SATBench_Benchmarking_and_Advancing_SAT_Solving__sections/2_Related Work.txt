SAT solving with GNNs. Existing GNN- based SAT solvers can be broadly categorized into two branches (Holden et al., 2021; Guo et al., 2022): standalone neural solvers and neural- guided solvers. Standalone neural solvers utilize GNNs to solve SAT instances directly. For example, a stream of research (Bünz & Lamm, 2017; Selsam et al., 2019; Jaszczur et al., 2020; Cameron et al., 2020; Shi et al., 2023) focuses on predicting the satisfiability of a given formula, while several alternative approaches (Amizadeh et al., 2019a;b; Ozolins et al., 2022; Li et al., 2023; Yan et al., 2023) aim to construct a satisfying assignment. Neural- guided solvers, on the other hand, integrate GNNs with modern SAT solvers, trying to improve their search heuristics with the prediction of GNNs. These methods typically train GNN models using supervised learning on some tasks such as unsat- core variable prediction (Selsam & Bjørner, 2019; Wang et al., 2021), satisfying assignment prediction (Zhang et al., 2020), glue variable prediction (Han, 2020), and assignment marginal prediction (Li & Si, 2022), or through reinforcement learning (Yolcu & Póczos, 2019; Kurin et al., 2020) by modeling the entire search procedure as a Markov decision process. Despite the rich literature on SAT solving with GNNs, there is no benchmark study to evaluate and compare the performance of these GNN models. We hope the proposed G4SATBench would address this gap.
SAT datasets. Several established SAT benchmarks, including the prestigious SATLIB (Hoos & Stützle, 2000) and the SAT Competitions over the years, have provided a variety of practical instances to assess the performance of modern SAT solvers. Regrettably, these datasets are not particularly amenable for GNNs to learn from, given their relatively modest scale (less than 100 instances for a specific domain) or overly extensive instances (exceeding 10 million variables and clauses). To address this issue, researchers have turned to synthetic SAT instance generators (Giráldez- Cruz & Levy, 2015; 2017; Lauria et al., 2017; Selsam et al., 2019), which allow for the creation of a flexible number of instances with customizable settings. However, most of the existing datasets generated from these sources are limited to a few domains (less than 3 generators), small in size (less than 10k instances), or easy in difficulty (less than 40 variables within an instance), and there is no standardized dataset for evaluation. In G4SATBench, we include a variety of synthetic generators with carefully selected configurations, aiming to construct a broad collection of SAT datasets that are highly conducive for training and evaluating GNNs.