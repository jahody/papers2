Evaluation on the same distribution. Table 1 shows the benchmarking results of each GNN baseline when trained and evaluated on datasets possessing identical distributions. All GNN models exhibit strong performance across most easy and medium datasets, except for the medium SR dataset. This difficulty can be attributed to the inherent characteristic of this dataset, which includes satisfiable and unsatisfiable pairs of medium- sized instances distinguished by just a single differing literal. Such a subtle difference presents a substantial challenge for GNN models in satisfiability classification. Among all GNN models, the different graph constructions do not seem to have a significant impact on the results, and NeuroSAT (on LCG\*) and GGNN (on VCG\*) achieve the best overall performance.  

Table 1: Classification accuracy of satisfiability on identical distribution.   

<table><tr><td rowspan="2">Graph</td><td rowspan="2">Method</td><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td><td>k-Vercov</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td></tr><tr><td rowspan="4">LCG*</td><td>NeuroSAT</td><td>96.00</td><td>96.33</td><td>98.83</td><td>96.59</td><td>97.92</td><td>99.77</td><td>99.99</td><td>78.02</td><td>84.90</td><td>99.57</td><td>96.81</td><td>89.39</td><td>99.67</td></tr><tr><td>GCN</td><td>94.43</td><td>94.47</td><td>98.79</td><td>97.53</td><td>98.24</td><td>99.59</td><td>99.98</td><td>69.39</td><td>82.67</td><td>99.53</td><td>96.16</td><td>85.72</td><td>99.16</td></tr><tr><td>GGNN</td><td>96.36</td><td>95.70</td><td>98.81</td><td>97.47</td><td>98.80</td><td>99.77</td><td>99.97</td><td>71.44</td><td>83.45</td><td>99.50</td><td>96.21</td><td>81.20</td><td>99.69</td></tr><tr><td>GIN</td><td>95.78</td><td>95.37</td><td>98.14</td><td>96.98</td><td>97.60</td><td>99.71</td><td>99.97</td><td>70.54</td><td>82.80</td><td>99.49</td><td>95.80</td><td>83.87</td><td>99.61</td></tr><tr><td rowspan="3">VCG*</td><td>GCN</td><td>93.19</td><td>94.92</td><td>97.82</td><td>95.79</td><td>98.72</td><td>99.54</td><td>99.99</td><td>66.35</td><td>83.75</td><td>99.49</td><td>95.48</td><td>82.99</td><td>99.42</td></tr><tr><td>GGNN</td><td>96.75</td><td>96.25</td><td>98.77</td><td>96.44</td><td>98.88</td><td>99.68</td><td>99.98</td><td>77.12</td><td>85.11</td><td>99.57</td><td>96.48</td><td>83.63</td><td>99.62</td></tr><tr><td>GIN</td><td>96.04</td><td>95.71</td><td>98.47</td><td>96.95</td><td>97.33</td><td>99.59</td><td>99.98</td><td>73.56</td><td>85.26</td><td>99.49</td><td>96.55</td><td>89.41</td><td>99.38</td></tr></table>  

Evaluation across different distributions. To assess the generalization ability of GNN models, we evaluate the performance of NeuroSAT (on LCG\*) and GGNN (on VCG\*) across different datasets and difficulty levels. As shown in Figure 3 and Figure 4, NeuroSAT and GGNN struggle to generalize effectively to datasets distinct from their training data in most cases. However, when trained on the SR dataset, they exhibit better generalization performance across different datasets. Furthermore, while both GNN models
demonstrate limited generalization to larger formulas beyond their training data, they perform relatively better on smaller instances. These observations suggest that the generalization performance of GNN models for satisfiability prediction is influenced by the distinct nature and complexity of its training data. Training on more challenging instances could potentially enhance their generalization ability.  

<center>Figure 3: Classification accuracy of satisfiability across different datasets. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>  

<center>Figure 4: Classification accuracy of satisfiability across different difficulty levels. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>  

Due to the limited space, Figure 4 exclusively displays the performance of NeuroSAT and GGNN on the SR and 3- SAT datasets. Comprehensive results on the other five datasets, as well as the experimental results on different massage passing iterations, are provided in Appendix C.2.