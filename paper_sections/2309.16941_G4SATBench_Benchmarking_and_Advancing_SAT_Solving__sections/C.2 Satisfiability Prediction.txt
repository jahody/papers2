Evaluation across different difficulty levels. The complete results of NeuroSAT and GGNN across different difficulty levels are presented in Figure 6. Consistent with the findings on the SR and 3- SAT datasets, both GNN models exhibit limited generalization ability to larger instances beyond their training data, while displaying relatively better performance on smaller instances. This observation suggests that training these models on more challenging instances could potentially enhance their generalization ability and improve their performance on larger instances.  

<center>Figure 6: Classification accuracy of satisfiability across different difficulty levels. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>
Evaluation with different message passing iterations. To investigate the impact of message-passing iterations on the performance of GNN models during training and testing, we conducted experiments with varying iteration values. Figure 7 presents the results of NeuroSAT and GGNN trained and evaluated with different message passing iterations. Remarkably, using a training iteration value of 32 consistently yielded the best performance for both models. Conversely, employing too small or too large iteration values during training resulted in decreased performance. Furthermore, the models trained with 32 iterations also demonstrated good generalization ability to testing iterations 16 and 64. These findings emphasize the critical importance of selecting an appropriate message-passing iteration to ensure optimal learning and reasoning within GNN models.  

<center>Figure 7: Classification accuracy of satisfiability across different message passing iterations \(T\) . The x-axis denotes testing iterations and the y-axis denotes training iterations. </center>