Evaluation with different training losses. Table 2 presents the results of NeuroSAT (on \(\mathrm{LCG}^{*}\) ) and GGNN (on \(\mathrm{VCG}^{*}\) ) across three different training objectives. The results of other GNN models are listed in Table 10 in Appendix C.3. Interestingly, the unsupervised training methods outperform the supervised learning approach across the majority of datasets. We hypothesize that this is due to the presence of multiple satisfying assignments in most satisfiable instances. Supervised training tends to bias GNN models towards learning a specific satisfying solution, thereby neglecting the exploration of other feasible ones. This bias may compromise the models' ability to generalize effectively. Such limitations become increasingly apparent when the space of satisfying solutions is much larger, as seen in the medium CA and PS datasets. Additionally, it is noteworthy that employing \(\mathrm{UNS}_1\) as the loss function can result in instability during the training of some GNN models, leading to a failure to converge in some cases. Conversely, using \(\mathrm{UNS}_2\) loss demonstrates strong and stable performance across all datasets.  

In addition to evaluating the performance of GNN models under various training loss functions, we extend our analysis to explore how these models perform across different data distributions and under various inference algorithms. Furthermore, we assess the robustness of these GNN models when trained on noisy datasets that include unsatisfiable instances in an unsupervised fashion. For detailed results of these evaluations across different GNN baselines, please refer to Appendix C.3.
Table 2: Solving accuracy on identical distribution with different training losses. SUP denotes the supervised loss, \(\mathrm{UNS}_1\) and \(\mathrm{UNS}_2\) correspond to the unsupervised losses defined in Equation 5 and Equation 6, respectively. The symbol "-" indicates that some seeds failed during training. Note that only satisfiable instances are evaluated in this experiment.   

<table><tr><td rowspan="2">Graph</td><td rowspan="2">Method</td><td rowspan="2">Loss</td><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td><td>k-Verov</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td></tr><tr><td rowspan="3">LCG*</td><td rowspan="3">NeuroSAT</td><td>SUP</td><td>88.47</td><td>78.39</td><td>0.27</td><td>39.18</td><td>66.30</td><td>69.61</td><td>85.15</td><td>34.97</td><td>20.07</td><td>0.00</td><td>3.64</td><td>56.61</td><td>52.09</td></tr><tr><td>UNS1</td><td>82.30</td><td>80.23</td><td>82.17</td><td>89.23</td><td>88.34</td><td>96.74</td><td>99.36</td><td>25.00</td><td>30.40</td><td>35.45</td><td>60.28</td><td>41.45</td><td>95.06</td></tr><tr><td>UNS2</td><td>79.79</td><td>80.59</td><td>89.34</td><td>88.79</td><td>63.43</td><td>98.85</td><td>99.73</td><td>37.25</td><td>41.61</td><td>70.83</td><td>71.03</td><td>-</td><td>96.18</td><td>95.99</td></tr><tr><td rowspan="3">VCG*</td><td rowspan="3">GGNN</td><td>SUP</td><td>84.13</td><td>72.87</td><td>0.29</td><td>38.82</td><td>60.80</td><td>68.36</td><td>82.06</td><td>14.15</td><td>7.96</td><td>0.00</td><td>2.33</td><td>52.35</td><td>49.07</td></tr><tr><td>UNS1</td><td>76.39</td><td>76.55</td><td>78.13</td><td>84.44</td><td>84.60</td><td>97.49</td><td>-</td><td>16.55</td><td>22.84</td><td>28.12</td><td>44.89</td><td>54.29</td><td>-</td></tr><tr><td>UNS2</td><td>78.75</td><td>76.42</td><td>84.08</td><td>86.29</td><td>87.12</td><td>98.06</td><td>99.34</td><td>21.18</td><td>25.68</td><td>50.66</td><td>57.96</td><td>68.91</td><td>92.26</td><td>94.30</td></tr></table>