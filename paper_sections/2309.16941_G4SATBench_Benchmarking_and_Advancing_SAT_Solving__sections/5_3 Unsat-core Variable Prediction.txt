Evaluation on the same distribution. The benchmarking results presented in Table 3 exhibit the superior performance of all GNN models on both easy and medium datasets, with NeuroSAT consistently achieving the best results across most datasets. It is important to note that the primary objective of predicting unsat- core variables is not to solve SAT problems directly but to provide valuable guidance for enhancing the backtracking search process. As such, even imperfect predictions - for instance, those with a classification accuracy of \(90\%\) - have been demonstrated to be sufficiently effective in improving the search heuristics employed by modern CDCL- based SAT solvers, as indicated by previous studies (Selsam & Bj√∏rner, 2019; Wang et al., 2021).  

Table 3: Classification accuracy of unsat-core variables on identical distribution. Only unsatisfiable instances are evaluated.   

<table><tr><td rowspan="2">Graph</td><td rowspan="2">Method</td><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td><td>k-Verov</td></tr><tr><td rowspan="4">LCG*</td><td>NeuroSAT</td><td>90.76</td><td>94.43</td><td>83.69</td><td>86.20</td><td>99.93</td><td>95.80</td><td>94.47</td><td>90.07</td><td>99.65</td><td>85.73</td><td>88.53</td><td>99.97</td><td>97.90</td></tr><tr><td>GCN</td><td>89.17</td><td>94.35</td><td>82.89</td><td>85.32</td><td>99.93</td><td>95.74</td><td>94.43</td><td>88.11</td><td>99.65</td><td>85.71</td><td>87.70</td><td>99.96</td><td>97.89</td></tr><tr><td>GGNN</td><td>90.02</td><td>94.38</td><td>83.59</td><td>86.03</td><td>99.93</td><td>95.79</td><td>94.46</td><td>89.05</td><td>99.65</td><td>85.69</td><td>87.95</td><td>99.96</td><td>97.89</td></tr><tr><td>GIN</td><td>89.29</td><td>94.33</td><td>83.71</td><td>85.97</td><td>99.93</td><td>95.81</td><td>94.47</td><td>88.85</td><td>99.65</td><td>85.71</td><td>87.92</td><td>99.96</td><td>97.89</td></tr><tr><td rowspan="3">VCG*</td><td>GCN</td><td>88.57</td><td>94.34</td><td>83.17</td><td>85.27</td><td>99.93</td><td>95.79</td><td>94.46</td><td>88.17</td><td>99.65</td><td>85.70</td><td>87.37</td><td>99.96</td><td>97.90</td></tr><tr><td>GGNN</td><td>89.57</td><td>94.37</td><td>83.50</td><td>85.84</td><td>99.93</td><td>95.81</td><td>94.49</td><td>88.84</td><td>99.65</td><td>85.68</td><td>88.03</td><td>99.98</td><td>97.90</td></tr><tr><td>GIN</td><td>89.50</td><td>94.35</td><td>83.23</td><td>85.69</td><td>99.93</td><td>95.79</td><td>94.47</td><td>89.51</td><td>99.65</td><td>85.72</td><td>88.13</td><td>99.96</td><td>97.89</td></tr></table>  

We also conduct experiments to evaluate the generalization ability of GNN models on unsat- core variable prediction. Please see appendix C.4 for details.