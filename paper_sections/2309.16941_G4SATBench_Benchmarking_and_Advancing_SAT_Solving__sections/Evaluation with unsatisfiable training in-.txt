Evaluation with unsatisfiable training instances. Following previous works (Amizadeh et al., 2019a;b; Ozolins et al., 2022), our evaluation of GNN models focuses solely on satisfiable instances. However, in practical scenarios, the satisfiability of instances may not be known before training. To address this gap, we explore the effectiveness of training NeuroSAT using the unsupervised loss \(\mathrm{UNS}_2\) on noisy datasets that contain unsatisfiable instances. Table 11 presents the results of NeuroSAT when trained on such datasets, where \(50\%\) of the instances are unsatisfiable. Interestingly, incorporating unsatisfiable instances for training does not significantly affect the performance of the GNN model. This finding highlights the potential utility of training GNN models using \(\mathrm{UNS}_2\) loss on new datasets, irrespective of any prior knowledge regarding their satisfiability.  

Table 11: Solving accuracy of NeuroSAT when trained on noisy datasets. Values in parentheses indicate the performance difference compared to the model trained without unsatisfiable instances. The \(k\) -Clique dataset is excluded as NeuroSAT fails during training.   

<table><tr><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Domset</td><td>k-Vercov</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Domset</td><td>k-Vercov</td></tr><tr><td>78.84<br>(-0.95)</td><td>80.48<br>(-0.11)</td><td>87.01<br>(-2.33)</td><td>88.66<br>(-0.13)</td><td>98.00<br>(-0.85)</td><td>95.24<br>(-4.49)</td><td>37.21<br>(-0.04)</td><td>41.75<br>(+0.14)</td><td>76.49<br>(+5.64)</td><td>72.52<br>(+1.46)</td><td>94.93<br>(-1.25)</td><td>96.18<br>(+0.19)</td></tr></table>