Evaluation on the clause- learning augmented instances. CDCL- based SAT solvers enhance backtracking search with conflict analysis and clause learning, enabling efficient exploration of the search space by iteratively adding "learned clauses" to avoid similar conflicts in future searches (Silva & Sakallah, 1999). To assess whether GNN- based SAT solvers can learn and benefit from the backtracking search (with CDCL) heuristic, we augment the original formulas in the datasets with learned clauses and evaluate GNN models on these clause- augmented instances.
Table 4 shows the testing results on augmented SAT datasets. Notably, training on the augmented instances leads to significant improvements in both satisfiability prediction and satisfying assignment prediction. These improvements can be attributed to the presence of "learned clauses" that effectively modify the structure of the original formulas, thereby facilitating GNNs to solve with relative ease. However, despite the augmented instances being easily solvable using the backtracking search within a few search steps, GNN models fail to effectively handle these instances when trained on the original instances. These findings suggest that GNNs may not implicitly learn the CDCL heuristic when trained for satisfiability prediction or satisfying assignment prediction.  

Table 4: Results on augmented datasets. Values inside/outside parentheses denote the results of models trained on augmented/original instances.   

<table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="2">Easy Datasets</td><td colspan="2">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>SR</td><td>3-SAT</td></tr><tr><td rowspan="2">T1</td><td>NeuroSAT</td><td>100.00 (96.78)</td><td>100.00 (96.06)</td><td>100.00 (84.57)</td><td>96.78 (84.85)</td></tr><tr><td>GGNN</td><td>100.00 (97.66)</td><td>100.00 (95.46)</td><td>100.00 (84.01)</td><td>96.29 (85.80)</td></tr><tr><td rowspan="2">T2</td><td>NeuroSAT</td><td>85.05 (83.28)</td><td>83.50 (81.04)</td><td>51.95 (45.51)</td><td>39.00 (16.52)</td></tr><tr><td>GGNN</td><td>85.35 (83.42)</td><td>81.56 (79.99)</td><td>44.18 (40.09)</td><td>34.67 (14.75)</td></tr></table>  

Table 5: Results using contrastive pretraining. Values in parentheses denote the difference between the results without pretraining.   

<table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="2">Easy Datasets</td><td colspan="2">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>SR</td><td>3-SAT</td></tr><tr><td rowspan="2">T1</td><td>NeuroSAT</td><td>96.68 (+0.68)</td><td>96.23 (+0.10)</td><td>78.31 (+0.29)</td><td>85.02 (+0.12)</td></tr><tr><td>GGNN</td><td>96.46 (+0.29)</td><td>96.45 (+0.20)</td><td>76.34 (+0.78)</td><td>85.17 (+0.06)</td></tr><tr><td rowspan="2">T2</td><td>NeuroSAT</td><td>80.54 (+0.75)</td><td>79.71 (-0.88)</td><td>36.42 (-0.83)</td><td>41.23 (-0.38)</td></tr><tr><td>GGNN</td><td>80.66 (-0.34)</td><td>79.23 (-0.09)</td><td>33.44 (+0.07)</td><td>36.39 (+0.44)</td></tr></table>  

Evaluation with contrastive pretraining. Observing that GNN models exhibit superior performance on clause- learning augmented SAT instances, there is potential to improve the performance of GNNs by learning a latent representation of the original formula similar to its augmented counterpart. Motivated by this, we also experiment with a contrastive learning approach (i.e., SimCLR (Chen et al., 2020)) to pretrain the representation of CNF formulas to be close to their augmented ones (Duan et al., 2022), trying to explicitly embed the CDCL heuristic in the latent space through representation learning.  

The results of contrastive pretraining are presented in Table 5. In contrast to the findings in Duan et al. (2022), our results show limited performance improvement through contrastive pretraining, indicating that GNN models still encounter difficulties in effectively learning the CDCL heuristic in the latent space. This observation aligns with the conclusions drawn in Chen & Yang (2019), which highlight that static GNNs may fail to exactly replicate the same search operations due to the dynamic changes in the graph structure introduced by the clause learning technique.