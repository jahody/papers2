In G4SATBench, we provide the ground truth of satisfiability and satisfying assignments by calling the state-of-the-art modern SAT solver CaDiCaL (Fleury & Heisinger, 2020) and generate the truth labels for unsat- core variables by invoking the proof checker DRAT-trim (Wetzler et al., 2014). All neural networks in our study are implemented using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). For all GNN models, we set the feature dimension \(d\) to 128 and the number of message passing iterations \(T\) to 32. The MLPs in the models consist of two hidden layers with the ReLU (Nair & Hinton, 2010) activation function. To select the optimal hyperparameters for each GNN baseline, we conduct a grid search over several settings. Specifically, we explore different learning rates from \(\{10^{-3}, 5 \times 10^{-4}, 10^{-4}, 5 \times 10^{-5}, 10^{-5}\}\), training epochs from \(\{50, 100, 200\}\), weight decay values from \(\{10^{-6}, 10^{-7}, 10^{-8}, 10^{-9}, 10^{-10}\}\), and gradient clipping norms from \(\{0.1, 0.5, 1\}\). We employ Adam (Kingma & Ba, 2015) as the optimizer and set the batch size to 128, 64, or 32 to fit within the maximum GPU memory (48G). For the parameters \(\tau\) and \(\kappa\) of the unsupervised loss in Equation 4 and Equation 5, we try the default settings (\(\tau = t^{-0.4}\) and \(\kappa = 10\), where \(t\) is the global step during training) as the original paper (Amizadeh et al., 2019a) as well as other values (\(\tau \in \{0.05, 0.1, 0.2, 0.5\}\), \(\kappa \in \{1, 2, 5\}\)) and empirically find \(\tau = 0.1\), \(\kappa = 1\) yield the best results. Furthermore,
Table 9: Supported GNN models in G4SATBench.   

<table><tr><td>Graph</td><td>Method</td><td>Message-passing Algorithm</td><td>Notes</td></tr><tr><td rowspan="2">NeuroSAT</td><td>h(c), s(c) = LayerNormLSTM1</td><td>h(c), s(c) = LayerNormLSTM1</td><td>s, s1 are the hidden states which are initialized to zero vectors.</td></tr><tr><td>h(c), s(c) = LayerNormLSTM2</td><td>h(c), s(c) = LayerNormLSTM2</td><td></td></tr><tr><td rowspan="2">LCG*</td><td>h(c) = Linear1</td><td>h(c) = Linear1</td><td>d, d1 are the degrees of clause node c and literal node l in LCG respectively.</td></tr><tr><td>h(c) = Linear2</td><td>h(c) = Linear2</td><td></td></tr><tr><td rowspan="2">GGNN</td><td>h(c) = GRU1</td><td>h(c) = GRU1</td><td></td></tr><tr><td>h(c) = GRU2</td><td>h(c) = GRU2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = MLP1</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = MLP2</td><td></td></tr><tr><td rowspan="2">GCN</td><td>h(c) = Linear1</td><td>h(c) = Linear1</td><td>d, d1 are the degrees of clause node c and variable node v in VCG respectively.</td></tr><tr><td>h(c) = Linear2</td><td>h(c) = Linear2</td><td></td></tr><tr><td rowspan="2">VCG*</td><td>h(c) = GRU1</td><td>h(c) = GRU1</td><td></td></tr><tr><td>h(c) = GRU2</td><td>h(c) = GRU2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = MLP1</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = MLP2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = GIN</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = GIN</td><td></td></tr></table>  

it is important to note that we use three different random seeds to benchmark the performance of different GNN models and assess the generalization ability of NeuroSAT and GGNN using one seed for simplicity.