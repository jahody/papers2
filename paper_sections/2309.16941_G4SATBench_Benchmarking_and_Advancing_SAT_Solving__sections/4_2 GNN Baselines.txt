Graph constructions. It is important to note that traditional graph representations of a CNF formula often lack the requisite details for optimally constructing GNNs. Specifically, the LIG and VIG exclude clause- specific information, while the LCG and VCG fail to differentiate between positive and negative literals of the same variable. To address these limitations, existing approaches typically build GNN models on the refined versions of the LCG and VCG encodings. In the LCG, a new type of edge is added between each literal and its negation, while the VCG is modified by using two types of edges to indicate the polarities of variables within a clause. These modified encodings are termed the LCG\* and VCG\* respectively, and an example of them is shown in Figure 2. It is also worth noting alternative graph encodings like the And- Inverter- Graph (AIG), can be applied for SAT instances that are not in CNF. However, such representations are specialized to specific applications (like CircuitSAT) and are not designed for general purposes. Given this specialization, we choose to keep them outside the scope of the current G4SATBench.  

<center>Figure 2: LCG\* and VCG\* of the CNF formula \((x_{1} \vee \neg x_{2}) \wedge (x_{1} \vee x_{3}) \wedge (\neg x_{1} \vee x_{2} \vee x_{3})\) . </center>  

tion, while the VCG is modified by using two types of edges to indicate the polarities of variables within a clause. These modified encodings are termed the LCG\* and VCG\* respectively, and an example of them is shown in Figure 2. It is also worth noting alternative graph encodings like the And- Inverter- Graph (AIG), can be applied for SAT instances that are not in CNF. However, such representations are specialized to specific applications (like CircuitSAT) and are not designed for general purposes. Given this specialization, we choose to keep them outside the scope of the current G4SATBench.  

Message- passing schemes. G4SATBench enables performing various heterogeneous message- passage algorithms between neighboring nodes on the LCG\* or VCG\* encodings of a CNF formula. For the sake of illustration, we will take GNN models on the LCG\* as an example. We first define a \(d\) - dimensional embedding for every literal node and clause node, denoted by \(h_{l}\) and \(h_{c}\) respectively. Initially, all these embeddings are assigned to two learnable vectors \(h_{l}^{0}\) and \(h_{c}^{0}\) , depending on their node types. At the \(k\) - th iteration of
message passing, these hidden representations are updated as:  

\[\begin{array}{r l} & {h_{c}^{(k)} = \mathrm{UP D}\left(\underset {l\in \mathcal{N}(c)}{\mathrm{A G G}}\left(\left\{\mathrm{MLP}\left(h_{l}^{(k - 1)}\right)\right\} \right),h_{c}^{(k - 1)}\right),}\\ & {h_{l}^{(k)} = \mathrm{UP D}\left(\underset {c\in \mathcal{N}(l)}{\mathrm{A G G}}\left(\left\{\mathrm{MLP}\left(h_{c}^{(k - 1)}\right)\right\} \right),h_{-l}^{(k - 1)},h_{l}^{(k - 1)}\right),} \end{array} \quad (1)\]  

where \(\mathcal{N}(\cdot)\) denotes the set of neighbor nodes, MLP is the multi- layer perception, \(\mathrm{UP D}(\cdot)\) is the update function, and \(\mathrm{AGG}(\cdot)\) is the aggregation function. Most GNN models on \(\mathrm{LCG}^{*}\) use Equation 1 with different choices of the update function and aggregation function. For instance, NeuroSAT employs LayerNormL- STM (Ba et al., 2016) as the update function and summation as the aggregation function. In G4SATBench, we provide a diverse range of GNN models, including NeuroSAT (Selsam et al., 2019), Graph Convolutional Network (GCN) (Kipf & Welling, 2017), Gated Graph Neural Network (GGNN) (Li et al., 2016), and Graph Isomorphism Network (GIN) (Xu et al., 2019), on the both \(\mathrm{LCG}^{*}\) and \(\mathrm{VCG}^{*}\) . More details of these GNN models are included in Appendix B.