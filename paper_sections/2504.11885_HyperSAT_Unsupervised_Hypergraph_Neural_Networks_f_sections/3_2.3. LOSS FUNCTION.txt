Given a Weighted MaxSAT instance \(\phi = (\mathcal{X},\mathcal{C},\mathbf{w})\) and its hypergraph \(\mathcal{G} = (\mathcal{V},\mathcal{E},\mathbf{W})\) , we relax the Boolean variables \(\mathcal{X}\) into continuous probability parameters \(\mathbf{Y}(\gamma)\) where \(\gamma = (\pmb {R},\pmb {L}^{(0)},\pmb {W}_{Q},\pmb {W}_{K},\pmb {W}_{V},\widetilde{\pmb{W}}_{Q},\widetilde{\pmb{W}}_{K},\widetilde{\pmb{W}}_{V})\) represent the learnable parameters. The relaxation is defined as follows:  

\[\mathcal{X}\in \{0,1\}^{n}\longrightarrow \mathbf{Y}(\gamma)\in [0,1]^{n}. \quad (6)\]  

With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \(\gamma\) , enabling gradient- based optimization.  

In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \(\mathcal{L}_{\mathrm{task}}\) and a shared representation constraint loss \(\mathcal{L}_{\mathrm{shared}}\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows:  

\[\mathcal{L}_{\mathrm{total}} = \mathcal{L}_{\mathrm{task}} + \lambda \mathcal{L}_{\mathrm{shared}}, \quad (7)\]  

where \(\lambda \geq 0\) is a balancing hyperparameter.  

The primary task loss function is the relaxed optimization objective of the Weighted MaxSAT problem, as shown below:  

\[\mathcal{L}_{\mathrm{task}}(\mathbf{Y}) = \sum_{j = 1}^{m}w_{j}V_{j}(\mathbf{Y}), \quad (8)\]  

where  

\[V_{j}(\mathbf{Y}) = 1 - \prod_{i\in C_{j}^{+}}(1 - y_{i})\prod_{i\in C_{j}^{-}}y_{i}. \quad (9)\]  

Here, \(C_{j}^{+}\) and \(C_{j}^{- }\) are the index sets of variables appearing in the clause \(C_{j}\) in the positive and negative form, respectively. The term \(V_{j}(\mathbf{Y})\) represents the satisfaction of clause \(C_{j}\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \(w_{j}\) ensures that
more important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted MaxSAT problem.  

The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the HyperSAT network. Its form is given by  

\[\mathcal{L}_{\mathrm{shared}} = \left\| \pmb{L}_{+}^{(T - 1)} + \pmb{L}_{-}^{(T - 1)}\right\|_{F}^{2}, \quad (10)\]  

where \(\left\| \cdot \right\|_{F}\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space.  

It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted MaxSAT problem. On the one hand, a Weighted MaxSAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted MaxSAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances.