The core of the Weighted MaxSAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \(x\) and \(\neg x\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.
Considering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes.  

Specifically, given the \(l\) - th layer output of the HyperGCN \(\pmb{L}^{(l + 1)}\) , we divide it into two parts: \(\pmb{L}^{(l + 1)^{\prime}} =\) \(\left[\pmb{L}_{+}^{(l + 1)^{\prime}},\pmb{L}_{- }^{(l + 1)^{\prime}}\right]\) , where \(\pmb{L}_{+}^{(l + 1)^{\prime}}\in \mathbb{R}^{n\times d_{l + 1}}\) represent the positive literal node representations and \(\pmb{L}_{- }^{(l + 1)^{\prime}}\in \mathbb{R}^{n\times d_{l + 1}}\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows:  

\[\begin{array}{r l} & {\pmb{L}_{+}^{(l + 1)} = \mathrm{softmax}\left(\frac{\pmb{Q}_{+}^{(l + 1)}(\pmb{K}_{+}^{(l + 1)})^{\top}}{\sqrt{d_{l + 1}}}\right)\pmb{V}_{-}^{(l + 1)},}\\ & {\pmb{L}_{-}^{(l + 1)} = \mathrm{softmax}\left(\frac{\pmb{Q}_{-}^{(l + 1)}(\pmb{K}_{+}^{(l + 1)})^{\top}}{\sqrt{d_{l + 1}}}\right)\pmb{V}_{+}^{(l + 1)}.} \end{array} \quad (4)\]  

In this formulation,  

\[\begin{array}{r l} & {\pmb{Q}_{+}^{(l + 1)} = \pmb{W}_{Q}^{(l + 1)}\pmb{L}_{+}^{(l + 1)^{\prime}},\pmb{Q}_{-}^{(l + 1)} = \widetilde{\pmb{W}}_{Q}^{(l + 1)}\pmb{L}_{-}^{(l + 1)^{\prime}},}\\ & {\pmb{K}_{+}^{(l + 1)} = \pmb{W}_{K}^{(l + 1)}\pmb{L}_{+}^{(l + 1)^{\prime}},\pmb{K}_{-}^{(l + 1)} = \widetilde{\pmb{W}}_{K}^{(l + 1)}\pmb{L}_{-}^{(l + 1)^{\prime}},}\\ & {\pmb{V}_{+}^{(l + 1)} = \pmb{W}_{V}^{(l + 1)}\pmb{L}_{+}^{(l + 1)^{\prime}},\pmb{V}_{-}^{(l + 1)} = \widetilde{\pmb{W}}_{V}^{(l + 1)}\pmb{L}_{-}^{(l + 1)^{\prime}},} \end{array} \quad (5)\]  

where \(\pmb{W}_{Q}^{(l + 1)}\) , \(\pmb{W}_{K}^{(l + 1)}\) , \(\pmb{W}_{V}^{(l + 1)}\) are the learnable query, key and value projection matrices for the positive literal nodes at the \(l\) - th layer, and \(\widetilde{\pmb{W}}_{Q}^{(l + 1)}\) , \(\widetilde{\pmb{W}}_{K}^{(l + 1)}\) , \(\widetilde{\pmb{W}}_{V}^{(l + 1)}\) are the learnable query, key and value projection matrix for the negative literal nodes at the \(l\) - th layer.  

The final node representation at the \(l\) - th layer is obtained as \(\pmb{L}^{(l + 1)} = [\pmb{L}_{+}^{(l + 1)},\pmb{L}_{- }^{(l + 1)}]\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted MaxSAT problems.  

Building upon this, the transformer module in HyperSAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (ViT- 22B (Dehghani et al., 2023)), the transformer module in HyperSAT includes a LayerNorm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional  

LayerNorm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the LayerNorm layer. The resulting sum is then passed through another LayerNorm layer to stabilize the representations before proceeding to the next layer.  

The final layer of the network is a softmax layer. We reshape the iterated \(\pmb{L}^{(T)^{\prime}}\in \mathbb{R}^{2n\times 1}\) into \(\hat{\pmb{L}}^{(T)^{\prime}}\in \mathbb{R}^{n\times 2}\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \(\hat{\pmb{L}} = \mathrm{softmax}(\hat{\pmb{L}}^{(T)^{\prime}})\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \(\pmb{Y} = \hat{\pmb{L}}_{- 1}\) , which is the first column of \(\hat{\pmb{L}}\) .