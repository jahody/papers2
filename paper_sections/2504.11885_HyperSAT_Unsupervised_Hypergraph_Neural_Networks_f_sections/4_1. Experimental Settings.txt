We compare the performance of HyperSAT against GNN- based methods for solving Weighted MaxSAT problems.  

Baseline Algorithms. We compare our proposed HyperSAT against GNN- based methods. The following algorithms are considered as baselines: (i) HypOp (Heydaribeni et al., 2024): an advanced unsupervised learning framework  

that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted MaxSAT problems.  

Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St√ºtzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted MaxSAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \([1,10]\) uniformly at random.  

Table 1. The parameters of the datasets.   

<table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table>  

Model Settings. HyperGCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \(7 \times 10^{- 2}\) . The balancing hyperparameter of loss function \(\lambda\) is \(2 \times 10^{- 3}\) . The FFN consists of two linear transformations and a ReLU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \(10^{- 4}\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained.  

Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and PyTorch 1.13.0.