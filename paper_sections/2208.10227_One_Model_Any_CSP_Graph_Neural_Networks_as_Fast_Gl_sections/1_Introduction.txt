Constraint Satisfaction Problems (CSP) are a ubiquitous framework for specifying combinatorial search and optimization problems. They include many of the best- known NP- hard problems such as Boolean satisfiability (SAT), graph coloring (COL) and maximum cut (MAXCUT) and can flexibly adapted to model specific application dependent problems. CSP solution strategies range from general solvers based on methods such as constraint propagation or local search (see Russell et al. (2020), Chapter 6) to specialized solvers for individual problems like SAT (see Biere et al. (2021)). In recent years, there is a growing interest in applying deep learning methods to combinatorial problems including many CSPs (e.g. Khalil et al. (2017), Selsam et al. (2018), TÃ¶nshoff et al. (2021)). The main motivation for these approaches is to learn novel heuristics from data rather than crafting them by hand.  

Graph Neural Networks (see Gilmer et al. 2017) have emerged as an effective tool for learning powerful, permutation invariant functions on graphs using deep neural networks, and they have become one of the main architectures in the field of neural combinatorial optimization. Problem instances are modelled as graphs and then mapped to approximate solutions with GNNs. However, most methods use graph reductions and GNN architectures that are problem specific, and transferring them across combinatorial tasks requires considerable engineering, limiting their use cases. Designing a generic neural network architecture and training procedure for the general CSP formalism offers a powerful alternative. Then learning heuristics for any specific CSP becomes a purely data driven process requiring no specialized graph reduction or architecture search.   

We propose a novel GNN based reinforcement learning approach to general constraint satisfaction. The main contributions of our method called \(\mathrm{ANYCSP}^{1}\) can be summarized as follows: We define a new graph representation for general CSP instances which is generic and well suited as an input for recurrent GNNs. It allows us to directly process all CSPs with one unified architecture and no prior reduction to more specific CSPs, such as SAT. In particular, one \(\mathrm{ANYCSP}\) model can take every CSP instance as input, even those with domain sizes, constraint arities, or relations not seen during training. Training is unsupervised using policy gradient ascent with a carefully tailored reward scheme that encourages exploration and prevents the search to get stuck in local maxima. During inference, a trained \(\mathrm{ANYCSP}\) model iteratively searches the space of assignments to the variables of the CSP instance for an optimal solution satisfying the maximum number of constraints. Crucially, the search is global; it allows transitions between any two assignments in a single step. To enable this global search we use policy gradient methods to handle the exponential action spaces efficiently. This design choice speeds up the search substantially, especially on large instances. We thereby overcome a primary bottleneck of previous neural approaches based on local search, which only flips the values of a single or a few variables in each step. GNN based local search tends to scale poorly to large instances as one GNN forward pass takes significantly more time than one step of classical local search heuristics. \(\mathrm{ANYCSP}\) accounts for this and exploits the GNNs inherent parallelism to refine the solution globally in each step.  

We evaluate \(\mathrm{ANYCSP}\) by learning heuristics for a range of important CSPs: COL, SAT, MAXCUT and gen
eral CSP benchmark instances. We demonstrate that our method achieves a substantial increase in performance over prior GNN approaches and can compete with conventional algorithms. ANYCSP models trained on small random graph coloring problems are on par with state- of- the- art coloring heuristics on structured benchmark instances. On MAX- \(k\) - SAT, our method scales to test instances 100 times larger than the training data, where it finds better assignments than state- of- the- art conventional search heuristics despite performing 1000 times fewer search iterations.