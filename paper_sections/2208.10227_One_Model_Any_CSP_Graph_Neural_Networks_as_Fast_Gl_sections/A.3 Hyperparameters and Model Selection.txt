Before training (and hyperparameter tuning) we sample fixed validation datasets of 200 instances from the given distribution of CSP instances. We usually modify the distribution to yield larger instances than those used for training. This favors the selection of models that generalize well to larger instances, which is almost always desirable. Exact details on how the validation distribution differs from the training distribution in each experiment are provided for in Section B. During validation we perform \(T_{\mathrm{val}} = 200\) search iterations on each validation instance. The metric used for selection is the number of unsatisfied constraints in the best solution averaged over all validation instances. To save compute resources we perform only 100K training steps with each hyperparameter configuration and only perform the full 500K steps of the training with the best configuration.  

The aggregation function \(\bigoplus \in \{\mathrm{SUM},\mathrm{MEAN},\mathrm{MAX}\}\) is a key hyperparameter. The choice of \(\bigoplus\) is critical for performance, as we observed MAX aggregation to consistently perform best on decision problems but poorly on maximization tasks. The hidden dimension is set to \(d = 128\) . We also validated some models with \(d = 64\) but larger models seem to be more capable. We did not increase \(d\) further to avoid memory bottlenecks. We tuned the discount factor \(\lambda \in \{0.5,0.75,0.9,0.99\}\) and found the value of 0.75 to yield the best results in all of our experiments. The learning rate is initialized as \(\mathrm{lr} = 5\cdot 10^{- 6}\) and decays linearly throughout training to a final value of \(\mathrm{lr} = 5\cdot 10^{- 7}\) . All model train with a batch size of 25. We also considered larger batch sizes of 50 and 100 without improvement. Table 5 specifies the final configuration used in each experiment.