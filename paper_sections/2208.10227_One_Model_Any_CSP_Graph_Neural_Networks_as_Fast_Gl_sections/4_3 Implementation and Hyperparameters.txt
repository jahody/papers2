We implement ANYCSP in PyTorch 3. The code for relabeling \(G(\mathcal{I},\alpha^{(t)})\) in each iteration \(t\) is also fully based on PyTorch and is GPU- compatible. We implement generalized sparse matrix multiplication in the COO format in CUDA. This helps to increase the memory efficiency and speed of the message passes between values and constraints. We plan to publish this extension as a stand- alone software package or merge it with PyTorch Sparse to make it accessible to the broader Graph Learning community.  

We choose a hidden dimension of \(d = 128\) for all experiments. We train with the Adam optimizer for 500K training steps with a batch size of 25. Training a model takes between 24 and 48 hours, depending on the data. During training, we set the upper number of iterations to \(T_{\mathrm{train}} = 40\) . During testing, we usually run ANYCSP with a timeout rather than a fixed upper number of iterations \(T\) . All hyperparameters are provided in Appendix A.  

For each training distribution \(\Omega\) we implement data loaders that sample new instances on- the- fly in each training step. With our hyperparameters we therefore train each model on 12.5 Million sampled training instances. We use fixed subsets of 200 instances sampled from each distribution before training as validation data. The exact generation procedures for each training distribution are provided in Appendix B.