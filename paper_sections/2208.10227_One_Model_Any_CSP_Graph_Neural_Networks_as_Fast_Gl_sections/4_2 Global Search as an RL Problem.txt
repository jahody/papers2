We deploy the policy GNN \(\pi_{\theta}\) as a trainable search heuristic. Note that a single GNN \(\pi_{\theta}\) can search for solutions on any given CSP instance. ANYCSP takes a CSP instance \(\mathcal{I}\) and a parameter \(T \in \mathbb{N}\) as input and outputs a sequence
<center>Figure 2: Illustration of a run of ANYCSF on a given CSP instance \(\mathcal{I}\) . We iteratively apply our policy GNN \(\pi_{\theta}\) to the constraint value graph \(G(\mathcal{I},\alpha^{(t - 1)})\) of \(\mathcal{I}\) and the current assignment \(\alpha^{(t - 1)}\) . From this we obtain a soft assignment \(\phi^{(t)}\) from which the next assignment \(\alpha^{(t)}\) is sampled freely with no restrictions to locality. </center>  

\(\alpha = \alpha^{(0)},\ldots ,\alpha^{(T)}\) of assignments for \(\mathcal{I}\) . The initial assignment \(\alpha^{(0)}\) is simply drawn uniformly at random. In each iteration \(1\leq t\leq T\) the policy GNN \(\pi_{\theta}\) is applied to the current constraint value graph \(G(\mathcal{I},\alpha^{(t - 1)})\) to generate a new soft assignment \(\phi^{(t)}\) . The next assignment \(\alpha^{(t)}\sim \phi^{(t)}\) is then sampled from the predicted soft assignment by drawing a new value \(\alpha^{(t)}(X)\) for all variables \(X\) independently and in parallel without imposing any restrictions on locality. Any number of variables may change their value in each iteration which makes our method a global search heuristic. This allows ANYCSF to modify different parts of the solution simultaneously to speed up the search. Figure 2 provides a visual illustration of the overall process. Formally, our action space is the set of all assignments for the input instance, one of which must be chosen as the next assignment in each iteration \(t\) . This set is extremely large for many CSPs, with up to \(10^{50}\) assignments to choose from for some of our training instances. Despite this, we found standard policy gradient descent algorithms to be effective and stable during training.  

Rewarding Iterative Improvements We devise a reward scheme that assigns a real- valued reward \(r^{(t)}\) to each generated assignment \(\alpha^{(t)}\) . A simple approach would be to use the quality \(Q_{\mathcal{I}}(\alpha^{(t)})\) as a reward. However, we found that models trained with this reward tend to get stuck in local maxima and have comparatively poor performance. Intuitively, this simple reward scheme immediately punishes the policy for stepping out of a local maximum causing stagnating behavior.  

We, therefore, choose a more sophisticated reward system that avoids this issue. First, we define the auxiliary variable \(q^{(t)} = \max_{t^{\prime}< t}Q_{\mathcal{I}}(\alpha^{(t^{\prime})})\) , which tracks the highest quality achieved before iteration \(t\) . We then define the reward in iteration \(t\) as follows:  

\[r^{(t)} = \left\{ \begin{array}{ll}0 & \mathrm{if} Q_{\mathcal{I}}(\alpha^{(t)}) \leq q^{(t)},\\ Q_{\mathcal{I}}(\alpha^{(t)}) - q^{(t)} & \mathrm{if} Q_{\mathcal{I}}(\alpha^{(t)}) > q^{(t)}. \end{array} \right. \quad (3)\]  

The policy earns a positive reward in iteration \(t\) if the new assignment \(\alpha^{(t)}\) satisfies more constraints than any assignment generated in the previous steps. In this case, the reward is the margin of improvement. Note that the reward is 0 in any step in which the new assignment is not an improvement over the previous best regardless of whether the quality of the solution is increasing or decreasing. This reward is designed to encourage \(\pi_{\theta}\) to yield iteratively improving assignments while being agnostic towards how the assignments change between improvements. Our reward is conceptually similar to  

that of ECO- DQN (Barrett et al. 2020). The main difference is that we do not add intermediate rewards for reaching local maxima. Inductively, we observe that the total reward over all iterations is given by \(\sum_{t = 1}^{T}r^{(t)} = q^{(T + 1)} - Q_{\mathcal{I}}(\alpha^{(0)})\) . For any input instance \(\mathcal{I}\) the total reward is maximal (relative to \(Q_{\mathcal{I}}(\alpha^{(0)})\) ) if and only if the highest achieved quality \(q^{(T + 1)}\) is the optimal quality for \(\mathcal{I}\) . In Appendix C we provide an ablation study where we compare our reward scheme to the simpler choice of using \(Q_{\mathcal{I}}(\alpha^{(t)})\) directly as a reward.  

Markov Decision Process For a given input \(\mathcal{I}\) we model the procedure described so far as a Markov Decision Process \(\mathcal{M}(\mathcal{I})\) which will allow us to deploy standard reinforcement learning methods for training: The state in iteration \(t\) is given by \(s^{(t)} = (\alpha^{(t)},q^{(t)})\) and contains the current assignment and highest quality achieved before step \(t\) . The initial assignment \(\alpha^{(0)}\) is drawn uniformly at random and \(q^{(0)} = 0\) . The space of actions \(\mathcal{A}\) is simply the set of all possible assignments for \(\mathcal{I}\) . The soft assignments produced by the policy \(\pi_{\theta}\) are distributions over this action space. After the next action is sampled from this distribution, the state transition of the MDP is deterministic and updates the state with the chosen assignment and its quality. The reward \(r^{(t)}\) at time \(t\) is defined as in Equation 3.  

Training During training, we assume that some data generating distribution \(\Omega\) of CSP instances are given. We aim to find a policy that performs well on this distribution of inputs. Ideally, we need to find the set of parameters \(\theta^{*}\) which maximizes the expected total reward if we first draw an instance from \(\Omega\) and then apply the model to it for \(T_{\mathrm{train}}\in \mathbb{N}\) steps:  

\[\theta^{*} = \arg \max_{\theta}\underset {\alpha \sim \pi_{\theta}(\mathcal{I})}{\mathbb{E}}\left[\sum_{t = 1}^{T_{\mathrm{train}}}\lambda^{t - 1}r^{(t)}\right] \quad (4)\]  

The discount factor \(\lambda \in (0,1]\) and the number of search iterations during training \(T_{\mathrm{train}}\) are both hyperparameters. Starting with randomly initialized parameters \(\theta\) , we utilize REINFORCE (Williams 1992) to train \(\pi_{\theta}\) with stochastic policy gradient ascent. REINFORCE is a natural choice for training ANYCSF since its complexity does not depend on the size of the action space \(\mathcal{A}\) . Soft assignments allow us to efficiently sample the next assignment \(\alpha \sim \phi\) and recover its probability \(\mathbf{P}(\alpha |\phi) = \prod_{X}\phi (\alpha (X))\) . These are the only operations on
<center>Figure 3: Survival plot for RB50. The x-axis gives the wall clock runtime in seconds. The y-axis counts the cumulative number of instances solved within a given time. </center>  

the action space required for REINFORCE. Note that we use vanilla REINFORCE without a baseline or critic network and we sample a single trajectory for every training instance. We found this simple version of the algorithm to be surprisingly robust and effective in our setting. Details on how the policy gradients are computed are provided in Appendix A.