We evaluate ANYCSP on a wide range of well- known CSPs: Boolean satisfiability (3- SAT) and its maximisation version  

Table 1: Results on structured Graph Coloring instances. We provide the number of instances solved with a 20 Minute timeout for both splits, each containing 50 instances with chromatic number less than 10 and at least 10, respectively.  

<table><tr><td>METHOD</td><td>COL&amp;lt;10</td><td>COL≥10</td></tr><tr><td>RUNCSP</td><td>33</td><td>-</td></tr><tr><td>CoSoCo</td><td>49</td><td>33</td></tr><tr><td>PICAT</td><td>49</td><td>38</td></tr><tr><td>GREEDY</td><td>16</td><td>15</td></tr><tr><td>DSATUR</td><td>38</td><td>28</td></tr><tr><td>HYBRIDEA</td><td>50</td><td>40</td></tr><tr><td>ANYCSP</td><td>50</td><td>40</td></tr></table>  

(MAX- \(k\) - SAT for \(k = 3,4,5\) ), graph colorability ( \(k\) - COL), maximum cut (MAXCUT) as well as random CSPs (generated by the so- called MODEL RB). These problems are of high theoretical and practical importance and are commonly used to benchmark CSP heuristics. We train one ANYCSP model for each of these problems using randomly generated instances. Recall that the process of learning problem- specific heuristics with ANYCSP is purely data- driven as our architecture is generic and can take any CSP instance as input.  

We will compare the performance of ANYCSP to classical solvers and heuristics as well as previous neural approaches. When applicable, we also tune the configuration of the classical algorithms on our validation data to ensure a fair comparison. All neural approaches run with one NVIDIA Quadro RTX A6000 GPU with 48GB of memory. All classical approaches run on an Intel Xeon Platinum 8160 CPU (2.1 GHz) and 64GB of RAM.  

MODEL RB First, we evaluate ANYCSP on general CSP benchmark instances generated by the MODEL RB (Xu and Li 2003). Our training distribution \(\Omega_{\mathrm{RB}}\) consists of randomly generated MODEL RB instances with 30 variables and arity 2. The test dataset RB50 contains 50 satisfiable instances obtained from the XCSP project (Audemard et al. 2020). These instances each contain 50 variables, domains with 23 values and roughly 500 constraints. They are commonly used as part of the XCSP Competition to evaluate state- of- the- art CSP solvers. Note that the hardness of MODEL RB problems comes from the dense, random constraint relations chosen at the threshold of satisfiability and even instances with 50 variables are very challenging. We will compare ANYCSP to three state- of- the- art CSP solvers: Picat (Zhou 2022), ACE (Lecoutre 2022) and CoSoCo (Audemard 2018). Picat is a SAT- based solver while ACE and CoSoCo are based on constraint propagation. Picat in particular is the winner of the most recent XCSP Competition (Audemard, Lecoutre, and Lonca 2022). No prior neural baseline exists for this problem.  

Figure 3 provides a the results on the RB50 dataset. All algorithms run once on each instance with a 20 Minute timeout. ANYCSP solves the most instances by a substantial margin. The second strongest approach is the CoSoCo solver which solves 34 instances in total, 8 less than ANYCSP. Within the timeout of 20 Minutes, ANYCSP will perform 500K search iterations. Recall that we set \(T_{\mathrm{train}} = 40\) . Therefore,
Table 2: MAxCUT results on Gset graphs. The graphs are grouped by their vertex counts and we provide the mean deviation from the best known cut size.   

<table><tr><td>METHOD</td><td>|V|=800</td><td>|V|=1K</td><td>|V|=2K</td><td>|V|≥3K</td></tr><tr><td>GREEDY</td><td>411.44</td><td>359.11</td><td>737.00</td><td>774.25</td></tr><tr><td>SDP</td><td>245.44</td><td>229.22</td><td>-</td><td>-</td></tr><tr><td>RUNCSP</td><td>185.89</td><td>156.56</td><td>357.33</td><td>401.00</td></tr><tr><td>ECO-DQN</td><td>65.11</td><td>54.67</td><td>157.00</td><td>428.25</td></tr><tr><td>ECORD</td><td>8.67</td><td>8.78</td><td>39.22</td><td>187.75</td></tr><tr><td>ANYCSP</td><td>1.22</td><td>2.44</td><td>13.11</td><td>51.63</td></tr></table>  

the learned policy generalizes to searches that are over 10K times longer than those seen during training.  

Graph Coloring We consider the problem of finding a conflict- free vertex coloring given a graph \(G\) and number of colors \(k\) . The corresponding CSP instance has variables for each vertex, domains containing the \(k\) colors and one binary " \(\neq\) "- constraint for each edge. We train on a distribution \(\Omega_{\mathrm{COL}}\) of graph coloring instances for random graphs with 50 vertices. We mix Erdős- Rényi, Barabási- Albert and random geometric graphs in equal parts. The number of colors is chosen to be in [3, 10]. As test instances we use 100 structured benchmark graphs with known chromatic number \(\mathcal{X}(G)\) . The instances are obtained from a collection of hard coloring instances commonly used to benchmark heuristics<sup>4</sup>. They are highly structured and come from a wide range of synthetic and real problems. We divide the test graphs into two sets with 50 graphs each: \(\mathrm{COL}_{< 10}\) contains graphs with \(\mathcal{X}(G) < 10\) and \(\mathrm{COL}_{> 10}\) contains graphs with \(\mathcal{X}(G) \geq 10\) . The graphs in \(\mathrm{COL}_{> 10}\) have up to 1K vertices, 19K edges and a chromatic number of up to 73. This experiment tests generalization to larger domains and more complex structures.  

We compare the performance to three problem specific heuristics: a simple greedy algorithm, the classic heuristic DSATUR (Brélaz 1979) and the state- of- the- art heuristic HybridEA (Galinier and Hao 1999), all implemented efficiently by Lewis et al. (2012); Lewis (2015). We also evaluate the best two CSP solvers from the MODEL RB experiment. The neural baseline RUNCSP is also tested on \(\mathrm{COL}_{< 10}\) . Unlike ANYCSP, RUNCSP requires us to fix a domain size before training. Therefore, we must train one RUNCSP model for each tested chromatic number \(4 \leq \mathcal{X}(G) \leq 9\) and omit testing on \(\mathrm{COL}_{> 10}\) . We use the same training data as Tönshoff et al. (2021) for their experiments on structured coloring benchmarks.  

Table 1 provides the number of solved \(k\) - COL instances from both splits. ANYCSP is on par with HybridEA which solves the most instances of all baselines. RUNCSP solves significantly fewer instances than ANYCSP on \(\mathrm{COL}_{< 10}\) and outperforms only the simple greedy approach. ANYCSP solves 40 out of the 50 instances in \(\mathrm{COL}_{> 10}\) . The optimally colored graphs include the largest instance with 73 colors. Since ANYCSP trains with 3 to 10 colors the trained model  

Table 3: Number of solved 3-SAT benchmark instances from SATLIB. For each number of variables there are 100 satisfiable test instances.   

<table><tr><td>METHOD</td><td>SL50</td><td>SL100</td><td>SL150</td><td>SL200</td><td>SL250</td></tr><tr><td>RLSAT</td><td>100</td><td>87</td><td>67</td><td>27</td><td>12</td></tr><tr><td>PDP</td><td>93</td><td>79</td><td>72</td><td>57</td><td>61</td></tr><tr><td>WALK SAT</td><td>100</td><td>100</td><td>97</td><td>93</td><td>87</td></tr><tr><td>PROB SAT</td><td>100</td><td>100</td><td>97</td><td>87</td><td>92</td></tr><tr><td>ANYCSP</td><td>100</td><td>100</td><td>100</td><td>97</td><td>99</td></tr></table>  

is able to generalize to significantly larger domains.  

MAXCUT For MAXCUT we train on the distribution \(\Omega_{\mathrm{MCUT}}\) of random unweighted Erdős- Rényi graphs with 100 vertices and an edge probability \(p \in [0.05, 0.3]\) . Our test data is Gset (Ye 2003), a collection of commonly used MAXCUT benchmarks of varying structure with 800 to 10K vertices. We evaluate three neural baselines: RUNCSP, ECO- DQN (Barrett et al. 2020) and ECORD (Barrett, Personson, and Laterre 2022). RUNCSP is also trained on \(\Omega_{\mathrm{MCUT}}\) . We train and validate ECO- DQN and ECORD models with the same data that Barrett, Personson, and Laterre (2022) used for their Gset experiments. We omit S2V- DQN (Khalil et al. 2017) since ECO- DQN and ECORD have been shown to yield substantially better cuts. We adopt the evaluation setup of ECORD and run the neural methods with 20 parallel runs and a timeout of 180s on all unweighted instances of Gset. The results of a standard greedy construction algorithm and the well- known SDP based approximation algorithm by Goemans and Williamson (1995) are also included as classical baselines. Both are implemented by Mehta (2019). SDP runs with a 3 hour timeout for graphs with up to 1K vertices.  

Table 2 provides results for Gset. We divide the test graphs into groups by the number of vertices (8- 9 graphs per group) and report the mean deviation from the best- known cuts obtained by Benlic and Hao (2013) for each method. ANYCSP outperforms all baselines across all graph sizes by a large margin. Recall that RUNCSP trains on a soft relaxation of MAXCUT while ECO- DQN and ECORD are both neural local search approaches. Neither concept matches the results of our global search approach trained with policy gradients.  

3- SAT For 3- SAT we choose the training distribution \(\Omega_{\mathrm{3SAT}}\) as uniform random 3- SAT instances with 100 variables. The ratio of clauses to variables is drawn uniformly from the interval [4, 5]. For 3- SAT we test on commonly used benchmark instances for uniform 3- SAT from SATLIB<sup>5</sup>. The test set \(\mathrm{SLN}\) contains 100 instances with \(N \in \{50, 100, 150, 200, 250\}\) variables each. The density of these formulas is at the threshold of satisfiability. We evaluate two neural baselines: RLSAT (Yolcu and Póczos 2019) and PDP (Amizadeh, Matusevych, and Weimer 2019). PDP is also trained on \(\Omega_{\mathrm{3SAT}}\) . We train RLSAT with the curriculum learning dataset for 3- SAT provided by its authors, since its reward scheme is incompatible with our partially unsatisfi
Table 4: Results on Max- \(k\) -SAT instances with 10K variables. For each \(k\in \{3,4,5\}\) we provide the mean number of unsatisfied clauses over 50 random instances.   

<table><tr><td>METHOD</td><td>3CNF</td><td>4CNF</td><td>5CNF</td></tr><tr><td>WALKSAT</td><td>2145.28</td><td>1556.68</td><td>1685.10</td></tr><tr><td>CCLS</td><td>1567.24</td><td>1323.14</td><td>1315.96</td></tr><tr><td>SATLIKE</td><td>1595.86</td><td>1188.56</td><td>1152.88</td></tr><tr><td>ANYCSP</td><td>1537.46</td><td>1126.44</td><td>1103.14</td></tr></table>  

able training instances. We also adopt the experimental setup of RLSAT, which limits the evaluation run by the number of search steps instead of a timeout. The provided code for both PDP and RLSAT is comparatively slow and a timeout would compare implementation details rather than the capability of the learned algorithms. We also evaluate two conventional local search heuristics: The classical WalkSAT algorithm (Selman et al. 1993) based on random walks and a modern probabilistic approach called probSAT (Balint and Schöning 2018). Like Yolcu and Póczos (2019), we apply stochastic boosting and run each method 10 times for 10K steps on every instance. PDP is deterministic and only applied once to each formula.  

Table 3 provides the number of solved instances for each tested size. All compared approaches do reasonably well on small instances with 50 variables. However, the performance of the two neural baselines drops significantly as the number of variables increases. ANYCSP does not suffer from this issue and even outperforms the classical local search algorithms on the three largest instance sizes considered here.  

MAX- \(k\) - SAT We train on the distribution \(\Omega_{\mathrm{MSAT}}\) of uniform random MAX- \(k\) - SAT instances with 100 variables and \(k\in \{3,4\}\) . Here, the clause/variable ratio is chosen from [5, 8] and [10, 16] for \(k = 3\) and \(k = 4\) , respectively. These formulas are denser than those of \(\Omega_{3\mathrm{SAT}}\) since we aim to train for the maximization task. Our test data for MAX- \(k\) - SAT consists of uniform random \(k\) - CNF formulas generated by us. For each \(k\in \{3,4,5\}\) we generate 50 instances with 10K variables each. The number of clauses is chosen as 75K for \(k = 3\) , 150K for \(k = 4\) and 300K for \(k = 5\) . These formulas are therefore 100 times larger than the training data and aim to test the generalization to significantly larger instances as well as unseen arities, since \(k = 5\) is not used for training. Neural baselines for SAT focus primarily on decision problems. For MAX- \(k\) - SAT we therefore compare ANYCSP only to conventional search heuristics: the classical (Max- )WalkSAT (Selman et al. 1993) and two state- of- the- art MAX- SAT local search heuristics CCLS (Luo et al. 2015) and SATLike (Cai and Lei 2020). Table 4 provides a comparison. We provide the mean number of unsatisfied clauses after processing each instance with a 20 Minute timeout. Remarkably, ANYCSP outperforms all classical baselines by a significant margin.  

We point out that the conventional search heuristics all perform over 100M search steps in the 20 Minute timeout. ANYCSP performs less than 100K steps on each instance in this experiment. The GNN cannot match the speed with  

<center>Figure 4: Detailed results for MAX-5-SAT. For each test instance and each method we plot the number of unsatisfied clauses in the best found solution against the search step in which it was found. </center>  

which classical algorithms iterate, even though it is accelerated by a GPU. Despite this, ANYCSP consistently finds the best solutions. Figure 4 evaluates this surprising observation further. We plot the number of unsatisfied clauses in the best found solution against the search step in which the solution was found (Steps to Opt.) for all methods and all instances of our MAX- 5- SAT test data. We also provide the results of a modified ANYCSP version (ANYCSP Local defined in Appendix C) that is only allowed to change one variable at a time and is therefore a local search heuristic. Note that the \(x\) - axis is logarithmic as there is a clear dichotomy separating neural and classical approaches: Compared to conventional heuristics ANYCSP performs roughly three orders of magnitude fewer search steps in the same amount of time. When restricted to local search, ANYCSP is unable to overcome this deficit and yields worse results than strong heuristics such as SATLike. However, when ANYCSP leverages global search to parallelize refinements across the whole instance it can find solutions in 100K steps that elude state- of- the- art local search heuristics after well over 100M iterations.