The primary bottleneck of \(\mathrm{ANYCSP}\) is GPU memory. More specifically, the maximum instance size that can be pro
Table 5: Selected Hyperparameters for each considered CSP   

<table><tr><td></td><td>MODEL RB</td><td>k-COL</td><td>3-SAT</td><td>Max-k-SAT</td><td>MAXCUT</td></tr><tr><td>d</td><td>128</td><td>128</td><td>128</td><td>128</td><td>128</td></tr><tr><td>⊕</td><td>MAX</td><td>MAX</td><td>MAX</td><td>MEAN</td><td>SUM</td></tr><tr><td>λ</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td></tr><tr><td>Ttrain</td><td>40</td><td>40</td><td>40</td><td>40</td><td>40</td></tr><tr><td>batch size</td><td>25</td><td>25</td><td>25</td><td>25</td><td>25</td></tr><tr><td>lr</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td></tr></table>  

cessed is usually determined by the memory required for the message passes between values and constraints. Let \(\mathcal{F} = (\mathcal{X},\mathcal{C},\mathcal{D})\) be a CSP instance with constraints of arity \(k\) and domains of uniform size \(\ell\) . Then the constraint value graph will contain \(|\mathcal{C}|\cdot k\cdot \ell\) constraint edges. Constraint edges are represented with a sparse matrix. For each non- zero entry of the sparse matrix (edge), we store the row (outgoing node) and the column (incoming node). Thus the space complexity of storing constraint edges is \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell)\) . During message passing from values to constraints, each value generates two messages with length d. Those messages are stacked along the first dimension, resulting in a dense matrix with \(2\cdot |\mathcal{X}|\cdot \ell \cdot d\) entries. To pass messages, we use sparse dense matrix multiplication (SPMM) between the sparse matrix of the edges and the dense matrix of the messages generated from values. Alternatively, one can also use the scatter operation from the PyTorch Scatter library, but the scatter operation requires the construction of an intermediate tensor stacking the messages send along each edge. This allocates extra memory of size \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell \cdot d)\) . In contrast, SPMM only allocates memory for the result of the aggregation, but no intermediate tensor is build. As a result, the space complexity of the SPMM operation is \(\mathcal{O}(|\mathcal{C}|\cdot d)\) for all aggregation types. Combining all of the terms, we get a space complexity of \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell +|\mathcal{X}|\cdot \ell \cdot d + |\mathcal{C}|\cdot d)\) for the message passing from values to constraints. The message passing from constraints to values uses the same operations that are used for values to constraints, therefore, the space complexity is the same for both directions.  

PyTorch Sparse supports generalized SPMM only in CSR format \(^6\) . Their implementation requires the expensive conversion of the sparse matrix from COO to CSR. The adjacency matrix between the stack of generated messages and the constraints is re- wired in every iteration \(t\) according to the new edge labels. Converting a new large matrix into CSR format in every step would be prohibitively expensive. To avoid that, we implement generalized sparse dense matrix multiplication in COO format with CUDA. With our newly implemented function we can pass messages memory efficiently and faster.  

We also experimented with more advanced attention- based aggregation, namely GAT (Veličković et al. 2017). However, it did not improve the performance but made the construction of large, intermediate edge- level tensors in the message passes unavoidable. Due to this, we restrict our focus on the  

three basic aggregations of element- wise SUM, MEAN and MAX.