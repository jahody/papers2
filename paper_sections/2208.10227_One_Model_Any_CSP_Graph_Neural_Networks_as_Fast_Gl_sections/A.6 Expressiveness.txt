A well- known theoretical result on Graph Neural Networks is their correspondence to the Weisfeiler- Lehman isomorphism test. More specifically, standard message- passing GNNs can not distinguish more structures than the 1- dimensional Weisfeiler- Lehman test. For node- level tasks this prohibits a GNN to map two different nodes with identical \(n\) - hop subtrees to different outputs after \(n\) message passes. For some graph structures, such as regular graphs, this makes certain combinatorial tasks, such as graph coloring, fundamentally impossible with standard GNNs.  

Crucially, ANYCSP is not limited by 1- WL. Our policy GNN \(\pi_{\theta}\) has access to randomness which has been proven to strengthen GNN expressiveness beyond the WL- hierarchy (Abboud et al. 2021; Sato, Yamada, and Kashima 2021). In each iteration \(\pi_{\theta}\) predicts a soft assignment. From this we sample a hard assignment and pass it back to the GNN as a binary pattern. This process can be understood as giving the GNN oracle access to randomness in every iteration. Soft assignments are not just a way of outputting a new assignment but also provide the means with which \(\pi_{\theta}\) interacts with randomness. The GNN can learn to predict soft assignments with high variance to break symmetries through random sampling.  

Empirically this is also demonstrated in the MAXCUT experiment. The graphs \(G48\) and \(G49\) are both 4- regular toroidal graphs. ANYCSP computes optimal cuts for both graphs, which correspond to conflict- free 2- colorings. This could not be achieved by any function limited by 1- WL.
Algorithm 1: Forward Pass of \(\pi_{\theta}\) . All inner for-loops are parallelized.  

Input: CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) , Number of steps \(T\in \mathbb{N}\)  

Output: Soft Assignments \(\phi = \phi^{(1)},\ldots ,\phi^{(T)}\) , Assignments \(\alpha = \alpha^{(1)},\ldots ,\alpha^{(T)}\) , Rewards \(\pmb {r} = r^{(1)},\ldots ,r^{(T)}\)  

1: for \(X\in \mathcal{X}\) do  

2: \(\alpha^{(0)}(X)\sim \mathcal{D}(X)\)  

3: end for  

4: \(L_{V}^{(0)},L_{E}^{(0)}\leftarrow \mathrm{LABEL}(\mathcal{I},\alpha^{(0)})\)  

5: \(q^{(1)}\leftarrow Q_{\mathcal{I}}(\alpha^{(0)})\)  

6: for \(v\in \mathcal{V}\) do  

7: \(h^{(0)}(v)\leftarrow \mathbf{h}\)  

8: end for  

9: for \(t\in \{1,\ldots ,T\}\) do  

10: for \(v\in \mathcal{V}\) do  

11: \(x^{(t)}(v)\leftarrow \mathbf{E}\left(\left[h^{(t - 1)}(v),L_{V}^{(t - 1)}(v)\right]\right)\)  

12: \(m^{(t)}(v,0),m^{(t)}(v,1)\leftarrow \mathbf{M}_{\mathcal{V}}\left(x^{(t)}(v)\right)\)  

13: end for  

14: for \(C\in \mathcal{C}\) do  

15: \(y^{(t)}(C) = \bigoplus_{v\in \mathcal{V}(C)}m^{(t)}\left(v,L_{E}(C,v)\right)\)  

16: \(m^{(t)}(C,0),m^{(t)}(C,1) = \mathbf{M}_{\mathcal{C}}\left(y^{(t)}(C)\right)\)  

17: end for  

18: for \(v\in \mathcal{V}\) do  

19: \(y^{(t)}(v) = \bigoplus_{C\in \mathcal{V}(v)\cap \mathcal{C}}m^{(t)}(C,L_{E}(C,v))\)  

20: \(z^{(t)}(v) = \mathbf{U}_{\mathcal{V}}\left(x^{(t)}(v) + y^{(t)}(v)\right) + x^{(t)}(v)\)  

21: end for  

22: for \(X\in \mathcal{X}\) do  

23: \(z^{(t)}(X) = \mathbf{U}_{\mathcal{X}}\left(\bigoplus_{v\in D_{X}}z^{(t)}(v)\right)\)  

24: end for  

25: for \(v\in \mathcal{V}\) do  

26: \(h^{(t)}(v)\leftarrow \mathbf{G}\left(h^{(t - 1)}(v),z^{(t)}(v) + z^{(t)}(X)\right)\)  

27: \(o^{(t)}(v)\leftarrow \mathbf{O}(h^{(t)}(v))\)  

28: end for  

29: for \(v\in \mathcal{V}\) do  

30: \(\phi^{(t)}(v)\leftarrow \frac{\exp\left(o^{(t)}(v)\right)}{\sum_{v^{\prime}\in \mathcal{V}(X_{v})}\exp\left(o^{(t)}(v^{\prime})\right)}\)  

31: end for  

32: \(\alpha^{(t)}\sim \phi^{(t)}\)  

33: \(L_{V}^{(t)},L_{E}^{(t)}\leftarrow \mathrm{LABEL}(\mathcal{I},\alpha^{(t)})\)  

34: \(r^{(t)}\leftarrow \max \{Q_{\mathcal{I}}(\alpha^{(t)}) - q^{(t)},0\}\)  

35: \(q^{(t + 1)}\leftarrow \max \{q^{(t)},Q_{\mathcal{I}}(\alpha^{(t)})\}\)  

36: end for  

37: return \(\theta\) , \(\alpha\) , \(r\)  

Sample initial assignment uniformly.  

Get vertex + edge labels.  

Init. best prior quality.  

\(\mathbf{h}\) is the learned initial state.  

Values generate latent state.  

Values generate two messages.  

Constraints receive messages.  

Constraints generate messages.  

Values receive messages from constraints.  

Values receive messages.  

Variables receive states from values.  

Update recurrent states.  

Values predict scores.  

Apply softmax within each domain  

Sample next assignment.  

Relabel graph.  

Get Reward.  

Update best prior quality.
Algorithm 2: Training ANYCSP  

Input: Initial parameters \(\theta\) , training distribution \(\Omega\) , train_steps \(\in \mathbb{N}\) , batch_size \(\in \mathbb{N}\) , \(T_{\mathrm{train}} \in \mathbb{N}\) , \(\mathrm{lr} > 0\) , \(\lambda \in (0,1]\)  

Output: Trained parameters \(\theta\)  

1: for \(s \in \{1, \ldots , \mathrm{train\_steps}\}\) do  

2: for \(i \in \{1, \ldots , \mathrm{batch\_size}\}\) do  

3: \(\mathcal{F} \sim \Omega\)  

4: \(\phi_{\theta}, \alpha , \mathbf{r} \leftarrow \pi_{\theta}(\mathcal{F}, T_{\mathrm{train}})\)  

5: for \(t \in \{1, \ldots , T_{\mathrm{train}}\}\) do  

6: \(G_{t} \leftarrow \sum_{k = t}^{T} \lambda^{k - t} r^{(k)}\)  

7: end for  

8: \(\nabla_{\theta} J_{i} \leftarrow \nabla_{\theta} \sum_{t = 1}^{T} \left(G_{t} \sum_{X \in \mathcal{X}} \log \left(\phi_{\theta}^{(t)} (\alpha^{(t)} (X)) + \epsilon\right)\right)\)  

9: end for  

10: \(\theta \leftarrow \theta + \frac{\mathrm{lr}}{\mathrm{batch\_size}} \sum_{i} \nabla_{\theta} J_{i}\)  

11: end for  

12: return \(\theta\)  

\(\triangleright\) This loop is parallel across all \(i\) . \(\triangleright\) Sample training instance. \(\triangleright\) Apply policy network.  

Policy gradient for \(i\) - th instance in batch.  

\(\triangleright\) Average gradients and ascent.  

<center>Figure 6: A 2-coloring for a grid graph with size \(12 \times 12\) found by ANYCSP. Conflicting edges are shown in red. </center>