For a comprehensive overview on applying GNNs to combinatorial problems, we refer to Cappart et al. (2021). In this paper, we are primarily interested in end- 2- end approaches which seek to directly predict approximate solutions for combinatorial problems with trainable neural networks. Early work in this area was done by Bello et al. (2016), who learned TSP heuristics with Pointer Networks (Vinyals, Fortunato, and Jaitly 2015) and policy gradient descent. Several extensions of these ideas have since been proposed based on attention (Kool, van Hoof, and Welling 2018) and GNNs (Joshi et al. 2020). Khalil et al. (2017) propose a general method for graph problems, such as MAXCUT or Minimum Vertex Cover. They model the expansion of partial solutions as a reinforcement learning task and train a GNN with Q- learning to iteratively construct approximate solutions.  

A related group of approaches models local modifications to complete solutions as actions of a reinforcement learning problem. A GNN is then trained as a local search heuristic that iteratively improves candidate solutions through local changes. Methods following this concept are RLSAT (Yolcu and Póczos 2019) for SAT, ECO- DQN (Barrett et al. 2020) for MAXCUT, LS- DQN (Yao, Cai, and Wang 2021) for graph partitioning problems and TSP as well as BiHyb (Wang et al. 2021) for graph problems based on selecting and modifying edges. Like conventional search heuristics, these architectures can be applied for any number of search iterations to refine the solution. A shared drawback on large instances is the relatively high computational cost of GNNs, which slows down the search substantially when compared to classical algorithms. ECORD (Barrett, Parsonson, and Laterre 2022) addresses this issue for MAXCUT by applying a GNN only once before the local search, which is carried out by a faster GRU- based architecture without costly message passes. We address the same problem, but not by iterating faster, but by allowing global modifications in each iteration.  

A fundamentally different approach considers soft relaxations of the underlying problems which can optimized directly with SGD. Examples of this concept are PDP (Amizadeh, Matusevych, and Weimer 2019) for SAT and RUNCSP (Tönshoff et al. 2021) for all binary CSPs with fixed constraint language. These architectures can predict completely new solutions in each iteration but the relaxed differentiable objectives used for training typically do not capture the full hardness of the discrete problem.