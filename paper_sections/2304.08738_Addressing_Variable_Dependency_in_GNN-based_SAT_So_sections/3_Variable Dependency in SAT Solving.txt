Generally speaking, SAT and Circuit- SAT solving must consider variable dependency. In other words, they must "remember" what predictions have been made so far. A simple example is the 2- input XOR \((x \oplus y)\) . Here, \(x\) and \(y\) are symmetric — if we swap them, we will get exactly the same formula because XOR is commutative. However, we must assign different values to \(x\) and \(y\) in order to get a 1 as the result. If \(x\) has been assigned as 1, then \(y\) must be 0. This is the dependency between these two variables.  

Symmetry naturally exists in many SAT problems. Sometimes, it is part of the formula that is symmetric — for example, \((x \oplus y) \wedge z\) . When converted to AIG or CNF, a symmetric formula like \(x \oplus y\) will result in a symmetric AIG or CNF, as shown by Figure 1. It is not hard to see, symmetric nodes have symmetric predecessors and successors. Therefore, when GNN- based SAT solvers use message- passing to encode the graph structure, symmetric nodes will have the same node embeddings, unless they are distinguished by initialization. However, pure random initialization for all nodes provides no extra information for the neural network to distinguish the symmetric ones. On the other hand, a bias in initialization would introduce artefact that does not generalize. Therefore, prior works [Selsam et al., 2018; Amizadeh et al., 2018; Zhang et al., 2020] all used equal initial embeddings and therefore, they would not be able to distinguish symmetric nodes when predicting SAT assignments. We accompany our argument on random initialization with experimental results in the appendix.  

When individual node embeddings are directly used to predict variable assignments without considering the dependency among them, the inferred assignments will always be the same for the pair of symmetric nodes. As we have shown by the 2- input XOR example, some symmetric formulas reject equal variable assignments as their satisfying solutions. Therefore, NeuroSAT and DG- DAGRNN in [Selsam et al., 2018] and [Amizadeh et al., 2018] are unable to deal with these symmetric SAT or Circuit- SAT problems. We argue that a GNN- based SAT solver should sequentially predict variable assignments in order to take variable dependency into consideration. This is achieved by a recurrent neural network added in our model, explained in the next section.