Effectiveness of the RNN decoding layer. In this experiment, we train our AsymSAT model, the NeuroSAT model and the DG- DAGRNN model on the same 10 symmetric circuits and measure the training accuracy. We use two con
Table 2: Solution rate for the \(SR(n)\) problems   

<table><tr><td></td><td>SR(3)</td><td>SR(4)</td><td>SR(5)</td><td>SR(6)</td><td>SR(7)</td><td>SR(8)</td><td>SR(9)</td><td>SR(10)</td></tr><tr><td>AsymSAT</td><td>98.30%</td><td>100.00%</td><td>93.23%</td><td>94.51%</td><td>81.56%</td><td>82.90%</td><td>88.95%</td><td>85.45%</td></tr><tr><td>NeuroSAT</td><td>87.70%</td><td>74.47%</td><td>63.10%</td><td>59.57%</td><td>52.94%</td><td>48.40%</td><td>49.73%</td><td>43.82%</td></tr><tr><td>DG-DAGRNN</td><td>10.21%</td><td>15.23%</td><td>5.21%</td><td>1.83%</td><td>8.38%</td><td>5.70%</td><td>4.07%</td><td>4.24%</td></tr></table>  

Table 3: Solution rate for the \(V(n)\) problems   

<table><tr><td></td><td>V(3)</td><td>V(4)</td><td>V(5)</td><td>V(6)</td><td>V(7)</td><td>V(8)</td><td>V(9)</td><td>V(10)</td></tr><tr><td>AsymSAT</td><td>81.58%</td><td>67.50%</td><td>72.50%</td><td>55.50%</td><td>52.50%</td><td>60.00%</td><td>45.00%</td><td>47.50%</td></tr><tr><td>NeuroSAT</td><td>0.025%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.00%</td><td>0.0%</td><td>0.00%</td></tr><tr><td>DG-DAGRNN</td><td>35.00%</td><td>47.50%</td><td>47.50%</td><td>45.00%</td><td>30.00%</td><td>37.50%</td><td>37.50%</td><td>32.50%</td></tr></table>  

figurations for our AsymSAT model: one uses LSTM and the other uses GRU in the bi- directional RNN layer (the \(\mathcal{R}\) layer). We also add one case of removing the \(\mathcal{R}\) layer in AsymSAT as comparison. In this experiment, we set the learning rate of AsymSAT models as \(10^{- 3}\) and the number of iterations as 5. Table 1 illustrates the result for the symmetric circuits on five different models. Just as we discussed in Section 3, DG- DAGRNN and NeuroSAT cannot break the tie in symmetric circuits or symmetric CNF formulas. And there is no way to train these two models on this dataset. Thanks to the \(\mathcal{R}\) layer we introduced, our AsymSAT model can reach a solution rate of \(100.00\%\) with either LSTM or GRU in the \(\mathcal{R}\) layer. This shows the effectiveness of the \(\mathcal{R}\) layer for symmetric circuits.  

Setting the number of iterations. In this experiment, we study the effect of changing the number of iterations on our network. We set the iteration to be 5, 10, 15, and 20, respectively, and test on a mixed dataset with instances from \(SR(3)\) to \(SR(10)\) . We can see for AsymSAT with GRU, increasing the number of iteration from 5 to 10 will greatly improve the accuracy, then the solving rate barely increases for more iterations. AsymSAT with LSTM shows a similar result, while it peaks at around 15 iterations. It seems that AsymSAT with LSTM may have a higher potential to achieve a better accuracy. Therefore, in the following experiments on SAT solving, we mainly use AsymSAT with LSTM for comparison.