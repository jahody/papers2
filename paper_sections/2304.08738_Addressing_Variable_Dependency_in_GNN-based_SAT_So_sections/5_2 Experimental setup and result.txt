The dimension on the outcome of the \(\mathcal{R}\) layer is 10 and we use the Adam optimizer during training process. For NeuroSAT, and DG- DAGRNN, we follow the same configurations as described in [Selsam et al., 2018] and [Amizadeh et al., 2018]. To our best knowledge, the source code for the original DG- DAGRNN model is not publicly available. We build this model following the instructions in [Amizadeh et al., 2018]. We train and test all three models on a server with two NVIDIA GeForce RTX 3090 GPUs.