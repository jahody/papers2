In the high- level, we would like to build a machine- learning model that learns the mapping from a circuit graph to the 0- 1 assignment on input nodes: \(f: G \to L\) . There are plenty of existing GNN models that are designed to handle input data organized as a graph [Yolcu and Póczos, 2019; Selsam et al., 2018; Selsam and Björner, 2019]. There, each graph node is associated with a vector (the hidden state vector) which eventually represents some structural information around the node. Nodes exchange their knowledge of the graph structure by sending messages to their neighbors, and the hidden states will be gradually updated. The propagation of information is referred to as the message- passing mechanism, which essentially embeds the information about the graph structure into the hidden states.  

Graph embedding layers. When it comes to the implementation of message passing, there are various choices, for example, which direction the message flows towards, how to aggregate messages from several nodes, what is the order of hidden state updates. Therefore, different variants of message- passing can be implemented. In this work, we build upon the DAG- RNN framework [Shuai et al., 2016] to create a GNN architecture for sequential variable assignment prediction.  

To better explain our GNN architecture, we introduce the following notations. Each graph node \(v \in V_G\) is associated with a \(d\) - dimensional hidden state vector \(x_v\) , which is iteratively updated based on the messages from neighboring nodes. During message- passing, we distinguish the nodes that reach \(v\) following a directed edge (in other words, the
<center>Figure 1: (a) XOR implemented by AIG; (b) the DAG representation of (a); (c) the equi-satisfiable CNF with additional variable \(a\) and \(b\) , and the corresponding bipartite graph of XOR (here dotted line means the variable is negated in the clause). </center>  

predecessors) from those that leaves \(v\) (the successors). We only use the messages from predecessors in the forward pass, and likewise, the successors in the backward pass. The incoming messages are aggregated by an aggregator function \(\mathcal{A}\) , which is invariant to permutation of the elements in the input set. Finally, the aggregated message is used to update the hidden state of \(v\) by a standard GRU function \(GRU(\cdot)\) [Cho et al., 2014].  

In AsymSAT, message passing follows the topological order. In the forward pass, messages flow from circuit input nodes (which have no predecessors) to the only circuit output node (which has no successors). The hidden state vectors are updated sequentially. In the backward pass, messages flow from the circuit output node to the circuit input nodes. In each pass, the hidden state vectors are updated according to the following rule:  

\[x_{v}^{(k + 1)}\coloneqq GRU\left(p_{v},\mathcal{A}\left(\left\{m_{n}^{(k)}|n\in \mathcal{N}(v)\right\}\right)\right) \quad (1)\]  

Initially, \(p_{v} = N_{G}(v)\) , which is the node type vector of node \(v\) . So in the first forward pass, the type of a node is encoded into the hidden state vector. In all remaining passes, \(p_{v} = x_{v}^{(k)}\) , which is the hidden state vector resulted from the previous pass. We use three separate GRUs: \(GRU_{init}(\cdot)\) , \(GRU_{f}(\cdot)\) , \(GRU_{b}(\cdot)\) . Among the three, \(GRU_{init}(\cdot)\) is only used in the first forward pass. \(GRU_{f}(\cdot)\) is used for all remaining forward passes. \(GRU_{b}(\cdot)\) is used in the backward passes. We call one forward pass followed by one backward pass as an iteration. In Equation 1, \(\mathcal{N}(v)\) is either the predecessors or the successors of \(v\) . Their hidden state vectors are encoded into messages \(m_{n}^{(k)}\) by a learnable function \(\mathcal{M}: x_{n}^{(k)} \to m_{n}^{(k)}\) .  

Our graph embedding layers share some similarities with [Amizadeh et al., 2018] as both are built upon DAG- RNN. The difference here is mainly in the computation between two iterations. We use two GRUs for forward passes, because the size of a hidden state vector is different from that of the node type vector, whereas [Amizadeh et al., 2018] introduced a function to project hidden state vectors into the space  

of node type vectors after each iteration to keep the same dimensionality and use the same GRU. We argue that the projection could potentially introduce a loss of information and therefore, we employ two separate GRUs in the forward pass: \(GRU_{init}(\cdot)\) , \(GRU_{f}(\cdot)\) to handle either the node type vector or a hidden state vector from the previous pass.  

SAT assignment decoding layers. As discussed earlier, we would like to predict the Boolean assignments on the circuit input nodes sequentially. In this node- level prediction, we map a sequence of hidden state vectors of the circuit input nodes \(X = (x_{i_{1}}, x_{i_{2}}, x_{i_{3}}, \ldots)\) to a sequence of input assignment \(L\) . After iterations of message passing, these hidden state vectors encode the information related to the structure of the graph. If two input nodes are symmetric with respect to each other, we expect that their hidden state vectors will be the same. If we individually use each of these vectors to decode a 0- 1 assignment (namely, the concurrent prediction), the symmetric nodes will certainly map to the same variable assignment. As we have discussed in Section 3, SAT solutions must take variable dependency into consideration, therefore, in our model, we need to associate variable assignments of the same SAT problem.  

In our AsymSAT, we use a recurrent neural network (referred to as the \(\mathcal{R}\) layer) to generate sequential predictions on variable assignments, so that the model output on a certain circuit input node depends on the predictions of other nodes. We make this \(\mathcal{R}\) layer bi- directional to account for dependencies from both sides. A subsequent MLP will work as a selector to decide which direction is more preferred. Sequential prediction mimics classic (non- machine- learning- based) SAT solvers. These classic SAT solvers like GRASP [Marques- Silva and Sakallah, 1999] or MiniSAT [Sorensson and Een, 2005] pick decision variables one after another. Regarding the aforementioned XOR example, we expect this RNN layer will be able to learn to predict different variable assignments for the two symmetric variables after training with such examples.  

As a summary, we show the overall architecture of our
<center>Figure 2: (a) An example of an AIG circuit, (b) the graph embedding layers (for simplicity, we only draw the connections for two nodes.) (c) the SAT assignment decoding layers </center>  

Table 1: Percentage of the symmetric circuit problem solved   

<table><tr><td>AsymSAT w. LSTM</td><td>AsymSAT w. GRU</td><td>AsymSAT w.o. R layer</td><td>NeuroSAT DG-DAGRNN</td></tr><tr><td>100.00%</td><td>100.00%</td><td>0.00%</td><td>0.00%</td></tr></table>  

AsymSAT model in Figure 2.