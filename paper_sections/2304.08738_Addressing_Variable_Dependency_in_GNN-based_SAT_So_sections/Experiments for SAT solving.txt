In the following experiments, we compare the three models: our AsymSAT model with bi- directional LSTM in the \(\mathcal{R}\) layer, the NeuroSAT model, and the DG- DAGRNN model. We measure the performance using solution rate rather than the accuracy of predicting satisfiability. Solution rate is defined as the percentage of problems on which the network is able to predict one satisfying assignment.  

We admit that for different models, sometimes it is hard to make an absolutely fair comparison. As AsymSAT is supervised under the multi- bit SAT solution, whereas NeuroSAT is one- bit SAT/UNSAT supervision, AsymSAT is much more training- data- efficient and it can converge several orders of magnitude faster than NeuroSAT, although their parameter  

counts are roughly in the same scale (around 200K). Actually, the authors of NeuroSAT reported that it would take \(10^{7}\) SAT problems to train a full- fledged NeuroSAT [Selsam, 2018]. Although our training set is much smaller due to limitations of computing resources, we didn't observe the overfitting problem in the experiments judging from the training loss.  

Comparison on the \(SR(n)\) problems. We use \(8K\) \(SR(n)\) problems sampled uniformly from \(SR(U(3,8))\) to train the three models. The test set contains \(1.5K\) \(SR(n)\) problems \((n\) is from 3 to 10). For AsymSAT and DG- DAGRNN, CNF formulas are first converted into circuits to serve as the model input. Table 2 summarizes the performance measured on the \(SR(n)\) problem. The result shows that AsymSAT model has a better performance compared to NeuroSAT and DG- DAGRNN on this dataset. Overall, AsymSAT can reach more than \(90\%\) solution rate (averaged across \(SR(3)\) to \(SR(10)\) ), while NeuroSAT can only reach \(60\%\) . Furthermore, we supply more experimental results regarding larger \(SR(n)\) problems. When trained from \(SR(3)\) to \(SR(10)\) , AsymSAT outperforms NeuroSAT on \(SR(20)\) , \(SR(40)\) , \(SR(60)\) and \(SR(80)\) . The result is shown in Table 6. In our experiment, the performance of DG- DAGRNN is non- competitive to the other two. We conjecture that the unsupervised learning method in DG- DAGRNN suffers from the vanishing gradient problem if trained on circuits converted from CNF. We provide a detailed analysis of DG- DAGRNN in the appendix.  

Comparison on the \(V(n)\) problems. The training data is a mixture of \(8K\) \(SR(n)\) problems, \((n\) ranges from 3 to 10), and \(1.2K\) \(V(n)\) problems \((n\) ranges from 3 to 8). The test set is \(320\) \(V(n)\) problems \((n\) ranges from 3 to 10). Note that \(V(n)\) is a nontrivial dataset. On average, each \(V(10)\) problem has around 1K AND gates, more than those in circuits converted from the \(SR(10)\) problems (which each contain just about 200 AND gates). Even for the \(SR(40)\) problems, there are only approximately 600 - 800 AND gates per input. Therefore, \(V(n)\) problems can also demonstrate the generalization capability of the tested models. Although the indexing number \(n\) is relatively smaller in the \(V(n)\) dataset, there are plenty of logic gates in each circuit. These logic
Table 4: Solution rate for the larger \(V(n)\) problems   

<table><tr><td></td><td>V(11)</td><td>V(12)</td><td>V(13)</td><td>V(14)</td><td>V(15)</td></tr><tr><td>AsymSAT trained on SR(3..10)</td><td>45.00%</td><td>60.00%</td><td>45.00%</td><td>45.00%</td><td>52.50%</td></tr><tr><td>AsymSAT trained on SR(3..10) + V(3..8)</td><td>47.50%</td><td>47.50%</td><td>45.00%</td><td>60.00%</td><td>57.50%</td></tr></table>  

Table 5: Solution rate under different iterations   

<table><tr><td># of iterations</td><td>5</td><td>10</td><td>15</td><td>20</td></tr><tr><td>AsymSAT w. GRU</td><td>80.63%</td><td>90.32%</td><td>90.25%</td><td>89.11%</td></tr><tr><td>AsymSAT w. LSTM</td><td>79.76%</td><td>90.45%</td><td>93.07%</td><td>91.80%</td></tr></table>  

Table 6: Solution rate for the larger \(SR(n)\) problems   

<table><tr><td></td><td>SR(20)</td><td>SR(40)</td><td>SR(60)</td><td>SR(80)</td></tr><tr><td>AsymSAT</td><td>55.40%</td><td>27.20%</td><td>12.00%</td><td>5.00%</td></tr><tr><td>NeuroSAT</td><td>33.90%</td><td>19.60%</td><td>8.50%</td><td>4.50%</td></tr></table>  

gates will add up to the number of variables and clauses after Tseitin transformation. Therefore, the converted CNF inputs are challenging for NeuroSAT. This explains the poor performance of NeuroSAT in Table 3. We also show the generalizability of AsymSAT on the \(V(n)\) dataset. On larger \(V(n)\) problems, for example, \(V(15)\) , which is about \(128x\) the size of \(V(8)\) , AsymSAT still maintains a solution rate around \(50\%\) . It is not significantly affected by reducing the training set to only the \(SR(n)\) problems as shown by Table 4.  

In summary, our AsymSAT model is capable of breaking the tie in symmetric circuits and it achieves a higher solution rate in comparison with NeuroSAT and DG- DAGRNN on both medium- size CNF problems and large- size Circuit- SAT problems. This shows the effectiveness of using RNN to account for variable dependency in GNN- based SAT solving.