We prepare three datasets in total: the small- scale symmetric circuit examples, medium- size CNF formulas, and large random circuits with more than 1K logic gates.  

Small- scale symmetric AIG with asymmetric solutions. We manually construct 10 circuits with no more than 3 inputs. Within each circuit, there are at least two input nodes that are symmetric but require distinct assignments. We intentionally keep this training set small. If NeuroSAT and DG- DAGRNN are capable of handling symmetric circuits with asymmetric SAT solutions, they should easily reach a high training accuracy on this small dataset. However, our experiment result later will show that they are unable to predict any SAT solutions for this dataset.  

Medium- size randomly generated CNF formulas. We generate random CNF formulas in the same way as described by [Selsam et al., 2018]. We refer to this dataset as the \(SR(n)\) problem, where \(n\) is the number of variables. CNF formula  

for \(SR(n)\) problems can be converted into the circuit form using the principle of Shannon's Decomposition as suggested by [Amizadeh et al., 2018].  

Large randomly generated AIGs. We generate random AIGs using the AIGEN tool [Jacobs and Sakr, 2021], which was designed to create random test circuits to check and profile the EDA tools. By default, AIGEN generates sequential logic circuits (those with storage elements). We extract the combinational logic circuits from the sequential logic circuits. We refer to this dataset as the \(V(n)\) problem, where \(n\) stands for the number of circuit input nodes. \(V(n)\) problems can be converted into CNF using Tseitin transformation. Compared to \(SR(n)\) problems, \(V(n)\) is a nontrivial dataset even when \(n\) is relatively small. For example, each \(V(10)\) problem has more than 1K logic gates on average. The corresponding CNF formulas contain more than 1K variables, which is much larger than the largest dataset \(SR(40)\) used in the prior work [Selsam et al., 2018].