We implemented the BPGAT architecture in Python, leveraging the PyTorch framework [19]. The model is trained to minimize the Mean Squared Error (MSE) between the natural logarithm \(\ln \hat{Z}\) of the true number of models of the input formula, and the output of the model \(\ln \hat{Z}\) .  

BPGAT Training Protocol We trained the model for 1000 epochs using the Adam optimizer [12] with an initial learning rate of \(10^{- 4}\) , halving it every 200 epochs using a learning rate scheduler.  

Given an input formula \(\phi\) with \(n\) variables \(\{x_{i}\}_{i = 1}^{n}\) and \(m\) clauses \(\{f_{j}\}_{j = 1}^{m}\) and its factor graph representation \(G = (V,E)\) , it is preprocessed before being fed to the network in such a way that \(\forall j\in \{1,\ldots ,m\} ,\forall \{x_{1},\ldots ,x_{k}\} \in \mathcal{N}(f_{j}),f_{j}(x_{1},\ldots ,x_{k}) = 1\) for the assignment of \(\{x_{1},\ldots ,x_{k}\}\) that makes clause \(f_{j}\) evaluate to true, 0 otherwise, to ensure that \(Z\) of Equation 1 actually represents the count of models satisfying \(\phi\) .
In order to compute attention coefficients of Equations 11 and 12, we used a 3- layer GAT network, having respectively \(4,4,6\) attention heads. Moreover, we used a 3- layer MLP \(\Delta\) , with ReLU activation between hidden layers, to transform factor- to- variable messages, in place of a fixed- scalar damping parameter. For variable- to- factor messages a damping parameter \(\alpha = 0.5\) has been used. The MLP of the final layer (Equation 9) is a 3- layer feedforward network, with ReLU non- linearity between hidden layers. The number of iterations \(T\) of the message passing scheme has been set to 5. This architectural choices have been made after performing a set of preliminary and ablation studies, whose results can be found in supplementary material, Section C.1.  

BPGAT Training Data The training dataset \(\mathcal{D} = \{(\phi_{i},\ln Z_{i})\}\) consists of a set of 1000 pairs of CNF SAT formulae \(\phi_{i}\) and the logarithm \(\ln Z_{i}\) of their true model count. Such formulae are drawn from a distribution of random formulae built in the following way:  

1. Given input parameters \((n v_{m i n},n v_{m a x})\) , the number of variables of the current formula \(\phi\) is chosen as \(n_{v a r}\sim \mathcal{U}([n v_{m i n},n v_{m a x}])\) . 
2. Given input parameters \((n c_{m i n},n c_{m a x})\) , the number of clauses of \(\phi\) is chosen as \(n_{c l}\sim \mathcal{U}([n c_{m i n},n c_{m a x}])\) . 
3. For each clause \(c_{j}\) , with \(j\in \{1,\ldots ,n_{c l}\}\) , a number \(k\sim 2 + \mathrm{Bernoulli}(0.7)+\) Geometric(0.4) of variables are chosen uniformly at random from the input \(n_{v a r}\) variables; this leads to clause having 5 variables in average. Once the variables for the current clause \(c_{j}\) are chosen, each of them is negated with probability 0.5; before adding \(c_{j}\) to the formula \(\phi\) , it is checked that \(c_{j}\) is different from all the other clauses already in the formula. 
4. As soon as all the \(n_{c l}\) clauses are generated, the resulting formula \(\phi\) is fed to Minisat [10] and checked for satisfiability: it is added to the dataset \(\mathcal{D}\) if and only if it is SAT. 
5. All formulae \(\phi_{i}\) in the dataset \(\mathcal{D}\) are fed to sharpSAT [24], an exact #SAT solver to generate the true count \(Z_{i}\) , and add its logarithm \(\ln Z_{i}\) to the dataset.  

The dataset used for training BPGAT has been generated using \((n v_{m i n},n v_{m a x}) = (10,30)\) and \((n c_{m i n},n c_{m a x}) = (20,50)\) ; this produced formulae having an average of 19.87 variables and 34.87 clauses.  

Fine- tuning Protocol In order to allow the model to extrapolate to data distributions different from the one seen during training, without requiring a large amount of labeled data (as ground- truth labels might be costly to obtain), we use a pre- trained BPGAT as weight initializer for fine- tuning towards new formulae distributions.  

In particular, we initialize the weights of our model using those of BPGAT trained for 500 epochs with the protocol described earlier in this Section and data drawn from the previously proposed random distribution, and we train it for 250 epochs with a learning rate of \(10^{- 6}\) , with only 250 labeled examples.