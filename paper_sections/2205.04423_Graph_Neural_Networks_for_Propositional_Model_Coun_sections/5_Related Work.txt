In [2] a graph neural network model is proposed to solve the weighted disjunctive normal form counting problem (weighted #DNF). Differently from CNF #SAT, there is an approximate algorithm that admits a fully polynomial randomized approximation scheme (FPRAS), which allows the generation of hundreds of thousands of training data points, as opposed to our method which is forced to learn in a limited data regime.  

A quite significant body of work has been recently developed to learn how to solve \(NP\) - complete combinatorial optimization problems leveraging graph neural networks [5]. Among them, methods tackling the Boolean satisfiability problem (SAT) are of particular interest for this work. They are divided into approaches leveraging GNNs as end- to- end solvers [18,22,3] (i.e. trained to output the solution directly from the input instance), and approaches in which the network is used as a computational tool to learn data- driven heuristics, in the loop of modern branch and bound solvers [30,16].  

As already mentioned, our architecture builds upon the model of [15]. Besides the architectural differences that have been underlined in Section 3, the main distinction between the two models resides in the training protocol. Indeed, in [15] training data
consist in a sample of formulae drawn from the benchmarks used to test the architecture. Our model is instead trained on a set of random formulae, which are very fast to obtain, and then eventually fine- tuned towards the specific test distribution. The architectural improvements guarantee a sensibly better performance in terms of scalability and generalizability than the BPNN model, as shown in Section B of the supplementary material. We believe that the attentional layer allows the network to focus on regions of the input formulae which are more significant for the #SAT problem.