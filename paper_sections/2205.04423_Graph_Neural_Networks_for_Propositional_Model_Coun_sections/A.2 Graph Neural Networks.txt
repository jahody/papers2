Graph Neural Networks (GNNs) [21] are a class of deep learning models that natively handle graph- structured data, with the goal of learning, for each vertex \(v\) in the input graph \(G = (V,E)\) , a state embedding \(\mathbf{h}_{v} \in \mathbb{R}^{d}\) , encoding information coming from neighboring nodes. The Message Passing Neural Network (MPNN) model [11] provides a general framework for GNNs, abstracting commonalities between many existing approaches in the literature. As the name suggests, the defining characteristic of MPNNs is that they use a form of neural message passing in which real- valued vector messages are exchanged between nodes (in particular between 1- hop neighboring vertices), for a fixed number of iterations.  

In detail, during each message passing iteration \(t\) in a MPNN, the hidden representation \(\mathbf{h}_{v}^{(t)}\) of each node \(v\) is updated according to the information \(\mathbf{m}_{v}^{(t)}\) aggregated from its neighborhood \(\mathcal{N}(v)\) as:  

\[\begin{array}{r l} & {\mathbf{m}_{v}^{(t + 1)} = \sum_{w\in \mathcal{N}(v)}M_{t}(\mathbf{h}_{v}^{(t)},\mathbf{h}_{w}^{(t)})}\\ & {\mathbf{h}_{v}^{(t + 1)} = U_{t}(\mathbf{h}_{v}^{(t)},\mathbf{m}_{v}^{(t + 1)})} \end{array} \quad (17)\]
where \(M_{t}\) is called message function and \(U_{t}\) is called message update function: both are arbitrary differentiable functions (typically implemented as neural networks).  

After running \(T\) iterations of this message passing procedure, a readout phase is executed to compute the final embedding \(\mathbf{y}_{v}\) of each node \(v\) as:  

\[\mathbf{y}_{v} = R(\{\mathbf{h}_{v}^{(T)}|v\in G\}) \quad (18)\]  

where \(R\) (the readout function) needs again to be a differentiable function, invariant to permutations of the node states.  

Hence, the intuition behind MPNNs is that at each iteration every node aggregates information from its immediate graph neighbors, so as iterations progress each node embedding contains information from further reaches of the graph.  

Our model can be regarded as a MPNN taking as input the factor graph representing the input CNF SAT formula, whose message passing phase is the one detailed in Equations 11 and 12, and the readout step is that of Equation 9. It is worth noting that the distinction between factor and variable nodes makes the input graph heterogeneous. This is reflected into the MPNN architecture by using different update functions for factor and variable node embeddings, hence splitting each message passing iteration into two different phases: first every factor receives messages from its neighboring variables, then every variable receives messages from its neighboring factors.