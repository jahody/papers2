Attention mechanisms, as seen in Section 2.2, are a technique that allows a deep learning model to combine input representations additively, while also enforcing permutation invariance. GNNs endowed with an attention mechanism are a promising research direction in the neuro- symbolic computing scenario [5,13,17], as they might enhance structured reasoning and efficient learning.  

This is the reason for modifying the BPNN architecture by augmenting it with a GAT- style attention mechanism (i.e. analogous to that of Equations 5, 6, 7). In what follows we will refer to our architecture as BPGAT, to underline the fact that it puts together the algorithmic structure of BP and the computational formalism of GATs.  

BPGAT works by taking as input a bipartite factor graph \(G = (V,E)\) , with factor nodes \(\{f_{j}\}_{j = 1}^{m}\) and variable nodes \(\{x_{i}\}_{i = 1}^{n}\) , representing a CNF SAT formula and outputs an approximation \(\ln \hat{Z}\) of the logarithm of the exact number of solutions of the input formula. As for BPNN, messages between nodes are partitioned into factor- to- variable messages and variable- to- factor messages, and computations are performed in the log- space.  

Considering variable- to- factor messages, at each iteration \(k + 1\) every variable node \(x_{i}\) contains the aggregated messages \(\hat{m}_{i \to j}^{(k)}\) received from its neighborhoods at the previous iteration \(k\) , and needs to receive messages from its adjacent clause nodes, and aggregate them. Such aggregation is done using a GAT- style multi- head attention mechanism: as for the GAT attentional layer, the model computes attention coefficients \(\alpha_{ij}\) for every factor node \(f_{j}\) in the neighborhood of the variable node \(x_{i}\) by projecting each message \(\hat{m}_{i \to j}^{(k)} \in \mathbb{R}^{2}\) and variable node hidden representation representation \(\hat{m}_{i \to j}^{(k)} \in \mathbb{R}^{2}\) via a weight matrix \(W \in \mathbb{R}^{2 \times 2}\) , then passing the concatenation of the projected messages through a single- layer feedforward neural network \(\mathbf{a} \in \mathbb{R}^{4}\) , then feeding the results to a LeakyReLU and finally applying softmax to normalize them across the neighborhood \(\mathcal{N}(x_{i})\) of \(x_{i}\) . More formally, attention coefficients are computed as:  

\[\alpha_{ij}^{(k + 1)} = \frac{\exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{i \to j}^{(k)},W\hat{m}_{j \to i}^{(k)})]))}{\sum_{k \in \mathcal{N}(x_{i})} \exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{i \to j}^{(k)},\hat{m}_{k \to i}^{(k)})]))} \quad (10)\]
The new variable- to- factor messages are then computed as a weighted sum of the incoming factor- to- variable messages, using attention coefficients of Equation 10, as:  

\[\hat{m}_{i\rightarrow j}^{(k + 1)}(x_{i}) = \sum_{c\in \mathcal{N}(x_{i})\backslash j}\alpha_{i j}^{(k + 1)}W\hat{m}_{c\rightarrow i}^{(k)}(x_{i}) \quad (11)\]  

In order to stabilize the learning process, the multi- head attention mechanism of Equation 7 is used, by replicating \(K\) times the computations of Equations 10 and 11 and aggregating the results. To perform the aggregation of the outputs of each of the attention heads, concatenation has been used for the internal layers, average for the final one.  

Analogous computations are performed to compute factor- to- variable messages:  

\[\begin{array}{r l} & {\alpha_{j i}^{(k + 1)} = \frac{\exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{j\rightarrow i}^{(k)},\hat{W}\hat{m}_{i\rightarrow j}^{(k)}]))}{\sum_{x_{v}\in \mathcal{N}(f_{j})}\exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{j\rightarrow i}^{(k)})},\hat{W}\hat{m}_{i\rightarrow v}^{(k)}))]}}\\ & {\hat{m}_{j\rightarrow i}^{(k + 1)}(x_{i}) = \mathrm{LSE}_{x_{1},\ldots ,x_{k}\in \mathcal{N}(f_{j})\backslash x_{i}}\left(f_{j}(x_{i},x_{1},\ldots ,x_{k}) + \right.}\\ & {\qquad \left. + \sum_{x_{v}\in \mathcal{N}(f_{j})\backslash x_{i}}\alpha_{j i}^{(k + 1)}W\hat{m}_{v\rightarrow i}^{(k)}(x_{i})\right)} \end{array} \quad (12)\]  

Also for this type of messages, a multi- head attention mechanism is deployed, where the output of each head is concatenated in internal layers and averaged in the final layer.  

Once this message passing phase has run for \(T\) iterations, the readout phase is executed. This consists in applying as final layer the one described in Equation 9, in order to obtain \(\ln \hat{Z}\) , i.e. an approximation of the natural logarithm of the number of models of the CNF SAT formula encoded by the input factor graph \(G\) .