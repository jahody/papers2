This work provides several contributions:  

- We establish the promise of application-specific solvers with performance guarantees, by connecting "hands-on" research from machine learning with theoretical results from computer science.- We introduce a new, theoretically motivated loss function, which we call the Lov√°sz Local loss, which rewards an oracle's exploitation of the local structure of SAT instances. We are not aware of any work in the field whose loss is motivated by known bounds on the resulting solver performance.- We construct and train two oracle-based SLS solvers using an Interaction network [16].- We empirically investigate the ability of these solvers to solve random 3-SAT at varying levels of difficulty (as measured by the common \(\alpha\) -ratio between clauses and variables). Our experiments show significant boosts in performance, with the ML-based solvers solving instances that have, on average, a \(17\%\) higher value of \(\alpha\) and doing so in \(35\%\) fewer steps, with a larger than \(8x\) improvement in the median number of steps.- We show that, in our experiments, providing continuous access to an oracle produces significantly better results (both in number of steps needed and number of instances solved) while an algorithm that only uses the oracle to initialise a candidate, only solves instances in fewer steps and is not able to solve more instances.