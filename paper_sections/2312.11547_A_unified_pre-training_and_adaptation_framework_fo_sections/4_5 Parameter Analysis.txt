In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \(d\) , the number of attention heads \(h\) , the weight of losses \(\lambda\) , and the number of pertaining layers \(L\) . We recorded the performance of different \(d\) , \(h\) , \(\lambda\) , \(L\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \(d\) was greater than or equal to 16. Our
model achieved optimal performance when \(d\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \(k\) was 10. The optimal value of weight \(\lambda\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three.