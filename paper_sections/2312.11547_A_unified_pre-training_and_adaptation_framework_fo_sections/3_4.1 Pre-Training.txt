In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs.  

To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \(x_{1} \lor x_{2} \lor \neg x_{3}\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.
After obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \(\mathbf{X}_{ini}\) and \(\mathbf{C}_{ini}\) into low- dimensional features \(\mathbf{X}^{(0)}\) and \(\mathbf{C}^{(0)}\) ; the bipartite GNN backbone \(\mathrm{Bip - GNN}(\cdot)\) from section 3.3 extracts high- level node features for variables \(\mathbf{X}^{(L)}\) and clauses \(\mathbf{C}^{(L)}\) where \(L\) is the number of layers; and fully connected prediction network \(\mathrm{FC}(\cdot)\) maps node features to their labels.  

To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification:  

\[\mathcal{L}_{c} = \sum_{i = 1}^{n}\mathrm{BCE}(p_{i},y_{i}), \quad (3.14)\]  

where \(y_{i}\in \{0,1\}\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \(p_{i}\in \{0,1\}\) is the prediction of the pre- training model from the prediction network \(\mathrm{FC}(\cdot)\) . It is worth noting that we only consider the classification loss under \(n\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables.