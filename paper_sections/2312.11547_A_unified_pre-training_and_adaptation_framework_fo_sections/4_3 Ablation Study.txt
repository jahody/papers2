In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \(\mathbf{w} / \mathbf{o}\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \(\mathbf{w} / \mathbf{o}\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \(\mathbf{w} / \mathbf{o}\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \(\mathbf{w} / \mathbf{o}\) Att is the model that keeps bipartite GNNs without the attention mechanism. \(\mathbf{w} / \mathbf{o}\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism.  

We reported the \(p\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems.