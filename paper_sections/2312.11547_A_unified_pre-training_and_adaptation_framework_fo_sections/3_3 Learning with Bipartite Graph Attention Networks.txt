In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme:  

\[\begin{array}{r l} & {\mathbf{a}_{v}^{(l)} = \mathrm{AGG}(\mathbf{x}_{u}\mid u\in \mathcal{N}(v)),}\\ & {\mathbf{x}_{v}^{(l + 1)} = \mathrm{COM}(\mathbf{a}_{v}^{(l)},\mathbf{x}_{v}^{(l)}),} \end{array} \quad (3.8)\]  

where \(u\) and \(v\) denote the nodes, \(\mathbf{a}_{v}\) denotes the feature vector accumulated from neighbor nodes, \(\mathbf{x}_{v}^{(l)}\) denotes the features at layer \(l\) and \(\mathbf{x}_{v}^{(0)}\) is the initial attributes, \(\mathrm{AGG}(\cdot)\) and \(\mathrm{COM}(\cdot)\) are the aggregation and combination functions for message passing and feature updating, and \(\mathcal{N}(v)\) denotes the set of neighbors for \(v\) .  

The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes.  

For an input bipartite graph \(\tilde{\mathcal{G}} = (\tilde{\nu}_{x},\tilde{\nu}_{c},\tilde{\mathcal{E}})\) , the attributed matrices of variables and clauses are denoted as \(\mathbf{C}\in \mathbb{R}^{m\times d}\) and \(\mathbf{X}\in \mathbb{R}^{n\times d}\) where \(m\) and \(n\) denote the numbers of clauses and variables, and \(d\) is the dimension of attributes. For symbolic convenience, we use \(v_{x}(i)\) and \(v_{c}(i)\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \(x_{i}\) and \(c_{i}\) and their attributed vectors are denoted as \(\mathbf{x}_{i}\) and \(\mathbf{c}_{i}\) accordingly.  

Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \(\mathbf{X}_{ini}\) and \(\mathbf{C}_{ini}\) .  

The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \(c_{i}\) and its neighbor variable \(x_{j}\) , the layer- wise aggregation from \(x_{j}\) to \(c_{i}\) through attention is represented as:  

\[\begin{array}{r l} & {\alpha_{x_{j}\to c_{i}}^{(l)} = \frac{\exp\left(\langle(\mathbf{w}_{Q}^{(l)}\mathbf{c}_{i}^{(l)}),(\mathbf{w}_{K}^{(l)}\mathbf{x}_{j}^{(l)})\rangle\right)}{\sum_{v_{x}(k)\in\mathcal{N}(v_{c}(i))}\exp\left(\langle(\mathbf{w}_{Q}^{(l)}\mathbf{c}_{i}^{(l)}),(\mathbf{w}_{K}^{(l)}\mathbf{X}_{k}^{(l)})\rangle\right)}}\\ & {\mathbf{c}_{i}^{(l + 1)} = \mathrm{MLP}\left(\mathbf{c}_{i}^{(l)},\sum_{v_{x}(j)\in\mathcal{N}(v_{c}(i))}\alpha_{x_{j}\to c_{i}}^{(l)}(\mathbf{w}_{V}^{(l)}\mathbf{x}_{j}^{(l)})\right),} \end{array} \quad (3.11)\]  

where \(\alpha_{x_{j}\to c_{i}}\) denotes the attention score between clause \(c_{i}\) and variable \(x_{j}\) , \(\mathbf{w}_{Q}^{(l)}\in \mathbb{R}^{d}\) , \(\mathbf{w}_{K}^{(l)}\in \mathbb{R}^{d}\) , and \(\mathbf{w}_{V}^{(l)}\in \mathbb{R}^{d}\) are learnable parameters of weights, \(\mathbf{c}_{i}^{(l)}\) and \(\mathbf{x}_{j}^{(l)}\) are features of \(i\) - th clause and \(j\) - th variable at layer \(l\) , \(\mathcal{N}(v_{c}(i))\) is the set of neighbor variable nodes for clause \(c_{i}\) , \(\langle \cdot ,\cdot \rangle\) is the dot product operation, \(\mathbf{c}_{i}^{(l + 1)}\) is the updated features for clause \(c_{i}\) , and MLP denotes the multi- layer perceptron.  

Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \(x_{i}\) , the layer- wise aggregation from \(c_{j}\) to \(x_{i}\) can be denoted as:  

\[\begin{array}{r l} & {\alpha_{c_{j}\to x_{i}}^{(l)} = \frac{\exp\left(\langle(\tilde{\mathbf{w}}_{Q}^{(l)}\mathbf{x}_{i}^{(l)}),(\tilde{\mathbf{w}}_{K}^{(l)}\mathbf{c}_{j}^{(l)})\rangle\right)}{\sum_{v_{c}(k)\in\mathcal{N}(v_{x}(i))}\exp\left(\langle(\tilde{\mathbf{w}}_{Q}^{(l)}\mathbf{x}_{i}^{(l)}),(\tilde{\mathbf{w}}_{K}^{(l)}\mathbf{c}_{k}^{(l)})\rangle\right)},}\\ & {\mathbf{x}_{i}^{(l + 1)} = \mathrm{MLP}\left(\mathbf{x}_{i}^{(l)},\sum_{v_{c}(j)\in\mathcal{N}(v_{x}(i))}\alpha_{c_{j}\to x_{i}}^{(l)}(\tilde{\mathbf{w}}_{V}^{(l)}\mathbf{c}_{j}^{(l)})\right),} \end{array} \quad (3.12)\]
<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(Â·)), and fully connected classification layers \(\mathrm{FC}(\cdot)\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center>  

where \(\alpha_{c_{j}\to x_{i}}\) denotes the attention score between clause \(c_{j}\) and variable \(x_{i}\) , \(\tilde{\mathbf{w}}_{Q}^{(l)}\) , \(\tilde{\mathbf{w}}_{K}^{(l)}\) , and \(\tilde{\mathbf{w}}_{V}^{(l)}\) are parameters, and \(\mathcal{N}(v_{x}(i))\) is the neighbor set of clause nodes for variable \(x_{i}\) .  

After the two steps of aggregation, we can obtain the updated features of variables \(\mathbf{X}^{(L)}\) and clauses \(\mathbf{C}^{(L)}\) where \(L\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone.