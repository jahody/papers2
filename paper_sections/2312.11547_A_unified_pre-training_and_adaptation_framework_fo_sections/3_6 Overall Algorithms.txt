Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes.  

The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1.  

Algorithm 1 Problem Transfer via Max- SAT  

Input: A CO graph \(\mathcal{G} = (\mathcal{V},\mathcal{E})\)  

Objective function: max \(f(S)\)  

Constraint condition: \(g_{i}(S)\leqslant b_{i},b_{i}\in \Omega\)  

Output: A bipartite graph \(\tilde{\mathcal{G}} = (\tilde{\mathcal{V}}_{x},\tilde{\mathcal{V}}_{c},\tilde{\mathcal{E}})\)  

#From a graph to Max- SAT clauses#  

1: Generate soft clauses \(C_{s}\) with objective function max \(f(S)\)  

2: Generate hard clauses \(C_{h}\) with constraint condition \(g_{i}(S)\leqslant b_{i},b_{i}\in \Omega\)  

#From Max- SAT clauses to a bipartite graph#  

3: Construct nodes \(\tilde{\nu}_{x}\) and \(\tilde{\nu}_{c}\) for each variable \(v_{i}\) and clause \(c_{j}\)  

4: Construct edges \(e_{(v_{i},c_{j})}^{\prime}\in \tilde{\mathcal{E}},\forall v_{i}\in c_{j}\)  

The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2.  

The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3.