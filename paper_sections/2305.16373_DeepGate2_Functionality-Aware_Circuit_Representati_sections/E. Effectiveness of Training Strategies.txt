To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural  

correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137.  

We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \((L_{prob})\) and identifying reconvergence structures \((L_{rc})\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \(L_{func} = 0.0594\) , which is \(51.47\%\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \(L_{func}\) value of 0.0594, which is \(51.47\%\) smaller than that of the latter.