1) Dataset Preparation: We use the circuits in DeepGate [7], which are extracted from ITC'99 [23], IWLS'05 [24], EPFL [25] and OpenCore [26]. These circuits consists of 10,824 AIGs with sizes ranging from 36 to 3,214 logic gates. To obtain the incomplete truth table, we generate 15,000 random patterns and record the corresponding response. Following the data preparation method described in Section III-B, we construct a dataset comprising 894,151 node pairs. We create 80/20 training/test splits for model training and evaluation.
<center>Fig. 4. Functionality-aware circuit learning framework </center>  

2) Evaluation Metrics: We assess our DeepGate2 with two tasks. The first task is to predict the logic probability for each logic gate. We calculate the average prediction error (PE) as Eq. (13), where the set \(\mathcal{V}\) includes all logic gates.  

\[PE = \frac{1}{|\mathcal{V}|}\sum_{i\in \mathcal{V}}|P_{i} - \hat{P}_{i}| \quad (13)\]  

The second task is to identify the logic equivalence gates within a circuit. A gate pair \((i,j)\) is considered as a positive pair if these two logic gates \(i\) and \(j\) have the same function, where the pairwise Hamming distance of truth tables \(D_{(i,j)}^{T} = 0\) . If the similarity \(S_{(i,j)}\) between these two embedding vectors \(h f_{i}\) and \(h f_{j}\) exceeds a certain threshold, the model will recognize the gate pair \((i,j)\) as equivalent. The optimal threshold \(\theta\) is determined based on the receiver operating characteristic (ROC). The evaluation metrics is formally defined in Eq. (14), where \(TP\) , \(TN\) , \(FP\) , \(FN\) are true positive, true negative, false positive and false negative, respective, and \(M\) is the total number of gate pairs. In the following experiments, the performance on logic equivalence gates identification is measured in terms of Recall, Precision, F1- Score and area under curve (AUC).  

\[\begin{array}{rl} & {TP = \frac{\sum((D_{(i,j)}^{T} = =0)\&(S_{(i,j)} > \theta))}{M}}\\ & {TN = \frac{\sum((D_{(i,j)}^{T} > 0)\&(S_{(i,j)}< \theta))}{M}}\\ & {FP = \frac{\sum((D_{(i,j)}^{T} > 0)\&(S_{(i,j)} > \theta))}{M}}\\ & {FN = \frac{\sum((D_{(i,j)}^{T} = =0)\&(S_{(i,j)}< \theta))}{M}} \end{array} \quad (14)\]  

We conduct the following performance comparisons on 10 industrial circuits, with circuit sizes ranging from \(3.18k\) gates to \(40.50k\) gates.  

3) Model Settings: In the one-round GNN model configuration, the dimension of both structural embedding \(hs\) and functional embedding \(hf\) is 64. Both \(MLP_{prob}\) and \(MLP_{rc}\) contain 1 hidden layer with 32 neurons and a ReLu activation function.  

The model is trained for 60 epochs to ensure each model can converge. The other models [7], [8] mentioned in the following experiments maintain their original settings and are trained until they converge. We train all the models for 80 epochs with batch-size 16 on a single Nvidia V100 GPU. We adopt the Adam optimizer [27] with learning rate \(10^{- 4}\) and weight decay \(10^{- 10}\) .  

TABLE I PERFORMANCE OF DEEPGATE (V1) AND OUR PROPOSED DEEPGATE2 (V2) ON LOGIC PROBABILITY PREDICTION   

<table><tr><td rowspan="2">Circuit</td><td rowspan="2">#Gates</td><td colspan="2">DeepGate</td><td colspan="2">DeepGate2</td><td colspan="2">Reduction</td></tr><tr><td>PE</td><td>Time</td><td>PE</td><td>Time</td><td>PE</td><td>Time(x)</td></tr><tr><td>D1</td><td>19,485</td><td>0.0344</td><td>36.89s</td><td>0.0300</td><td>2.23s</td><td>12.79%</td><td>16.56</td></tr><tr><td>D2</td><td>12,648</td><td>0.0356</td><td>16.11s</td><td>0.0309</td><td>1.18s</td><td>13.20%</td><td>13.66</td></tr><tr><td>D3</td><td>14,686</td><td>0.0355</td><td>21.42s</td><td>0.0294</td><td>1.44s</td><td>17.18%</td><td>14.92</td></tr><tr><td>D4</td><td>7,104</td><td>0.0368</td><td>5.89s</td><td>0.0323</td><td>0.50s</td><td>12.23%</td><td>11.89</td></tr><tr><td>D5</td><td>37,279</td><td>0.0356</td><td>131.22s</td><td>0.0316</td><td>7.82s</td><td>11.24%</td><td>16.79</td></tr><tr><td>D6</td><td>37,383</td><td>0.0325</td><td>133.02s</td><td>0.0285</td><td>8.10s</td><td>12.31%</td><td>16.42</td></tr><tr><td>D7</td><td>10,957</td><td>0.0357</td><td>13.02s</td><td>0.0316</td><td>0.90s</td><td>11.48%</td><td>14.44</td></tr><tr><td>D8</td><td>3,183</td><td>0.0406</td><td>1.60s</td><td>0.0341</td><td>0.17s</td><td>16.01%</td><td>9.64</td></tr><tr><td>D9</td><td>27,820</td><td>0.0368</td><td>77.20s</td><td>0.0322</td><td>4.73s</td><td>12.50%</td><td>16.34</td></tr><tr><td>D10</td><td>40,496</td><td>0.0327</td><td>154.00s</td><td>0.0290</td><td>8.89s</td><td>11.31%</td><td>17.33</td></tr><tr><td>Avg.</td><td></td><td>0.0356</td><td>59.04s</td><td>0.0310</td><td>3.59s</td><td>13.08%</td><td>16.43</td></tr></table>