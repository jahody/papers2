DeepGate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. DeepGate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the DeepGate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability.  

<center>Fig. 1. An example of reconvergence structure </center>  

First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. DeepGate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, DeepGate cannot differentiate the functional difference between two circuits if they have the same probability.  

Second, DeepGate is not efficient enough to deal with large circuit. Specifically, DeepGate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in DeepGate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \(h_i\) is the embedding vector of node \(i\) . Since, DeepGate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation.  

We emphasize that the limitations of DeepGate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision.