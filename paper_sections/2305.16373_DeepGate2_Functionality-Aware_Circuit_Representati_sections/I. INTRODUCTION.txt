The application of Deep Learning (DL) techniques in Electronic Design Automation (EDA) has attracted a lot of attention, including routing [1], [2], synthesis [3], [4], and testing [5], [6]. Among learning- based EDA solutions, circuit representation learning [7]- [12] has emerged as a promising research paradigm. This paradigm adopts a two- step approach instead of training individual models from scratch for each EDA task. First, a general model is pre- trained with task- agnostic supervision. Then, the model is fine- tuned for specific downstream tasks, resulting in improved performance.  

One representative technique, called DeepGate, embeds both the logic function and structural information of a circuit as vectors on each gate. DeepGate uses signal probabilities as the supervision task and applies an attention- based graph neural network (GNN) that mimics the logic computation procedure for learning. It has achieved remarkable results on testability analysis [5] and SAT solving problems [13]. However, we argue that its capabilities can still be dramatically enhanced.  

On one hand, DeepGate falls short in circuit functionality supervision. Generally speaking, the circuit truth table represents functionality in its most direct form. However, obtaining a complete truth table through exhaustive simulation is infeasible due to the exponentially increasing time requirement in relation to the number of primary inputs (PIs). DeepGate, as an alternative, employs the ratio of logic- 1 in the truth table as a functionality- related supervision metric. It then approximates this value as the probability of logic- 1 under randomized simulations performed a limited number of times. This approach, however, has a notable caveat. For example, a NOT gate and its fan- in gate can both have a logic- 1 probability of 0.5, but their truth tables are entirely opposite, thus highlighting the inadequacy of this supervision method.  

On the other hand, DeepGate assigns the same initial embedding to all the PIs. Although these homogeneous embeddings reflect the equal logic probability of 0.5 for each PI under random simulation, they do not offer unique identifiers for individual PIs. Consequently, the model lacks the capacity to discern whether gates are reconvergent and driven by common PIs, information that is vital for circuit analysis. To preserve the logical correlation of gates, the model must execute multiple forward and backward propagation rounds to compensate for the absence of PI identification, which it achieves by revealing differences in local structure. Nevertheless, a model that involves many rounds of message- passing is time- consuming and inefficient, particularly when dealing with large circuits.   

In response to these challenges, we present DeepGate2, an innovative functionality- aware learning framework that notably advances the original DeepGate solution in both learning effectiveness and efficiency. Specifically, we incorporate the pairwise truth table difference of logic gates as supplementary supervision. This involves obtaining an incomplete truth table via rapid logic simulation, and then calculating the Hamming distance between the truth tables of two logic gates, referred to as the 'pairwise truth table difference'. Subsequently, we construct a functionality- aware loss function with the following objective: to minimize the disparity between the pairwise node embedding distance in the embedding space and the pairwise truth table difference, which serves as the ground truth. As a result, our proposed supervision introduces authentic functional information, a stark contrast to the initial DeepGate model, which predominantly depended on statistical facets of functionality.  

Moreover, we introduce a single- round GNN architecture that efficiently encapsulates both structural and functional characteristics. Our GNN segregates the node embeddings into two discrete elements: functional embeddings and structural embeddings, each initialized differently. For the initial functional embeddings of the PIs, we assign a uniform vector to denote the equal logic probability shared among all PIs. For the initial structural embeddings of the PIs, we allocate a set of orthogonal vectors. These vectors, unique and uniformly spaced apart in the embedding space, mirror the functional independence of each PI. By transmitting these PI embeddings to the internal logic gates through two separate aggregation streams, our model effectively amalgamates both structural and functional data through a singular round of forward message- passing procedure.  

We execute a range of experiments to highlight the effectiveness and efficiency of our proposed circuit representation learning framework. When compared to the predecessor, DeepGate, our model, DeepGate2, demonstrates a significant accuracy improvement and achieves an order of magnitude speedup in logic probability prediction. To further demonstrate the generalization capability of DeepGate2 in critical downstream applications that heavily rely on circuit functionality, we integrate it into EDA tools to aid logic synthesis [14] and SAT solving [15]. The experimental results further validate the efficacy of our DeepGate2 framework.  

The remainder of this paper is organized as follows. Section II surveys related work. We then detail the proposed DeepGate2 framework
in Section III. We compare DeepGate2 with the original DeepGate and another functionality- aware solution [8] in Section IV. Next, we apply DeepGate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper.