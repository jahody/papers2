The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous DeepGate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space.  

We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, DeepGate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by DeepGate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between
<center>Fig. 2. Problem statement: the embedding vectors should be close if circuit functions are similar </center>  

A and B is equal to the distance between A and D. Nonetheless, as indicated in the truth table, Circuit A is equivalent to Circuit C, similar to Circuit B (with only 2 different bits), but dissimilar to Circuit D (with 5 different bits).  

We expect that the model will bring together or separate the circuits in embedding space according to their truth tables. Therefore, the expected DeepGate2 model not only identifies the logic equivalent nodes, but also predicts the functional similarity. Thus, we can apply such functionality- aware circuit learning model to provide benefits for the real- world applications.