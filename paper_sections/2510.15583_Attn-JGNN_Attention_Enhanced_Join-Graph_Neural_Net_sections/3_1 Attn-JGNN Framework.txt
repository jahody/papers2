For a given Conjunctive Normal Form (CNF) formula, we first encode it as a factor graph: an edge is established between a variable \(x_i\) and a clause \(C_j\) if \(x_i\)
appears in \(C_{j}\) , We then use an external tree decomposition tool to decompose this factor graph into a join- graph (consistent with the definition in Section 2.2), generating a set of clusters \(\{C_{1}, C_{2}, \ldots , C_{k}\}\) . Each cluster contains variables and clauses that form a local substructure (see Fig. 2). At the input layer, we initialize two types of features: \(h_{v}\) and self- identifying node feature \(h_{\phi}\) . The core architecture of the Attn- JGNN consists of two GAT layers(denoted GAT1 and GAT2), one MLP layer, and one pooling layer. \(GAT1\) and \(GAT2\) are cyclically invoked during message passing until convergence. \(GAT1\) is responsible for local variation- clause message passing, \(GAT2\) is responsible for cross- cluster message passing, and aggregates messages through splicing- pooling operation. Finally, \(b_{i}(C_{i})\) and \(b_{i}(x_{i})\) are estimated by the MLP layer to output the final number of models. The design of the architecture is in line with the iterative nature of the IJGP algorithm, that is, local first, then global, and the results within a cluster directly affect the propagation weight between clusters.  

<center>Fig. 2: In the picture A,B,C... representing variables(Clause nodes are hidden, and clause nodes cannot appear on edges),the shared variables between the two clusters act as edge-lable. the same factor graph can be decomposed into different tree decomposition forms, figure (a) shows a low tree width but with poor accuracy, while figure (b) shows a high tree width, featuring high complexity but high accuracy </center>