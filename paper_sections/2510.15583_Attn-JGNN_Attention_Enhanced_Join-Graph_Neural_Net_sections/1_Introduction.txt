Given a propositional formula, the model counting problem (#SAT) aims to compute the number of satisfying assignments. As a fundamental problem in computer science, model counting has a wide range of practical applications, including probabilistic inference [26, 9], probabilistic databases [12], probabilistic programming [16], neural network verification [4], network reliability [4]. However, most of these model counting problems are #P- hard [34], posing significant challenges to computation. Although the scalability of exact model counters has been substantially improved, the inherent difficulty of this problem remains unchanged. Consequently, researchers have turned to exploring approximate methods to address model counting problems in real- world scenarios. The state- of- the- art approximate counting methods, such as ApproxMC [6], satss [17], STS [15], PartialKC [21], have improved computational efficiency, yet they usually require invoking external SAT solvers in practical applications. Since the SAT problem itself is NP- hard, how to further enhance computational efficiency and scalability remains a major challenge in this field.  

With neural networks demonstrating excellent learning abilities, various machine learning especially deep learning methods have been proposed for proposition model count [33, 25, 35], including independent neural solver, directly predict the satisfaction of a given task distribution in implementing occuring [2, 3]. Another research focus is to construct a general neural network framework by
learning the approximate values of the partition function in statistical physics as an approximate #SAT solver. This general network framework usually relies on propagation algorithms such as belief propagation algorithms. When the propagation algorithm converges, it corresponds to the critical point of Bethe free energy. The iterative process of the propagation algorithm is the process of finding the extreme point of bethe free energy [8]. Our work is based on this framework.  

A recent work, NSNet [23], a general graph neural network framework, describes the satisibility problem as a probabilistic reasoning problem on the graph, relying only on simple belief propagation (BP) as the message update rule in the latent space, and estimates the partition function to complete the approximate prediction. Encouraging results were shown on the #SAT question. However, although the BP algorithm is accurate in the tree structure, it inevitably generates repetitive messages when facing complex loop structures, resulting in NSNet being able to handle only specific graph structures and the solution accuracy being limited by the BP algorithm.  

Another kind of approximate model counter BPGAT [27] by extending the BPNN architecture [18], by introducing mechanism of attention, give important variables or higher weights of clause, Thereby improving the accuracy of understanding. However, due to the huge overhead brought by the global attention mechanism, it has not shown a very good effect on large- scale tasks, which is also limited by the graph structure.  

To solve the above problems, this paper proposes to use the Iterative join- graph Propagation (IJGP) [11] algorithm combined with the attention mechanism [5, 37] to solve the #SAT problem, which is called Attention Enhanced Join- Graph Propagation (Attn- JGNN). The IJGP algorithm is an approximate reasoning algorithm for probabilistic graphical models (such as Bayesian networks and Markov networks), aiming to effectively calculate the marginal probability or conditional probability of variables. The key idea is to approximate the precise solution by constructing a simplified join- graph and iteratively passing local messages. Compared with BP, IJGP can flexibly control the structure of the graph and the message- passing strategy by controlling the tree width of tree decomposition.  

We put the relevant variables and clause nodes into a clustering structure, connect different clusters through marked edges to form a join- graph, and apply the attention mechanism in each cluster of the join- graph to achieve a hierarchical effect. The Attn- JGNN model parameterizes the IJGP in the latent space through GNN and simulates its message update using the attention mechanism. IJGP avoids the repeated transmission of messages on the ring through edge marking, and its unique tree decomposition structure also enables us to better introduce the attention mechanism, thereby reducing the time complexity by an order of magnitude. Finally, similar to the previous framework, learn the partition function to approximately estimate the number of models.  

Specifically, in view of the hierarchical structure differences in message passing within and between clusters, we adopt a hierarchical structure where two
attention layers are respectively responsible for message passing within and between clusters to improve the solution efficiency. We added a constraining awareness module in the loss function in the form of a regularization term, which prioritizes easily satisfied clauses and penalizes variable assignments that violate the constraints. Meanwhile, a dynamic attention mechanism is adopted. By dynamically increasing or decreasing the number of attention heads along with the time step, the training speed is improved and the resource consumption is reduced.  

In the ablation experiment, we proved that the above three improvements were effective. And IJGP is significantly superior to the BP algorithm. The experimental results on the BIRD and SATLIB benchmark datasets show that, with RMSE as the metric, compared with NSNet and BPGAT, the solution accuracy of Attn- JGNN has increased by \(31\%\) and \(45\%\) respectively.  

This paper constructs a neural network framework Attn- JGNN. This framework applies the hierarchical attention mechanism to the join- graph of the IJGP algorithm and optimizes the framework through two methods: constraint awareness and dynamic trimming of the attention head. It breaks through the limitations of the graph structure imposed by traditional propagation algorithms and is more efficient when combined with attention.