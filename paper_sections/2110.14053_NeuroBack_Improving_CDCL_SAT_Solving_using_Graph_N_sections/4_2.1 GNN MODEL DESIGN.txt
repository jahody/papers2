The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, GraphTrans Wu et al. (2021). However, in our particular SAT application context, GraphTrans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task.  

To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph.  

Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \(L\) stacked GNN blocks, a transformer subnet with \(M\) GSA transformer blocks and \(N\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, ViT- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency.