This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer.  

Preliminaries of SAT In SAT, a propositional logic formula \(\phi\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \((\wedge)\) of clauses. Each clause is a disjunction \((\vee)\) of literals. A literal is either a variable \(v\) , or its complement \(\neg v\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \(\phi = (v_{1} \vee \neg v_{2}) \wedge (v_{2} \vee v_{3}) \wedge v_{2}\) consists of three clauses \(v_{1} \vee \neg v_{2}\) , \(v_{2} \vee v_{3}\) and \(v_{2}\) ; four literals \(v_{1}\) , \(\neg v_{2}\) , \(v_{2}\) and \(v_{3}\) ; and three variables \(v_{1}\) , \(v_{2}\) , and \(v_{3}\) . One satisfying assignment of \(\phi\) is \(v_{1} = 1\) , \(v_{2} = 1\) , \(v_{3} = 0\) . The goal of a SAT solver is to check if a formula \(\phi\) is sat or unsat. A complete solver either outputs a satisfying assignment for \(\phi\) , or proves that no such assignment exists.
The backbone of a sat formula is the set of literals that are true in all its satisfying assignments. Thus backbone variables are the variables whose phases remain consistent across all satisfying assignments. In the given CNF formula \(\phi\) , there are two backbone variables, \(v_{1}\) and \(v_{2}\) , both maintaining a phase of 1 in all satisfying assignments of \(\phi\) .  

CDCL Algorithm CDCL makes SAT solvers efficient in practice and is one of the main reasons for the widespread use of SAT applications. The general idea of CDCL algorithm is as follows (see Marques- Silva et al. (2021) for details). First, it picks a variable on which to branch and decides a phase to assign to it with heuristics. It then conducts a Boolean propagation based on the decision. In the propagation, if a conflict occurs (i.e., at least one clause is mapped to 0), it performs a conflict analysis; otherwise, it makes a new decision on another selected variable. In the conflict analysis, CDCL first analyzes the decisions and propagations to investigate the reason for the conflict, then extracts the most relevant wrong decisions, undoes them, and adds the reason to its memory as a learned lesson (encoded in a clause called learned clause) in order to avoid making the same mistake in the future. After conflict analysis, the solver backtracks to the earliest decision level where the conflict could be resolved and continues the search from there. The process continues until all variables are assigned a phase (sat), or until it learns the empty clause (unsat).  

Phase Selection Heuristics As indicated above, CDCL SAT solving mainly relies on two kinds of variable related heuristics: variable branching heuristics and phase selection heuristics, which are orthogonal to each other. Phase Saving Pipatsrisawat & Darwiche (2007) is a prevalent phase selection heuristic in modern CDCL solvers. It returns a variable's last assigned polarity, either through decision or propagation. This heuristic addresses the issue of solvers forgetting prior valid assignments due to non- chronological backtracking. Rephasing Biere & Fleury (2020); Fleury & Heisinger (2020) is more recent phase selection heuristic, proposed to reset or modify saved phases to diversify the exploration of the search space. The state- of- the- art CDCL solver, Kissat Biere & Fleury (2022), incorporates the latest advancements in phase saving and rephrasing heuristics.  

GNN GNNs Wu et al. (2020); Zhou et al. (2020) are a family of neural network architectures that operate on graphs Gori et al. (2005); Scarselli et al. (2008); Gilmer et al. (2017); Battaglia et al. (2018). Typical GNNs follow a recursive neighborhood aggregation scheme called message passing Gilmer et al. (2017). Formally, the input of a GNN is a graph defined as a tuple \(G = (V, E, W, H)\) , where \(V\) denotes the set of nodes; \(E \subseteq V \times V\) denotes the set of edges; \(W = \{W_{u,v} | (u, v) \in E\}\) contains the feature vector \(W_{u,v}\) of each edge \((u, v)\) ; and \(H = \{H_v | v \in V\}\) contains the feature vector \(H_v\) of each node \(v\) . A GNN maps each node to a vector- space embedding by updating the feature vector of the node iteratively based on its neighbors. For each iteration, a message- passing layer \(\mathcal{L}\) takes a graph \(G = (V, E, W, H)\) as an input and outputs a graph \(G' = (V, E, W, H')\) with updated node feature vectors, i.e., \(G' = \mathcal{L}(G)\) . Classic GNN models Gilmer et al. (2017); Kipf & Welling (2016); Hamilton et al. (2017) usually stack several message- passing layers to realize iterative updating. Prior work on utilizing GNNs to improve CDCL SAT solving will be reviewed in the next section.  

Graph Transformer Architecture Transformer Vaswani et al. (2017) is a family of neural network architectures for processing sequential data (e.g., text or image), which has recently won great success in nature language processing and computer vision. Central to the transformer is the self- attention mechanism, which calculates attention scores among elements in a sequence, thereby allowing each element to focus on other relevant elements in the sequence for capturing long- range dependencies effectively. Recent research Min et al. (2022); Wu et al. (2021); Mialon et al. (2021); Rong et al. (2020) has shown that combining the transformer with GNN results in competitive performance for graph and node classification tasks, forming the Graph Transformer architecture. Notably, GraphTrans Wu et al. (2021) is a representative model which utilizes a GNN subnet consisting of multiple GNN layers for local structure encoding, followed by a Transformer subnet with global self- attention to capture global dependencies, and finally incorporates a Feed Forward Network (FFN) for classification. The GNN model design in NeuroBack is inspired by GraphTrans.