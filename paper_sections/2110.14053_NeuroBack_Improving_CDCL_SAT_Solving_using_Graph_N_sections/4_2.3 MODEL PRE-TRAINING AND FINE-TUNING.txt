The NeuroBack model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable
<table><tr><td>DataBack-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>DataBack-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># BackboneVar</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># BackboneVar</td><td>48,266 (23%)</td></tr></table>  

Table 1: Details of DataBack. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set.  

phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5.  

Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, AdamW optimizer Loshchilov & Hutter (2017) with a learning rate of \(10^{- 4}\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer.