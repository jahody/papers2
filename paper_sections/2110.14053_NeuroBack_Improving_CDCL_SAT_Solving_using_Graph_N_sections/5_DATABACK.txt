We created a new dataset, DataBack, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the NeuroBack model. Accordingly, there are two subsets in DataBack: the pre- training set, DataBack- PT, and the fine- tuning set, DataBack- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called CadiBack Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. DataBack includes formulas solved within the time limit with at least one backbone variable.  

We observe that there exists a significant label imbalance in both DataBack- PT and DataBack- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \(f\) with \(n\) backbone variables \(b_{1},\ldots ,b_{n}\) , let \(\mathcal{L}_{f}:\{b_{1},\ldots ,b_{n}\} \to \{1,0\}\) denote the mapping of each backbone variable to its phase. The dual formula \(f^{\prime}\) is obtained from \(f\) by negating each backbone variable: \(f^{\prime} = f[b_{1}\to\) \(\neg b_{1},\ldots ,b_{n}\mapsto \neg b_{n}]\) . The dual \(f^{\prime}\) is still satisfiable and retains the same backbone variables as \(f\) , but with the opposite phases \(\mathcal{L}_{f^{\prime}}(b_{i}) = \neg \mathcal{L}_{f}(b_{i}),i\in \{1,\ldots ,n\}\) . For the given CNF formula example in Section 2 \(\phi = (v_{1}\vee \neg v_{2})\wedge (v_{2}\vee v_{3})\wedge v_{2}\) , having \(v_{1}\) and \(v_{2}\) as its backbone variables with phases \(\{v_{1},v_{2}\} \to \{1\}\) , the dual formula is \(\phi^{\prime} = (\neg v_{1}\vee v_{2})\wedge (\neg v_{2}\vee v_{3})\wedge \neg v_{2}\) , still having \(v_{1}\) and \(v_{2}\) as the backbone variables but with opposite phases \(\{v_{1},v_{2}\} \to \{0\}\) . This data augmentation strategy doubles the size of DataBack with a perfect balance in positive and negative backbone labels. In the rest of the paper, DataBack- PT and DataBack- FT refer to the augmented, balanced datasets.  

DataBack- PT The CNF formulas in DataBack- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St√ºtzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.
<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table>  

Table 2: The performance on the validation set of both pre- trained and fine- tuned NeuroBack models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy.  

Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included DataBack- PT.  

The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. DataBack- PT thus contains a diverse set of formulas.  

DataBack- FT Given that NeuroBack will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, DataBack- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While DataBack- FT is considerably smaller in size compared to DataBack- PT, its individual formulas are distinctly larger than those in DataBack- PT.