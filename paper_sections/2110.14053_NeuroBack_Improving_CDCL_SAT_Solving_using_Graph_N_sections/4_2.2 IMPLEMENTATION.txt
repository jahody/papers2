The current implementation of NeuroBack model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer.  

GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the ViT transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. LayerNorm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \(L = 4\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \(M = 3\) and \(N = 3\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using PyTorch Paszke et al. (2019) and PyTorch Geometric Fey & Lenssen (2019).