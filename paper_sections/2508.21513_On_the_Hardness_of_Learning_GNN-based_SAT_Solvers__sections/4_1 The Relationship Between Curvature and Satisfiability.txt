We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \(\alpha \in [3,5]\) in steps of \(\Delta \alpha = 0.1\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \(\alpha_{c} \approx 4.267\) . Considering its widespread use and downstream performance, we train the NeuroSAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \(2N\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \(N = 256\) , with the results being summarized in Figure 2.  

Our results show that by considering the probability of finding a solution at a given \(\alpha\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \(k\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \(\alpha_{d} \approx 3.927\) [32].
Test- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \(\alpha_{c}\) , a random \(k\) - SAT dataset consisting of mixed \(k\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems.  

We train both a Graph Convolutional Network (GCN)- solver [25] and NeuroSAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory.