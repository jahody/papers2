The results presented in the previous section allow us to proceed with a principled way of understanding performance limitations in GNN- based SAT solvers. This is due to the direct connection between the result in Theorem 3.1 to Theorem 4 of Topping et al. [47], which establishes that "edges with high negative curvature are those causing the graph bottleneck and thus leading to the oversquashing phenomenon". This seminal result states that if the gradients of the message passing functions ( \(\theta\) and \(\phi\) in Equation 1) are bounded, and there exists a sufficiently negatively curved edge compared to the degrees of its endpoints, then the derivative of the learned node representations around that edge vanishes. Intuitively, this can be understood as a difficulty of propagating the information in nodes at a reachable distance due to the fact that the graph structure limits the pathways where
<center>Figure 1: Average graph Ricci Curvature lower bound (a) and Balanced Forman Curvature (b) as a function of \(\alpha\) and \(k\) . Both quantities behave very similarly, both in terms of the smooth transition from flat to negative curvature and their magnitude, especially as \(\alpha \to 0\) and \(\alpha \to \infty\) , in line with the developed theory. An alternative version of this figure displaying the average graph OC in (b) is presented in Figure 4 of the Appendix. Best viewed in color. </center>  

information can travel. Simply stated, nodes in different neighborhoods need to pass all messages through the same edge(s), leading to a difficulty in learning fixed- length representations that can hold information on long range correlations. Formally, for large values of \(k\) , as the clause density \(\alpha \to \infty\) , any infinitesimally small value \(\delta > 0\) could be used in Theorem 4 of Topping et al. [47] such that \(Ric(i,j) \leq - 2 + \delta\) , leading to an exponentially decaying Jacobian of the node representations around \(i \sim j\) . This result leads us to the conclusion that GNN- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to SAT and the hardness of learning representations for long range communication. The interplay between \(k\) and \(\alpha\) in Theorem 3.1 provides additional insights. Indeed, for problems with large values of \(k\) or large values of \(\alpha\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. On the other hand, for large values of \(\alpha\) and relatively small values of \(k\) , the latter becomes crucial in deciding how well a GNN- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \(k\) . We confirm this fact empirically in Section 4.  

The intuition we provide for a more complete understanding of this crucial result is the following: At increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. In this scenario, the GNN will not be able to learn a fixed length representation that can "remember" the information of reachable, but not directly adjacent nodes. This means that the ability to learn a solver is compromised by an oversquashing phenomenon. For large values of \(k\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \(k\) on the BFC. Our theory motivates therefore how increasing values of \(k\) in random \(k\) - SAT would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \(\alpha\) . To visually understand the aforementioned concepts, we can again refer to Figure 1. As the value of \(k\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. The same holds for increasing values of \(\alpha\) , as expected. We provide additional visual depictions of this aforementioned explanation in Appendix A.5, where we plot two input graphs for random 3- SAT at small (Figure 5) and large (Figure 6) \(\alpha\) . In the following section, we will show that our results can be empirically confirmed for different GNN- based solvers.