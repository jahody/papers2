Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \(\alpha\) . Given an input graph \(G\) , we define the heuristics as:  

\[\omega (G) = -\mathbb{E}_{(i\sim j)}[Ric(i,j)]*\mathbb{E}[\alpha ],\quad \omega^{*}(G) = \frac{\omega(G)}{\mathbb{V}_{(i\sim j)}[Ric(i,j)]}, \quad (11)\]  

with the expectations being taken over the edges \(G\) . The averages of both heuristics, which we denote by \(\bar{\omega}\) and \(\bar{\omega}^{*}\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \(G\) is on average \((\omega (g))\) and how much this quantity concentrates \((\omega^{*}(G))\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of NeuroSAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \(\rho_{\bar{\alpha}} = 0.32\) , \(\rho_{\bar{\omega}} = 0.86\) and \(\rho_{\bar{\omega}^{*}} = 0.98\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper.