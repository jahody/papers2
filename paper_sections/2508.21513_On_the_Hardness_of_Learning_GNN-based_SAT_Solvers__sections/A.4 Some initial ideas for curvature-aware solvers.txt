As discussed during the conclusion, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. In this section, we provide some starting points and experiments for simple implementations of curvature aware solvers that could lead to future improvements. The goal of our implementations was to maintain efficiency and rely on straightforward ideas that could lead to performance improvements. For these purposes, we introduce two simple curvature- aware variants of message passing. The first is an adaptation of Ye et al.
Table 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Variation</td><td colspan="4">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan="3">GCN</td><td>Vanilla</td><td>0.510 ± 0.012</td><td>0.180 ± 0.048</td><td>0.470 ± 0.031</td><td>0.650 ± 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 ± 0.018</td><td>0.154 ± 0.017</td><td>0.422 ± 0.013</td><td>0.664 ± 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 ± 0.014</td><td>0.170 ± 0.010</td><td>0.422 ± 0.016</td><td>0.662 ± 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 ± 0.012</td><td>0.176 ± 0.031</td><td>0.416 ± 0.027</td><td>0.654 ± 0.027</td></tr><tr><td rowspan="3">NeuroSAT</td><td>Vanilla</td><td>0.690 ± 0.022</td><td>0.436 ± 0.032</td><td>0.734 ± 0.017</td><td>0.746 ± 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 ± 0.030</td><td>0.436 ± 0.015</td><td>0.742 ± 0.027</td><td>0.758 ± 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 ± 0.020</td><td>0.416 ± 0.046</td><td>0.724 ± 0.022</td><td>0.726 ± 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 ± 0.018</td><td>0.438 ± 0.028</td><td>0.742 ± 0.025</td><td>0.758 ± 0.020</td></tr></table>  

<center>Figure 3: Low dimensional visualization of the literal embeddings produced by NeuroSAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center>  

[49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \(k\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.
<center>(a) Average graph Ricci curvature lower bound (Equation 8) as a function of \(k\) and \(\alpha\) . </center>  

<center>(b) Average graph Ollivier-Ricci curvature as a function of \(k\) and \(\alpha\) . </center>  

Figure 4: Contour plots of the average graph Ricci curvature lower bound (a) and Ollivier- Ricci curvature (b) displaying the changes in average curvature as a function of \(k\) and \(\alpha\) . Both quantities behave similarly, especially as \(\alpha \to 0\) . We can see that in the case of the ORC, the stronger correction towards positive curvature due to the presence of cycles as \(\alpha \to \infty\) is in line with the general theory related to this discretization of the Ricci curvature [5]. Notice the difference between the contours of (b) and Figure 1b for large \(\alpha\) and \(k\) . Best viewed in color.  

<center>Figure 5: Visualization of an easy, in terms of clause density \(\alpha\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very small number of long-range interactions, as can clearly be seen in (b). Furthermore, for such very small \(\alpha\) , the average BFC approaches 0, in line with the developed theory. </center>