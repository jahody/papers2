Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and NeuroSAT solvers in Table 1.
Table 1: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers. By increasing the testing set's curvature at test-time through rewiring, both solvers are able to make big leaps in accuracy, especially in more difficult problems. Reducing the curvature of the problem facilitates long range communication and renders problems easier. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface and absolute improvements in parentheses.   

<table><tr><td>Model</td><td>Variation</td><td>3-SAT</td><td>4-SAT</td><td>Datasets</td><td>SR</td><td>CA</td></tr><tr><td rowspan="2">GCN</td><td>No Rewiring</td><td>0.510 ± 0.012</td><td>0.180 ± 0.048</td><td>0.470 ± 0.031</td><td>0.650 ± 0.016</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.626 ± 0.021 (+0.116)</td><td>0.374 ± 0.045 (+0.194)</td><td>0.696 ± 0.035 (+0.226)</td><td>0.670 ± 0.048 (+0.020)</td><td></td></tr><tr><td rowspan="2">NeuroSAT</td><td>No Rewiring</td><td>0.690 ± 0.022</td><td>0.436 ± 0.032</td><td>0.734 ± 0.017</td><td>0.746 ± 0.018</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.820 ± 0.030 (+0.130)</td><td>0.686 ± 0.029 (+0.250)</td><td>0.902 ± 0.004 (+0.168)</td><td>0.828 ± 0.029 (+0.082)</td><td></td></tr></table>  

Table 2: Average generalization error (1- testing accuracy) over 5 different runs with the NeuroSAT model on SAT benchmark datasets [28]. The error is reported alongside the average clause density \(\bar{\alpha}\) and the curvature-based heuristics \(\bar{\omega}\) and \(\bar{\omega}^{*}\) . Our heuristics display very strong linear correlation with the generalization error, unlike the average clause density, making it possible to predict how hard each benchmark will be for a GNN-based solver.   

<table><tr><td rowspan="2">Problem</td><td rowspan="2">Generalization Error</td><td colspan="3">Hardness Heuristic</td></tr><tr><td>α</td><td>ω</td><td>ω*</td></tr><tr><td>3-SAT</td><td>0.31</td><td>4.59</td><td>4.12</td><td>97.41</td></tr><tr><td>4-SAT</td><td>0.56</td><td>9.08</td><td>9.81</td><td>612.32</td></tr><tr><td>SR</td><td>0.27</td><td>6.09</td><td>5.30</td><td>125.30</td></tr><tr><td>CA</td><td>0.25</td><td>9.73</td><td>6.30</td><td>123.27</td></tr></table>  

This fact leads us to conjecture that the relationship between input data and model performance is prevalent throughout Neural Combinatorial Optimization (NCO) [48], and different architectural designs are necessary for different problems. Furthermore, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. We report results for some straightforward curvature- aware solvers in Appendix A.4, which do not report consistent improvements. A direct avenue for future work that we consider promising in this field is the application of continuous graph diffusion dynamics for learning[11, 21], which generalize the recurrence mechanism.  

Finally, regarding the theoretical aspect, we believe that promising avenues for future work that were not directly addressed here are the characterization of both curvature and topological quantities in distribution and not just in the mean. Another direction that is interesting is making a connection between the various critical points of the clause density and the curvature, such that the well- known phase transitions of SAT can be directly related to novel geometric order parameters.  

Closing Remarks. In conclusion, our results highlight that the limitations of GNN- based SAT solvers cannot be fully understood without considering the geometric properties of the input. Our study presents, to the best of our knowledge, the first attempt at a theoretical understanding of these neural solvers, by establishing a direct connection between their negatively curved graph representations and oversquashing in GNNs. We provide empirical evidence of this connection and verify that it is prevalent on more constrained instances. Beyond SAT, we expect these insights to be valuable for other domains where graph representations of combinatorial problems are employed. Combinatorial Optimization (CO) provides an interesting venue to study the reasoning behavior of NNs, and we hope that this paper makes a case for such studies. In conclusion, we hope that bridging concepts from deep learning, geometry, and physics, will pave the way for principled advances in the design of neural solvers.