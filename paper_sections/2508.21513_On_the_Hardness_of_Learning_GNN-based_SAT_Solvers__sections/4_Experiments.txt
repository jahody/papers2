Experimental Setting. To validate our theory, we perform different experiments on various datasets, the details of which will be provided in the following subsections. The experiments consist in firstly exploring the behavior of a GNN- based solvers and relating it to the input graph BFC. Based on these
<center>Figure 2: (a) Average BFC as a function of \(\alpha\) for random 3-SAT problems with \(N = 256\) . The color is used as a representation for the average solvability of the problems at a given \(\alpha\) by the NeuroSAT model [46], with a group labeled as solvable if \(50\%\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \(\alpha_{c} \approx 4.267\) [32]. The average BFC drops monotonically with \(\alpha\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \(\alpha\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with NeuroSAT as a function of the variance and average of the BFC. Notice how as \(\alpha\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \(\alpha\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center>  

results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using PyTorch [41], PyTorch- Geometric [17] and PyTorch Lightning [15]. The networks were trained for 100 epochs using the AdamW optimizer [29], with learning rate \(\eta = 0.0001\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs.