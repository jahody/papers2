GNNs constitute a flexible tool for learning representations of graph- structured data. Representing the input data in the form of a graph
allows one to encode complex relations and sparsity structures. GNNs then allow to encode inductive biases such as invariance to various transformations [4]. For these reasons, GNNs are frequently used in applications of machine learning to combinatorial optimization [6, 11, 19] where optimization problems are often amenable to graph- based representations.  

Typically, a GNN would enhance a manually designed solver by replacing various decision heuristics with their predictions after being trained either in a supervised or reinforcement learning mode [1, 11]. Another area of research focuses on end- to- end approaches where the GNN is trained to produce the final answer [29]. From a practical point of view, these end- to- end approaches are interesting because they can potentially find more efficient solutions than those proposed by algorithm designers [31].  

As other data- driven algorithms, GNNs used for combinatorial optimization make a trade- off between performance on some restricted subset of inputs and generalization to the whole domain of possible inputs. In the extreme case, the input distribution may be skewed to the extent that the GNN only needs to recognize superficial features of the input graph.  

In this work, we focus on the end- to- end approaches. We demonstrate these improvements with the popular NeuroSAT architecture [29], which has demonstrated the ability to exhibit nontrivial behavior resembling a search in a continuous space, rather than mere classification based on superficial statistics.  

The NeuroSAT Architecture. We demonstrate our enhancement using the NeuroSAT architecture with several simplifications. NeuroSAT is a GNN that operates on an undirected bipartite graph of literals and clauses. In this graph, each literal is connected to clauses that contain this literal. The MP process alternates between two steps that update the representations of clauses and literals, respectively. The embeddings of literals and clauses are updated by two independent LSTMs. The messages from clause to literals are produced with a 3- layer MLP that takes the embeddings of a clause as an input, and similarly in the opposite direction. After several rounds of MP iterations, the vector representation of each literal is passed into another MLP used to produce a vote for the satisfiability of the formula. These votes are averaged across all literals to produce the final prediction. A more detailed description is provided in the Supplementary material S.2.