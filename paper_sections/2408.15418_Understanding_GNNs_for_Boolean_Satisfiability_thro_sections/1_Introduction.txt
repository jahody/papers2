Graph Neural Networks (GNNs) are routinely used in the context of combinatorial problems [6, 11, 19] because graphs are often better suited to represent these problems than other input formats  

for ML models. Very often, the architecture and the loss function are designed by ad hoc decisions and without insight into their suitability. In this paper, we aim to shed light on the mechanism by which GNNs can find approximate solutions to many combinatorial problems. We target Boolean Satisfiability (SAT), which can be seen as a prototypical combinatorial problem known to have many practical applications.  

Previously, GNNs have already been shown to learn to predict the satisfiability status of CNF formulas [29]. Although these learned solutions lag far behind the solvers used in practice in terms of the size of problems they are able to solve, they can be practically leveraged as guiding heuristics [28] inside these solvers and therefore it is desirable to understand the process by which they arrive at their outputs.  

Boolean Satisfiability has an optimization version of the problem called MAX- SAT where the goal is to find an assignment that maximizes the number of satisfied clauses. Obviously, from an optimal solution to MAX- SAT, one can obtain the solution to the decision problem by checking if all clauses are satisfied or not.  

As we demonstrate in this paper, the GNN learns to predict satisfiability by converting the decision problem to the optimization problem. It is then natural to try to interpret the message passing (MP) process of a GNN as an optimization of a continuous relaxation of the original discrete optimization problem. Recently, Kyrillidis et al. [18] demonstrates scenarios where solving a continuous relaxation formulation may provide benefits over solving the formula using standard solvers (i.e., having a better performance for non- CNF formulas). In their case, they do not use any form of learning and design the continuous solver manually. This suggests promising future applications of GNN- based solutions. GNN- based solutions could potentially bring improvements over manually designed continuous solvers because they can adapt to the specifics of a given distribution of problems. Also, having a better understanding of learned solutions (the discovered approximation algorithm) can eventually lead to a confluence of continuous solvers and GNNs, which would be an analog of Physics- Informed Neural Networks that use neural networks to improve PDE or ODE solvers [24] in the context of combinatorial problems.  

When treating the message- passing process of a GNN as an optimization of a continuous relaxation of the original problem, the relaxed variables are in this context represented by high- dimensional vectors. This hints at a possible connection to approximation algorithms based on Semidefinite Programming (SDP). The solving process of an SDP solver can be understood as an incremental optimization of a set of vectors that represent the variables of the problem. After convergence, these vectors are rounded to give a solution to the original discrete problem.
Another way of interpreting the MP process of a GNN is through the lens of Belief Propagation algorithms [23]. In the context of Boolean Satisfiability, Belief Propagation algorithms operate on similar literal- clause factor graphs as a GNN. A particular version called Survey Propagation also sends messages from literals to clauses in the form of 3- dimensional vectors. In the domain of random satisfiability, these algorithms have been proven very effective [3] and their theoretical properties are well- studied.  

In this work, we demonstrate that these connections could give us novel insights into how trained GNNs operate and bring about improvements in terms of their training speed and accuracy. To our knowledge, these connections have not been explored before. We present the following novel contributions:  

We demonstrate several similarities between the empirical behavior of trained GNN and two well- studied approximation algorithms for Boolean Satisfiability. Motivated by these connections, we design a training curriculum that speeds up the training process by an order of magnitude. For a trained model with fixed weights, we propose to sample different initializations of literal embeddings and to apply a decimation procedure (inspired by Belief Propagation). This substantially increases the number of solved problems (problem is considered solved if the network correctly predicts satisfiability and produces a satisfying assignment for satisfiable instances).  

In Section 2, we provide relevant background information; Sections 3 and 4 describe our contribution; Section 5 contains experimental results and is followed by related work in Section 6, conclusion in Section 7 and limitations in Section 8.