An important feature of the NeuroSAT architecture is that the number of MP iterations does not need to be fixed because each round of MP is realized with the same function. In the original paper, the authors demonstrated that the model trained with 26 MP iterations on problems with up to 40 variables was able to generalize to much larger problems with up to 200 variables. This is achieved just by iterating MP for more steps (hundreds or even thousands). Therefore, we can view the architecture as an iterative algorithm with an adaptive number of steps that could depend on the difficulty of the problem. During training, the number of iterations needs to be fixed so that the problems can be solved in batches, but during inference, each problem can run for a different number of steps.  

As was already shown in the original paper, when the problem is satisfiable and the model correctly predicts it, the vectors of literals form two well- separated clusters. Empirically, once the vectors form two well- separate clusters, subsequent updates do not change the vectors significantly. Informally speaking, MP iterations can be viewed as optimization steps of an implicit energy function of the trained model [14]. Unsatisfied clauses should increase the energy and the minimum energy should be achieved when the maximum number of clauses are satisfied. For satisfiable formulas, this occurs when the vectors form two well- separated clusters, which makes the whole process qualitatively similar to the optimization of the SDP relaxation described in Section 2.3. In the experimental section 5 (and in the Supplementary material S.5), we further verify the connection to SDP by visualizing the evolution of the SDP objective evaluated on the NeuroSAT embeddings after each MP round. Figure 4 shows that this objective function increases until it reaches a fixed point.  

Therefore, we can set up a stopping criterion that stops the MP process once the vectors stop to change significantly. This could be viewed as an analog of a convergence threshold of iterative solvers for continuous problems or BP.  

As mentioned in Section 2.3, the number of iterations required is well correlated with the difficulty of the problem. This motivates our curriculum training procedure, which trains the model by incrementally enlarging the training set with bigger problems and increasing the number of MP operations. For each new problem size, the model is trained until a certain accuracy is reached, and after that, larger problems are added to the training set and the number of MP rounds is incremented accordingly. With this procedure and several simplifications of the original model, we achieve almost an order of magnitude faster convergence to the same accuracy as reported in the original paper (85%).  

A similar observation was recently made by Garg et al. [9] in a study of the in- context learning capabilities of a trained transformer. The authors observe that the trained model is able to perform optimization of a loss function (a high- dimensional regression) as the input passes through individual layers. They also experimentally demonstrated that it is possible to significantly accelerate the emergence of this capability if the model is trained incrementally by increasing the dimensionality of the regression. In our case, we also incrementally increase the number of MP iterations together with the number of variables within the formula, which speeds up the training even further.  

In the experimental section 5 (and in the Supplementary material S.6), we also describe an experiment in which we trained NeuroSAT with an SDP- like loss function instead of the classification loss. The trained model reached a smaller accuracy but was not as dependent on the curriculum as the original model, because the MAX- SAT objective gives more information than the 1- bit supervision of SAT.