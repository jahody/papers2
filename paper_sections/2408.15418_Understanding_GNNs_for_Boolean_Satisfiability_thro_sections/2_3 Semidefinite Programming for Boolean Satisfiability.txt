Semidefinite programming (SDP) is a mathematical optimization technique that is primarily used for problems involving positive semidefinite matrices. In SDP, a linear objective function is optimized over a feasible region given by a spectrahedron (an intersection of a convex cone formed by positive semidefinite matrices and an affine subspace) [25]. Along with the broad scope of applications, SDP has also been used to design approximation algorithms for discrete NP- hard problems [10]. This is achieved by lifting variables of a problem to a vector space and optimizing a loss function expressed in terms of these vectors. Here we illustrate this process on a Semidefinite Relaxation of a MAX- 2- SAT problem.  

MAX- 2- SAT is a version of MAX- SAT in which each clause contains at most two literals. The semidefinite relaxation of a MAX- 2- SAT problem can be formulated as follows [13]: To each Boolean variable \(x_{i}\) (where \(i\in \{1,2,\ldots ,n\}\) ), a new variable \(y_{i}\in \{- 1,1\}\) is associated, and an additional variable \(y_{0}\) is introduced (this variable can be understood as representing the value true). By definition, \(x_{i}\) is true if and only if \(y_{i} = y_{0}\) , otherwise, it is false. Using these new variables, we can represent each clause by an expression that is maximized when the clause is satisfied (considering only values in \(\{- 1,1\}\) ). Each expression contains binary products between variables used in the given clause (more on this in the Supplementary material S.2). By summing the expressions of all the clauses in the formula, we obtain a quadratic objective function that gives a maximal value when the maximum number of clauses is satisfied. Therefore, the whole problem may be stated as an integer quadratic program where the constraints restrict the values of the variables to \(\{- 1,1\}\) .  

The SDP relaxation is obtained by lifting each variable \(y_{i}\) to a \((n + 1)\) - dimensional unit vector \(\mathbf{y}_{i}\) . Therefore, the binary products \(y_{i}\cdot y_{j}\) in the objective function are replaced by inner products \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) . This can be compactly represented in matrix form if we substitute each inner product \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) by a scalar \(Y_{ij}\) of a matrix \(Y\) . The fact that these scalars correspond to inner products could be encoded by the restriction to positive- semidefinite matrices \(Y\) . We can thus represent the original MAX- 2- SAT problem as the following SDP:  

Maximize: \(\operatorname {Tr}(WY)\) Subject to: \(Y_{ii} = 1\) for all \(i\in \{0,1,\ldots ,n\}\) \(Y\ge 0,\)  

where Tr denotes the trace of a matrix. Both \(Y\) and \(W\) are \((n+\) \(1)\times (n + 1)\) matrices. Matrix \(W\) is a coefficient matrix of the objective function derived from the clauses. A more detailed derivation is available in the Supplementary material S.1.  

Positive semidefinitness of matrix \(Y\) assures that the matrix can be uniquely factorized as \(Y = Y^{\frac{1}{2}}(Y^{\frac{1}{2}})^T\) . Rows of the matrix \(Y^{\frac{1}{2}}\) are real vectors \(\mathbf{y}_{i}\) for all \(i\in \{0,\ldots ,n\}\) and values in the original matrix \(Y_{ij}\) are their inner products \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) for all \(i,j\in \{0,\ldots ,n\}\) The constraints \(Y_{ii} = 1\) assures that all vectors \(\mathbf{y}_{i}\) lie on \((n + 1)\) dimensional unit sphere.  

The solver for this SDP optimizes the numbers in the matrix \(Y\) but using the factorization, we can possibly visualize what happens with the vectors \(\mathbf{y}_{i}\) . The process starts with random unit vectors which are continuously updated in order to maximize the objective function. If we would further fix the position of the vector \(\mathbf{y}_{0}\) (corresponding to the value true) we would see that the vectors of variables that will be set to true in the final assignment are getting closer to the vector \(\mathbf{y}_{0}\) and the vectors \(\mathbf{y}_{j}\) of variables that will be set to false will be moving away from it so that the inner product \(\langle \mathbf{y}_{0},\mathbf{y}_{j}\rangle\) is close to \(- 1\) . If the formula is satisfiable, the objective function drives the vectors to form two well- separated clusters. However, if only a few clauses could be satisfied at the same time, the vectors would end up being scattered.  

A simple way to round the resulting vectors \((\mathbf{y}_{1},\ldots ,\mathbf{y}_{n})\) and get the assignment for the original Boolean variables is to compute an inner product \(\langle \mathbf{y}_{0},\mathbf{y}_{i}\rangle\) and assign the value according to its sign. It is also possible to assign the values by picking a random separating hyperplane and it can be shown that this rounding gives 0.8785- approximation of the integer program optimum [12]. Similar SDPs can be obtained for different versions of MAX- SAT (with larger clauses). From an empirical observation, the convergence threshold of the SDP solver needs to be decreased significantly compared to MAX- 2- SAT in order to obtain a good approximation for these more complicated versions, which is related to our curriculum training procedure we introduce.  

We mention that the expressions of the clauses reach their maximum at 1 (when a clause is satisfied by the assignment). This means that the whole formula is satisfiable if the objective function achieves a value that is equal to the number of clauses in the formula. Another way to check satisfiability is to plug the obtained solution into the formula and check whether it is satisfied by it. Therefore, we can obtain an incomplete SAT solver from this SDP.  

In Section 4, we empirically demonstrate that the behavior of a trained GNN resembles the optimization process described above. With this intuition, we propose several improvements that lead to faster training time and higher accuracy.