5.3.1 Training Convergence with the Curriculum. To demonstrate the effectiveness of the proposed curriculum, we compare the training process with two baselines. The first is the publicly available implementation of NeuroSAT 4 and the second is our model without curriculum. We stop training each model once it reaches the validation accuracy reported in the original paper (85%). As visible in Figure 3, our model with the curriculum reaches this accuracy in approximately 30 minutes, while the other two baselines need to be trained for several hours. All models were trained on 1 GPU (NVIDIA A100).  

5.3.2 Sampling and Decimation. In Table 1, we show the increase in accuracy due to the enhancements described in Section 4. Together with the results on randomly generated problems, we also show results on three different structured problems whose details are described in the Supplementary material S3.2. The results show a noticeable increase in the number of solved problems for both enhancements (sampling and decimation). For decimation, we use only two passes, which means that if the first application of the
Table 1: Improvements obtained due to the sampling and decimation procedure. We test how many satisfiable problems could be solved with the model. SR(40) is the test set with random problems; the other problems are describe in the Supplementary meterial S3.2. For each problem type, we include the average number of variables. For larger problems, we run the model for more MP iterations. When decimation is used, we count the number of problems solved during the first and the second pass separately. #samples refers to the number of different initializations of literal embeddings.   

<table><tr><td>Problem type</td><td>#SAT problems</td><td>Avg. #var</td><td>First pass</td><td>Second pass</td><td>#MP iterations</td><td>#samples</td><td>Decimation</td><td>Solved</td></tr><tr><td rowspan="3">SR(40)</td><td rowspan="3">5000</td><td rowspan="3">40</td><td>4442</td><td>274</td><td rowspan="3">100</td><td>16</td><td>Yes</td><td>94 %</td></tr><tr><td>3990</td><td>-</td><td>1</td><td>No</td><td>80 %</td></tr><tr><td>4457</td><td>-</td><td>32</td><td>No</td><td>89.1 %</td></tr><tr><td rowspan="3">Latin Squares 9x9</td><td rowspan="3">200</td><td rowspan="3">196.9</td><td>186</td><td>14</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>100 %</td></tr><tr><td>95</td><td>-</td><td>1</td><td>No</td><td>47.5 %</td></tr><tr><td>192</td><td>-</td><td>32</td><td>No</td><td>96 %</td></tr><tr><td rowspan="3">Latin Squares 8x8</td><td rowspan="3">200</td><td rowspan="3">133.5</td><td>196</td><td>1</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>98.5 %</td></tr><tr><td>113</td><td>-</td><td>1</td><td>No</td><td>56.5 %</td></tr><tr><td>197</td><td>-</td><td>32</td><td>No</td><td>98.5 %</td></tr><tr><td rowspan="3">Logical Circuits</td><td rowspan="3">344</td><td rowspan="3">131.1</td><td>319</td><td>0</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>92.7 %</td></tr><tr><td>293</td><td>-</td><td>1</td><td>No</td><td>85.2 %</td></tr><tr><td>319</td><td>-</td><td>32</td><td>No</td><td>92.73 %</td></tr><tr><td rowspan="3">Sudoku 9x9</td><td rowspan="3">200</td><td rowspan="3">245.6</td><td>92</td><td>11</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>51.5 %</td></tr><tr><td>35</td><td>-</td><td>1</td><td>No</td><td>17.5 %</td></tr><tr><td>94</td><td>-</td><td>32</td><td>No</td><td>47 %</td></tr></table>  

GNN did not solve the formula, we fix the variables whose distances to the average vectors were below a threshold, simplify the formula, and then process it with the GNN again. For the second pass, we used only one initialization sample for each decimated formula. Therefore, if the first pass uses 16 samples, the second pass can also produce a maximum of 16 samples. To see the effects of decimation, we show results of runs with the same number of samples in total (32) but without decimation.  

We included results with 3 passes and comparison to BP in the Supplementary materials S.7 and S.8. We note, as should be obvious, that our method cannot certify unsatisfiability.