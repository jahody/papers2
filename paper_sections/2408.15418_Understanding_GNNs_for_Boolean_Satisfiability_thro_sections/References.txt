[1] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research 290, 2 (2021), 405- 421. [2] Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh (Eds.). 2021. Handbook of Satisfiability - Second Edition. Frontiers in Artificial Intelligence and Applications, Vol. 336. IOS Press. 437- 462 pages. https://doi.org/10.3233/FAIA200993 [3] Alfredo Braunstein, Marc Mezard, and Riccardo Zecchina. 2005. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms 27, 2 (2005), 201- 226. [4] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 (2021). [5] Chris Cameron, Rex Chen, Jason Hartford, and Kevin Leyton- Brown. 2020. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 3324- 3331. [6] Quentin Cappart, Didier Chetelat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. 2023. Combinatorial optimization and reasoning with graph neural networks. J. Mach. Learn. Res. 24 (2023), 130- 1. [7] Edmund Clarke, Daniel Kroening, and Flavio Lerda. 2004. A tool for checking ANSI- C programs. In Tools and Algorithms for the Construction and Analysis of Systems: 10th International Conference, TACAS 2004, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2004, Barcelona, Spain, March 29- April 2, 2004. Proceedings 10. Springer, 168- 176. [8] Karlis Freivalds and Sergejs Kozlovics. 2022. Denoising Diffusion for Sampling SAT Solutions. arXiv preprint arXiv:2212.00121 (2022). [9] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. 2022. What can transformers learn in- context? A case study of simple function classes. Advances in Neural Information Processing Systems 35 (2022), 30583- 30598. [10] Bernd Gartner and Jiri Matousek. 2012. Approximation algorithms and semidefinite programming. Springer Science & Business Media. [11] Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. 2019. Exact combinatorial optimization with graph convolutional neural networks. Advances in neural information processing systems 32 (2019). [12] Michel X Goemans and David P Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM) 42, 6 (1995), 1115- 1145. [13] Carla P Gomes, Willem- Jan Van Hoeve, and Lucian Leahu. 2006. The power of semidefinite programming relaxations for max- sat. In International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. Springer, 104- 118. [14] Stephen Gould, Richard Hartley, and Dylan Campbell. 2021. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 3988- 4004. [15] Lukas Kroc, Ashish Sabharwal, and Bart Selman. 2012. Survey propagation revisited. arXiv preprint arXiv:1206.5273 (2012). [16] Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, and Stefano Ermon. 2020. Belief propagation neural networks. Advances in Neural Information Processing Systems 33 (2020), 667- 678. [17] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. 2020. Can Q- learning with graph networks learn a generalizable branching heuristic for a SAT solver? Advances in Neural Information Processing Systems 33 (2020), 9608- 9621. [18] Anastasios Kyrillidis, Anshumali Shrivastava, Moshe Vardi, and Zhiwei Zhang. 2021. FourierSAT: A Fourier expansion- based algebraic framework for solving hybrid boolean constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1552- 1560. [19] Luis C Lamb, Artur Garcez, Marco Gori, Marcelo Prates, Pedro Avelar, and Moshe Vardi. 2020. Graph neural networks meet neural- symbolic computing: A survey and perspective. arXiv preprint arXiv:2003.00330 (2020). [20] Elitza Maneva, Elchanan Mossel, and Martin J Wainwright. 2007. A new look at survey propagation and its generalizations. Journal of the ACM (JACM) 54, 4 (2007), 17- es. [21] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. 2022. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 1- 8. [22] Christos H Papadimitriou. 1994. Computational complexity. Addison- Wesley. [23] Sejun Park and Jinwoo Shin. 2014. Max- Product Belief Propagation for Linear Programming: Applications to Combinatorial Optimization. In Conference on Uncertainty in Artificial Intelligence. https://api.semanticscholar.org/CorpusID: 15136373 [24] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. 2019. Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378 (2019), 686- 707. https://api.semanticscholar.org/CorpusID:57379996 [25] Motakuri Ramana and Alan J Goldman. 1995. Some geometric results in semidefinite programming. Journal of Global Optimization 7, 1 (1995), 33- 50. [26] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. 2023. Pretraining task diversity and the emergence of non- Bayesian in- context learning for regression. arXiv preprint arXiv:2306.15063 (2023). [27] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53- 65. [28] Daniel Selsam and Nikolaj Bjorner. 2019. Guiding high- performance SAT solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings 22. Springer, 336- 353. [29] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT Solver from Single- Bit Supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. OpenReview.net. https://openreview.net/forum?id= HJMc_iA5tm [30] Zhengyuan Shi, Min Li, Sadaf Khan, Hui- Ling Zhen, Mingxuan Yuan, and Qiang Xu. 2022. Satformer: Transformers for SAT solving. arXiv preprint arXiv:2209.00953 (2022). [31] Petar Velickovic and Charles Blundell. 2021. Neural algorithmic reasoning. Patterns 2, 7 (2021). [32] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, and Risto Miikkulainen. 2021. NeuroComb: Improving SAT solving with graph neural networks. arXiv preprint arXiv:2110.14053 (2021). [33] Emre Yolcu and Barnabas Poczos. 2019. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems 32 (2019). [34] Lenka Zdeborova and Florent Krzakala. 2016. Statistical physics of inference: Thresholds and algorithms. Advances in Physics 65, 5 (2016), 453- 552. [35] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. 2020. NLocalSAT: Boosting local search with solution prediction. arXiv preprint arXiv:2001.09398 (2020).  

[12] Michel X Goemans and David P Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM) 42, 6 (1995), 1115- 1145. [13] Carla P Gomes, Willem- Jan Van Hoeve, and Lucian Leahu. 2006. The power of semidefinite programming relaxations for max- sat. In International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. Springer, 104- 118. [14] Stephen Gould, Richard Hartley, and Dylan Campbell. 2021. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 3988- 4004. [15] Lukas Kroc, Ashish Sabharwal, and Bart Selman. 2012. Survey propagation revisited. arXiv preprint arXiv:1206.5273 (2012). [16] Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, and Stefano Ermon. 2020. Belief propagation neural networks. Advances in Neural Information Processing Systems 33 (2020), 667- 678. [17] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. 2020. Can Q- learning with graph networks learn a generalizable branching heuristic for a SAT solver? Advances in Neural Information Processing Systems 33 (2020), 9608- 9621. [18] Anastasios Kyrillidis, Anshumali Shrivastava, Moshe Vardi, and Zhiwei Zhang. 2021. FourierSAT: A Fourier expansion- based algebraic framework for solving hybrid boolean constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1552- 1560. [19] Luis C Lamb, Artur Garcez, Marco Gori, Marcelo Prates, Pedro Avelar, and Moshe Vardi. 2020. Graph neural networks meet neural- symbolic computing: A survey and perspective. arXiv preprint arXiv:2003.00330 (2020). [20] Elitza Maneva, Elchanan Mossel, and Martin J Wainwright. 2007. A new look at survey propagation and its generalizations. Journal of the ACM (JACM) 54, 4 (2007), 17- es. [21] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. 2022. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 1- 8. [22] Christos H Papadimitriou. 1994. Computational complexity. Addison- Wesley. [23] Sejun Park and Jinwoo Shin. 2014. Max- Product Belief Propagation for Linear Programming: Applications to Combinatorial Optimization. In Conference on Uncertainty in Artificial Intelligence. https://api.semanticscholar.org/CorpusID: 15136373 [24] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. 2019. Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378 (2019), 686- 707. https://api.semanticscholar.org/CorpusID:57379996 [25] Motakuri Ramana and Alan J Goldman. 1995. Some geometric results in semidefinite programming. Journal of Global Optimization 7, 1 (1995), 33- 50. [26] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. 2023. Pretraining task diversity and the emergence of non- Bayesian in- context learning for regression. arXiv preprint arXiv:2306.15063 (2023). [27] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53- 65. [28] Daniel Selsam and Nikolaj Bjorner. 2019. Guiding high- performance SAT solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings 22. Springer, 336- 353. [29] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT Solver from Single- Bit Supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. OpenReview.net. https://openreview.net/forum?id= HJMc_iA5tm [30] Zhengyuan Shi, Min Li, Sadaf Khan, Hui- Ling Zhen, Mingxuan Yuan, and Qiang Xu. 2022. Satformer: Transformers for SAT solving. arXiv preprint arXiv:2209.00953 (2022). [31] Petar Velickovic and Charles Blundell. 2021. Neural algorithmic reasoning. Patterns 2, 7 (2021). [32] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan, and Risto Miikkulainen. 2021. NeuroComb: Improving SAT solving with graph neural networks. arXiv preprint arXiv:2110.14053 (2021). [33] Emre Yolcu and Barnabas Poczos. 2019. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems 32 (2019). [34] Lenka Zdeborova and Florent Krzakala. 2016. Statistical physics of inference: Thresholds and algorithms. Advances in Physics 65, 5 (2016), 453- 552. [35] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. 2020. NLocalSAT: Boosting local search with solution prediction. arXiv preprint arXiv:2001.09398 (2020).