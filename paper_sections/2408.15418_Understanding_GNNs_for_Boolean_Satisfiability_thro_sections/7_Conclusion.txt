We uncover a connection between GNNs trained on combinatorial problems and two well- known approximation algorithms, SDP and BP. Using this connection, we enhance their training and inference procedure. In particular, we focus on the well- known NP- complete problem of Boolean Satisfiability (SAT). We introduce a curriculum training procedure, which enables a significantly faster iteration over experiments. Further, we apply a decimation procedure and initial- value sampling, which significantly increase the number of solved problems. For a problem to be considered solved, we not only require the correct prediction whether it is satisfiable or not, but we also require the GNN to produce a satisfying assignment for satisfiable problem instances.  

Even though the enhancements were presented in the domain of Boolean Satisfiability, we believe that they can easily be generalized to other domains where these approximation algorithms are used.
In future work, we plan to explore these similarities more closely and reverse engineer the algorithm learned by the GNN.