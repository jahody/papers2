Wang et al. (2024) suggests using the backbone membership of literals as a supervised training target and then setting variable polarities using the model predictions. We follow their methodology and train a GNN on the literal- level binary classification task using cross- entropy loss. As discussed in Section 3.2, we only train a model for the 3SAT instances and only use the satisfiable problems for training. The backbone of coloring problems is always empty due to the permutation symmetry of the colors, and some distributions, such as CRYPTO, predominantly consist of UNSAT instances.  

When evaluating, we set the polarity of a variable \(x\) as \(p(x) = 0\) if \(p_{\mathrm{backbone}}(-\alpha) > p_{\mathrm{backbone}}(x)\) and \(p(x) = 1\) otherwise. Here, \(p_{\mathrm{backbone}}(\ell)\) is the predicted probability of literal \(\ell\) belonging to the backbone. We further assign variable weights \(w(x)\) under the assumption that correctly assigning backbone literals in early search steps positively affects the runtime. To this end, we apply the transformation from Equation (21) to the mean backbone probability \(\overline{p}_{\mathrm{backbone}}(x) = 0.5(p_{\mathrm{backbone}}(-\alpha) + p_{\mathrm{backbone}}(x))\) to obtain a weight for each variable. Again, we tune the transformation parameter \(\alpha\) for both base solvers on the validation set.
Table 4: Hyperparameters of the supervised models.   

<table><tr><td></td><td>3SAT</td><td>3COL</td><td>CRYPTO</td></tr><tr><td>batch size</td><td>50</td><td>50</td><td>50</td></tr><tr><td>learning rate η</td><td>0.0001</td><td>0.0001</td><td>0.0001</td></tr><tr><td>weight decay</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>epochs</td><td>200</td><td>200</td><td>200</td></tr><tr><td>hidden dim d</td><td>256</td><td>256</td><td>256</td></tr><tr><td>model depth L</td><td>10</td><td>10</td><td>10</td></tr><tr><td>α Glucose</td><td>101</td><td>103</td><td>10-2</td></tr><tr><td>α March</td><td>10-2</td><td>10-2</td><td>101</td></tr></table>