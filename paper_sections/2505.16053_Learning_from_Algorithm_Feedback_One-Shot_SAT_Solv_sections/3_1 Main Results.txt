Table 1 provides the main results for both Glucose and March on our test sets. We observe that GNN- guided training with RLAF consistently accelerates the given base solver. The margin of improvement depends on the base solver and the class of problem instances. For 3SAT(400) problems, RLAF- guidance reduces the mean runtime of Glucose by \(69\%\) and \(41\%\) for satisfiable and unsatisfiable instances, respectively. Similar improvements are observed for satisfiable 3- coloring problems as well as cryptographic instances. For unsatisfiable coloring instances with 600 vertices, the runtime of Glucose is reduced by around \(24\%\) . The smallest margin of improvement is observed for the March solver on unsatisfiable 3SAT instances. While March+RLAF does need fewer decisions on average to solve this class of problems, the runtime is worse, as the small improvement does not compensate for the additional runtime overhead of the GNN forward pass. It is known that lookahead DPLL solvers like March are very strong baselines for unsatisfiable random instances, so this result is not surprising. For more structured problem classes, RLAF is able to accelerate the March solver substantially on both satisfiable and unsatisfiable instances. We emphasize that the wall- clock runtime of the GNN forward pass is included in the runtime measurements with RLAF- guidance. For the instances used here, this runtime is generally around 0.1 seconds or less, which is negligible compared to the solver runtimes on harder problems. We refer to Table 5 in the appendix for extended results that report the GNN overhead in detail. Overall, these results demonstrate that RLAF is able to train GNN- based solver guidance and that relying on comparatively easy problems for efficient training does not prevent the learned policy from generalizing to more complex problems at test time.
<center>Figure 4: Runtimes relative to the base solver Glucose for RLAF and supervised approaches based on Backbones and UNSAT cores. Less is better. </center>