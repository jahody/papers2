Leveraging deep learning in the context of combinatorial optimization (CO) problems has emerged as a major area of research (Cappart et al., 2021) and has been applied to a wide range of problems such as combinatorial graph problems (Khalil et al., 2017), SAT solving (Selsam et al., 2019), Mixed- Integer Programming (Khalil et al., 2022), and Constraint Satisfaction Problems (Tönshoff et al., 2023). Here, we primarily focus on work that aims to enhance SAT solvers with (graph) neural networks. One line of work suggests using predictions of predefined variable properties to guide SAT solver branching heuristics. Selsam and Björner (2019) train a GNN to predict whether variables belong to an UNSAT core. The branching heuristic is then guided by periodically resetting the solver's VSIDS scores to the GNN's predictions, thus making the guidance specific to VSIDS- based CDCL solvers and dependent on careful tuning of the reset frequency. Wang et al. (2024) predict whether literals occur in the backbone of satisfiable formulas and use these predictions to set the polarity of variables. Another line of work explores purely RL- based training for enhancing branching heuristics, eliminating the need for expert supervision. Kurin et al. (2020) uses Q- learning to train GNNs end- to- end as branching policies to minimize solver runtime, and Cameron et al. (2024) propose Monte Carlo Forest Search for guiding early branching decisions in SAT Solvers on UNSAT problems. Both methods require one GNN forward pass per guided branching decision, which creates a significant bottleneck as the GNN usually requires orders of magnitude more runtime than classical branching
# Algorithm 1 DPLL Solver  

1: Input: Formula \(\phi\) 2: function SOLVE \((\phi)\) 3: # Simplify formula 4: \(\phi \leftarrow \mathrm{UNIT - PROPAGATION}(\phi)\) 5: \(\phi \leftarrow \mathrm{PURE - LITERAL - ELIMINATION}(\phi)\) 6: 7: if \(\phi = \emptyset\) : return SAT 8: if \(\emptyset \in \phi\) : return UNSAT 9: 10: # Decide next branching variable 11: \(\ell \leftarrow \mathrm{PICK - LITERAL}(\phi)\) 12: return \(\mathrm{SOLVE}(\phi \land \{\ell \}) \vee \mathrm{SOLVE}(\phi \land \{\neg \ell \})\) 13: end function  

# Algorithm 2 Decision Heuristic  

1: Input: Formula \(\phi\) 2: function PICK- LITERAL \((\phi)\) 3: \(\hat{x}\leftarrow \mathrm{argmax}_{x}\mathrm{SCORE}(x)\) 4: return \(\hat{x}\) if PICK- SIGN \((\hat{x})\) else \(\neg \hat{x}\) 5: end function  

# Algorithm 3 Guided Decision Heuristic  

1: Input: Formula \(\phi\) , Parameters \(\mathcal{W} = (w, p)\) 2: function PICK- LITERAL- GUIDED \((\phi , \mathcal{W})\) 3: \(\hat{x} \leftarrow \mathrm{argmax}_{x} w(x) \cdot \mathrm{SCORE}(x)\) 4: return \(\hat{x}\) if \(p(\hat{x}) = 1\) else \(\neg \hat{x}\) 5: end function  

Figure 1: DPLL SAT solver and branching heuristics. Algorithm 1: A DPLL SAT solver performs backtracking search to solve a given CNF formula \(\phi\) . At each search step, the formula is simplified through unit propagation and pure literal elimination before selecting the next branching literal. Algorithm 2: Branching heuristics are often implemented by choosing the variable that maximizes some hand- crafted scoring function. Algorithm 3: We propose to extend existing branching heuristics by incorporating given variable weights into the branching decisions that scale the associated score of each variable. We additionally choose the sign of each literal according to a provided polarity.  

heuristics. Further related work is proposed by Han (2020), who accelerate cube- and- conquer solvers with supervised learning, Liu et al. (2024), who suggest improving clause deletion heuristics in CDCL SAT solvers with GNNs, and Zhai and Ge (2025), who use RL to speed up parallelized divide- and- conquer solvers.