We are utilizing GRPO as an online RL algorithm to learn the parameters of our policy GNN directly from observed solver costs. As a consequence, we train with the SAT solver in- the- loop and make \(M\cdot N\) calls to the solver per GRPO iteration. With our default parameters ( \(N = 100\) , \(M = 40\) ) we make 4000 SAT solver calls in each iteration. This imposes the practical constraint to train on a distribution \(\Omega\) of SAT problems where this number of solver calls is possible in an acceptable time on the underlying hardware. The work presented here intends to be a small- scale demonstration of RLAF as a training paradigm, and all training is performed on machines with one (multi- core) CPU and one GPU. Therefore, the training data in our experiments is chosen so that each instance is solvable by the given base solvers within a fraction of a second. In future work, the hardness and size of the training problems can be scaled up substantially by leveraging a distributed compute cluster for collecting the SAT solver feedback. Crucially, we demonstrate in Section 3 that after training, the learned policies do generalize to significantly harder and larger problems. The reliance on comparatively easy training problems is therefore not a significant limitation for learning effective GNN- guidance with RLAF.