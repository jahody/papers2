SAT Solving A Boolean formula in Conjunctive Normal Form (CNF) is a conjunction of clauses \(\phi = C_{1} \wedge \dots \wedge C_{m}\) , each clause being a disjunction of one or more literals \(C_{j} = (\ell_{j,1} \vee \dots \vee \ell_{j,k})\) . We denote by \(\mathrm{Var}(\phi) = \{x_{1}, \ldots , x_{n}\}\) the set of Boolean variables of \(\phi\) . The Boolean SAT problem is to decide whether or not there exists a satisfying assignment \(\alpha : \mathrm{Var}(\phi) \to \{0, 1\}\) that satisfies all clauses of a given formula \(\phi\) . This problem is well- known to be NP- complete and naturally arises in a wide range of applications (Biere et al., 2021). Modern SAT solvers predominantly stem from the Davis- Putnam- Logemann- Loveland (DPLL) algorithm, a backtracking search approach enhanced by unit propagation and pure literal elimination. Algorithm 1 provides a pseudocode description of a DPLL SAT solver. Many extensions of this general idea have been proposed to scale SAT solvers to larger, industrial instances. In particular, Conflict- Driven Clause Learning (CDCL) solvers significantly extend the DPLL framework by introducing clause learning and non- chronological backtracking. A common property of DPLL- derived solvers is the importance of the branching heuristic that picks the next branching literal in each search step (line 11 in Algorithm 1). Various branching heuristics have been proposed, and which heuristic performs best often depends on the structure of the given SAT formula \(\phi\) (Kullmann, 2021). Customizing branching heuristics towards a specific distribution of inputs generally requires expert knowledge and significant trial and error.  

Reinforcement Learning Reinforcement learning (RL) aims to learn policies for sequential decision- making problems where an agent interacts with an environment to learn through trial and error. An RL problem is usually formalized as a Markov Decision Process (MDP), which is defined as a tuple \((\mathcal{S}, \mathcal{A}, P, R)\) , where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of possible actions, \(P\) denotes the transition probabilities between states and \(R\) is the reward function. In probabilistic settings, policies \(\pi\) are stochastic mappings from states to distributions over actions, i.e., \(\pi (a|s)\) indicates the probability of selecting action \(a\) in state \(s\) . More generally, for continuous action spaces, i.e. \(\mathcal{A} = \mathbb{R}\) , \(\pi (a|s)\) is the probability density that a policy assigns to an action \(a\) . The primary objective in RL is to determine an optimal policy \(\pi^{*}\) that maximizes the expected cumulative discounted reward \(\mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^{t} R(s_{t}, a_{t}) \right]\) , where \(\gamma \in [0, 1)\) represents a discount factor that emphasizes earlier rewards. This formulation is the basis for various RL algorithms. Recently, RL algorithms based on policy gradients, such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) have been used extensively to fine- tune LLMs from human feedback (RLHF, Christiano et al. (2017); Ouyang et al. (2022)) and from verifiable rewards (RLVR, Lambert et al. (2024); Guo et al. (2025)).