We further aim to gain insights into the weight distributions learned through RLAF. In particular, we investigate whether the policies learned with different base solvers are related and whether they capture predefined variable properties, such as backbone and UNSAT core membership. To this end, Figure 5 compares the weights for 5000 randomly selected variables from the corresponding validation sets. Specifically, we plot the expected variable weight \(\mathbb{E}[w(x)]\) for the Glucose- trained policy on the x- axis and plot the corresponding value for the March- trained policy on the y- axis. We also report the Pearson correlation coefficient \((r)\) for these weights to quantify their correlation. For 3SAT, we only plot variables from satisfiable instances and additionally indicate whether each variable belongs to its instance's backbone. Likewise, we focus on unsatisfiable instances for 3COL and CRYPTO and indicate if a variable occurs in the UNSAT core extracted for the experiment in Section 3.2.  

We observe that the variable weights of the two policies are generally correlated, with a Pearson correlation coefficient \(r\) between 0.73 and 0.85. This indicates that the learned weightings capture structural properties that are inherent to the variables and accelerate the search across different
<center>Figure 5: Weight correlation between policies learned with different solvers. For each instance distribution, we randomly sample 5,000 variables \(x\) from the corresponding validation set and plot the expected variable weight \(E[w(x)]\) for the policies learned with either base solver. The color further indicates the backbone or UNSAT core membership of each variable. </center>  

solvers. We further observe that for the 3CoL and CRYPTO instances, the variables with high weights are predominantly members of the UNSAT core. For these problem instances, the RLAF- based training therefore self- discovered weight policies that correlate to existing handcrafted heuristics while performing better, as demonstrated in Section 3.2. For the 3SAT instances, we do not observe a clear correlation between the learned weight policies and backbone membership, showing that in this case, the trained models express functions that, while effective, do not resemble this particular handcrafted heuristic.