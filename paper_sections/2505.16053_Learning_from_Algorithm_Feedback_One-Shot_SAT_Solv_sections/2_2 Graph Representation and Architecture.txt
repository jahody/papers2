Our goal is to map an instance \(\phi\) to advantageous variable weights and polarities with a neural network. A natural approach is to map \(\phi\) to a suitable graph representation \(G(\phi) = (V(\phi), E(\phi))\) that captures the instance's structure. This graph can then be processed by a GNN that extracts structural information in a trainable manner. We represent \(\phi\) as a standard "Literal- Clause Graph" proposed in prior work Selsam et al. (2019). Note that this choice is modular; other graph representations have also been suggested in the literature and could also be used. We process this graph with a trainable GNN model \(\mathcal{N}_{\theta}\) that performs message passing to extract latent structural embeddings for every vertex. Here, \(\theta\) represents the vector that contains all trainable model parameters. The output of \(\mathcal{N}_{\theta}\) is a mapping \(y: \operatorname {Var}(\phi) \to \mathbb{R}^{2}\) that assigns two real numbers to each variable in the input formula \(\phi\) . The full model details are provided in Appendix A.