Our aim is to learn a policy GNN that guides the SAT solver towards lower computational costs on a given distribution of SAT instances. Formally, let \(\Omega\) be some training distribution of SAT problems. The objective is to learn model weights \(\theta\) that minimize the expected solver cost when applying the learned policy to instances sampled from \(\Omega\) :  

\[\theta^{*} = \underset {\theta}{\arg \min}\underset {\phi \sim \Omega ,\mathcal{W}\sim \pi_{\theta}(\phi)}{\mathbb{E}}[\mathrm{Cost}(\phi ,\mathcal{W})]. \quad (6)\]  

Here, \(\mathrm{Cost}(\phi ,\mathcal{W})\) is defined as the number of decisions required when running \(\mathrm{Solve}(\phi ,\mathcal{W})\) , which is the primary target metric we aim to minimize. We can view this objective as an RL problem by modeling the process of choosing \(\mathcal{W}\) as a single- step Markov Decision Process (MDP) where the input formula \(\phi\) is viewed as the state, and a single- step episode unfolds by choosing a variable parameterization \(\mathcal{W}\) as the action. Once the action is taken, the environment transitions immediately to a terminal state, yielding a reward \(R(\phi ,\mathcal{W}) = - \mathrm{Cost}(\phi ,\mathcal{W})\) that is the negative of the solver's cost (e.g., number of decisions). Note that we also experimented with directly using CPU time as a cost measure, but found this to yield less stable training due to the performance variance caused by noisy CPU utilization.  

We leverage Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to learn a policy for this RL problem. GRPO is a simplification of Proximal Policy Optimization (PPO) (Schulman et al., 2017) that eliminates the need for learning an additional value network. The initial model weights \(\theta_{0}\) are sampled at random. GRPO updates these model weights in iterations \(k\in \{1,\ldots ,K\}\) In iteration \(k\) , we first sample a batch of training instances from \(\mathcal{F} = \{\phi_{1},\ldots ,\phi_{N}\} \sim \Omega^{N}\) from the given training distribution. For each such formula \(\phi_{i}\) we sample \(M\) variable parameterizations \(\mathcal{W}_{i,1},\ldots ,\mathcal{W}_{i,M}\sim \pi_{\theta_{k - 1}}(\phi_{i})\) i.i.d. from the current policy. We then run \(\mathrm{Solve}(\phi_{i},\mathcal{W}_{i,j})\) for all \(i,j\in [N]\times [M]\) and measure the corresponding cost and reward. The group- relative advantage is then defined as  

\[\hat{A}_{i,j} = \frac{R(\phi_{i},\mathcal{W}_{i,j}) - \mathrm{mean}(\mathbf{R}_{i})}{\mathrm{std}(\mathbf{R}_{i})} \quad (7)\]  

where \(\mathbf{R}_{i} = \{R(\phi_{i},\mathcal{W}_{i,j}) \mid j\in \{1,\ldots ,M\} \}\) is the set of all rewards collected for the same instance \(\phi_{i}\) . The main objective is to maximize the clipped policy update function for each training instance \(\phi_{i}\) :  

\[\mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) = \frac{1}{M}\sum_{j}\left[\min \left(r_{i,j}(\theta)\hat{A}_{i,j},\mathrm{clip}(r_{i,j}(\theta),1 - \epsilon ,1 + \epsilon)\hat{A}_{i,j}\right)\right]. \quad (8)\]
Here, \(\epsilon \in (0,1)\) is a hyperparameter, and \(r_{i,j}(\theta)\) is defined as the probability ratio of the new policy and the policy learned in the previous GRPO iteration:  

\[r_{i,j}(\theta) = \frac{\pi_{\theta}(\mathcal{W}_{i,j}|\phi_{i})}{\pi_{\theta_{k - 1}}(\mathcal{W}_{i,j}|\phi_{i})} \quad (9)\]  

This objective aims to adjust the policy such that actions (e.g., variable parameterizations) with high advantage become more likely while avoiding excessively large distribution shifts by clipping the objective at a probability ratio determined by \(\epsilon\) . The full training objective combines \(\mathcal{L}_{\mathrm{PPO}}\) with an additional term that penalizes the KL divergence relative to the previous model weights \(\theta_{k - 1}\) to stabilize training further:  

\[\mathcal{L}(\theta \mid \phi_{i}) = \mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) - \beta \cdot \mathrm{KL}\left(\pi_{\theta}(\phi_{i}),\pi_{\theta_{k - 1}}(\phi_{i})\right). \quad (10)\]  

Here, the weight \(\beta \geq 0\) is an additional hyperparameter. Starting from the previous model weights \(\theta_{k - 1}\) , we learn updated model weights \(\theta_{k}\) by performing stochastic gradient ascent for a fixed number of steps to maximize this objective function for all training instances. This overall process repeats in the next round of GRPO. In the appendix, Algorithm 4 provides a complete formal specification of our training.