In Section 3, we have described the general GNN frameworks that can learn to solve MaxSAT problem. Based on the successful early attempts, we build two GNN models that accept a CNF formula as input, and output the assignment of literals. We abbreviate them to MS- NSFG and MS- ESFG, because they use NSFG and ESFG as the graph representation, respectively. The structures of models follow those in Eq. (3) and (4). Here we only describe the implementation of some key components, and the complete frameworks are shown in Appendix.  

Aggregating functions. The implementation of aggregating functions: \(\mathrm{AGG_L}\) , \(\mathrm{AGG_C}\) in MS- NSFG, and \(\mathrm{AGG_L}^+\) , \(\mathrm{AGG_L}^-\) , \(\mathrm{AGG_C}^+\) , \(\mathrm{AGG_C}^-\) in MS- ESFG, consists of two steps. First, an MLP module maps the embedding of each node  

to a message vector. After that, the messages from relevant nodes are summed up to get the final output. For example, the function \(\mathrm{AGG_L}\) in MS- NSFG which generates the message from literals and sends it to clause \(j\) , can be formalized as \(\sum_{(i,j)\in E} \mathrm{MLP}(L_i)\) , where \(L_i\) is the embedding of literal \(i\) .  

Updating functions. The updating functions: \(\mathrm{UPD_L}\) and \(\mathrm{UPD_C}\) in both MS- NSFG and MS- ESFG can be implemented by the LSTM module (Hochreiter and Schmidhuber 1997). For example, to implement the function \(\mathrm{UPD_C}\) , the embedding of clause \(j\) (denoted as \(C_j\) ) is taken as the hidden state, and the message generated from aggregation is taken as the input of LSTM.