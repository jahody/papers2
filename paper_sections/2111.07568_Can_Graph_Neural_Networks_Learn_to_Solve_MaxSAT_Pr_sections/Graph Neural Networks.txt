Graph neural networks (GNNs) are a family of neural network architectures that operate on graphs, which have shown great power on many tasks across domains, such as protein interface prediction, recommendation systems and traffic prediction (Zhou et al. 2020). For a graph \(G = \langle V,E\rangle\) where \(V\) is a set of nodes and \(E\subseteq V\times V\) is a set of edges, GNN models accept \(G\) as input, and represent each node \(v\) as an embedding vector \(h_v^{(0)}\) . Most popular GNN models follow the message passing process that updates the embedding of a node by aggregating the information of its neighbors iteratively. The operation of the \(k\) - th iteration (layer) of GNNs can be formalized as  

\[\begin{array}{r l} & {s_{v}^{(k)} = \mathrm{AGG}^{(k)}(\{h_{v}^{(k - 1)}|u\in \mathcal{N}(v)\} ,}\\ & {h_{v}^{(k)} = \mathrm{UPD}^{(k)}(h_{v}^{(k - 1)},s_{v}^{(k)}),} \end{array} \quad (1)\]  

where \(h_v^{(k)}\) is the embedding vector of node \(v\) after the \(k\) - th iteration. In the aggregating (messaging) step, a message \(s_v^{(k)}\) is generated for each node \(v\) by collecting the embeddings from its neighbors \(\mathcal{N}(v)\) . Next, in the updating (combining) step, the embedding of each node is updated combined with the message generated above. After \(T\) iterations, the final embedding \(h_v^{(T)}\) for each node \(v\) is obtained. If the task GNNs need to handle is at the graph level such as graph classification, an embedding vector of the entire graph should be generated from that of all the nodes:  

\[h_{G} = \mathrm{READOUT}(\{h_{u}^{(T)}|u\in V\}). \quad (2)\]  

The final embeddings can be decoded into the outputs by a learnable function such as multi- layer perceptron (MLP), whether for node- level or graph- level tasks. The modern GNN variants have made different choices of aggregating, updating and readout functions, such as concatenation, summation, max- pooling and mean- pooling. A more comprehensive review of GNNs can be found in (Wu et al. 2021).