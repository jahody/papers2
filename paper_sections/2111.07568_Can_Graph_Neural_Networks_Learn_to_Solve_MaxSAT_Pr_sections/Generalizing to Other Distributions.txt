Generalization is an important factor in evaluating the possibility to apply GNN models to solve those harder problems. We make the predictions by MS- NSFG and MS- ESFG on the testing sets with different distributions from the training sets, and the results are shown in Table 1 and 2, respectively. The first column is the dataset used for training the model, and the first row is the testing set. For each pair of training and testing datasets, we report the gap to optimal objective together with the approximation ratio in the brackets above, and the accuracy of assignments below. Here, the approximation ratio is defined as the ratio of predicted and optimal objectives.  

Here we mainly focus on three kinds of generalization. The first is generalizing to the datasets with different clause- variable proportion. From the results, both models trained on R2 (60, 600) and tested on R2 (60, 800) (or
Table 2: The accuracy of MS-NSFG on different combinations of training and testing sets.   

<table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.86 (99.8%)</td><td>1.42 (99.7%)</td><td>3.86 (98.6%)</td><td>1.15 (99.8%)</td><td>6.77 (98.5%)</td><td></td></tr><tr><td>91.9%</td><td>91.9%</td><td>76.1%</td><td>92.0%</td><td>75.4%</td><td></td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>1.19 (99.7%)</td><td>1.17 (99.8%)</td><td>4.59 (98.4%)</td><td>1.50 (99.7%)</td><td>7.96 (98.3%)</td><td></td></tr><tr><td>91.7%</td><td>92.4%</td><td>75.8%</td><td>91.6%</td><td>75.1%</td><td></td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>20.58 (96.0%)</td><td>29.10 (95.7%)</td><td>1.37 (99.5%)</td><td>27.92 (95.9%)</td><td>2.16 (99.5%)</td><td></td></tr><tr><td>78.1%</td><td>76.0%</td><td>83.3%</td><td>78.0%</td><td>82.2%</td><td></td></tr></table>  

Table 3: The accuracy of MS-ESFG on different combinations of training and testing sets.   

<table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.54 (99.8%)</td><td>1.12 (99.8%)</td><td>5.98 (97.9%)</td><td>0.69 (99.9%)</td><td>9.92 (97.9%)</td></tr><tr><td>92.3%</td><td>92.2%</td><td>74.5%</td><td>92.2%</td><td>73.9%</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.48 (99.9%)</td><td>0.78 (99.8%)</td><td>6.13 (97.8%)</td><td>0.62 (99.9%)</td><td>10.24 (97.8%)</td></tr><tr><td>92.3%</td><td>92.8%</td><td>74.7%</td><td>92.2%</td><td>74.2%</td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>14.83 (97.1%)</td><td>16.44 (97.5%)</td><td>1.32 (99.5%)</td><td>20.22 (97.0%)</td><td>1.98 (99.5%)</td></tr><tr><td>78.8%</td><td>79.5%</td><td>83.5%</td><td>78.6%</td><td>82.5%</td></tr></table>  

vice versa) can maintain almost the same prediction accuracy. The second is generalizing to larger and more difficult problems. We use two testing sets, R2 (80, 800) and R3 (50, 500), where the number of variables is larger than those appeared in the training sets, as well as more difficult to solve by MaxHS. It can be found that both models trained on Max2SAT datasets can generalize to work on R2 (80, 800) with a satisfactory accuracy. This also holds when the training set is R3 (30, 300) and the testing set is R3 (50, 500), which implies that GNN models are expected to be promising alternatives to help solve those difficult MaxSAT problems. The last is generalizing to other datasets with different parameter \(k\) . For example, the model is trained on Max2SAT but tested on Max3SAT problems. The results show that both models have limitations under this condition, since they cannot achieve an accuracy of assignments \(>80\%\) on every pair of training and testing sets, and the gap to optimal objective is not close enough, especially when trained on R3 (30, 300) and tested on Max2SAT datasets.