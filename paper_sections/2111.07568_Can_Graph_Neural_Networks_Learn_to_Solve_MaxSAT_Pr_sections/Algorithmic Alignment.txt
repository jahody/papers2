Our theoretical analysis framework is inspired by the algorithmic alignment theory proposed by (Xu et al. 2020), which provides a new point of view to understand the reasoning capability of neural networks. Formally, suppose a neural network \(\mathcal{N}\) with \(n\) modules \(\mathcal{N}_i\) , if by replacing each \(\mathcal{N}_i\) with a function \(f_i\) it can simulate, and \(f_1, \ldots , f_n\) generate a reasoning function \(g\) , we say \(\mathcal{N}\) aligns with \(g\) . As shown in that paper, even if different models such as MLPs, deep sets and GNNs have the same expressive power theoretically, GNNs tend to achieve better performance and generalization stably in the experiments. This can be explained by the fact that the computational structure of GNNs aligns well with the dynamic programming (DP) algorithms. Therefore, compared with other models, each component of GNNs only needs to approximate a simpler function, which means the algorithmic alignment can improve the sample complexity.