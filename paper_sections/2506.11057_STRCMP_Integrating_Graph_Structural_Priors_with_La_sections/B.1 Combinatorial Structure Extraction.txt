Data Representation of MILP problem. A mixed- integer linear programming (MILP) problem is formally defined as:  

\[\min_{\pmb {x}\in \mathbb{R}^{n}}\pmb {w}^{\top}\pmb {x},\quad \mathrm{s.t.}\pmb {A}\pmb {x}\leq \pmb {b},\pmb {l}\leq \pmb {x}\leq \pmb {u},\pmb {x}_{j}\in \mathbb{Z},\forall j\in \mathbb{I}, \quad (11)\]  

where \(\pmb {w}\in \mathbb{R}^{n}\) \(A\in \mathbb{R}^{m\times n}\) \(b\in \mathbb{R}^{m}\) \(l\in (\mathbb{R}\cup \{- \infty \})^{n}\) \(\pmb {u}\in (\mathbb{R}\cup \{\pm \infty \})^{n}\) , and the index set \(\mathbb{I}\subset \{1,2,\dots ,n\}\) specifies integer- constrained variables.  

To encode MILP instances, we design a bipartite graph \(\mathcal{G} = (\mathcal{C}\cup \mathcal{V},\mathcal{E})\) with constraint- variable interactions. The constraint node set \(\mathcal{C} = \{c_{1},\dots ,c_{m}\}\) corresponds to rows of \(A\pmb {x}\leq \pmb{b}\) , where each node \(c_{i}\) is associated with a 1D feature vector \(c_{i} = (b_{i})\) representing its right- hand side value. The variable node set \(\mathcal{V} = \{v_{1},\dots ,v_{n}\}\) represents decision variables, each equipped with a 9D feature vector \(v_{j}\) containing the objective coefficient \(w_{j}\) , variable type indicator (integer/continuous), and bound parameters \(l_{j}\) \(u_{j}\) . Edges \(\mathcal{E} = \{e_{i,j}\}\) connect constraint \(c_{i}\) to variable \(v_{j}\) iff \(a_{i,j}\neq 0\) , with edge features \(e_{i,j} = (a_{i,j})\) encoding the constraint coefficients.  

Furthermore, a mixed- integer linear programming (MILP) instance is encoded as a weighted bipartite graph with feature matrices \(\pmb {G} = (\pmb {C},\pmb {V},\pmb {E})\) , where \(\pmb {C}\) \(\pmb{V}\) , and \(\pmb{E}\) aggregate constraint node features \(c_{i}\) , variable node features \(v_{j}\) , and edge features \(e_{i,j}\) , respectively. The full specification of these features is summarized in Table 2, which preserves all structural and numerical information of the original MILP problem. Following standard practice, we utilize the observation function from Ecole [50] to generate these bipartite graph representations from MILP instances.  

Table 2: Description of the constraint, variable, and edge features in our bipartite graph representation for MILP instance.   

<table><tr><td>Tensor</td><td>Feature</td><td>Description</td></tr><tr><td rowspan="3">C</td><td>Constraint coefficient</td><td>Average of all coefficients in the constraint.</td></tr><tr><td>Constraint degree</td><td>Degree of constraint nodes.</td></tr><tr><td>Bias</td><td>Normalized right-hand-side of the constraint.</td></tr><tr><td rowspan="4">V</td><td>Objective</td><td>Normalized objective coefficient.</td></tr><tr><td>Variable coefficient</td><td>Average variable coefficient in all constraints.</td></tr><tr><td>Variable degree</td><td>Degree of the variable node in the bipartite graph representation.</td></tr><tr><td>Maximum variable coefficient</td><td>Maximum variable coefficient in all constraints.</td></tr><tr><td></td><td>Minimum variable coefficient</td><td>Minimum variable coefficient in all constraints.</td></tr><tr><td>E</td><td>Coefficient</td><td>Constraint coefficient.</td></tr></table>  

Data Representation of SAT problem. A Boolean satisfiability (SAT) problem consists of variables \(x_{i}\) and logical operators \(\wedge\) , \(\vee\) , and \(\neg\) . A formula is satisfiable if there exists a variable assignment that makes all clauses evaluate to true. Following [51], we focus on formulas in conjunctive normal form (CNF) – conjunctions (∧) of clauses, where each clause is a disjunction (∨) of literals (variables \(x_{i}\) or their negations \(\neg x_{i}\) ). Any SAT formula can be converted to an equisatisfiable CNF formula in linear time. For example, \((x_{1} \vee \neg x_{2}) \wedge (x_{2} \vee \neg x_{3})\) represents a CNF formula with two clauses.  

We utilize the Variable- Clause Graph (VCG) to represent SAT formulas. For a SAT formula, the VCG is constructed with nodes representing literals and clauses, and edges indicating the inclusion of a literal in a clause. The bipartite structure of VCGs ensures a one- to- one correspondence between CNF formulas and their graph representations. Formally, a bipartite graph \(\mathcal{G} = (\mathcal{V}_{1} \cup \mathcal{V}_{2}, \mathcal{E})\) is defined by its vertex set \(\mathcal{V}_{1} \cup \mathcal{V}_{2} = \{v_{1}, \ldots , v_{n}\}\) and edge set \(\mathcal{E} \subseteq \{(v_{i}, v_{j}) | v_{i}, v_{j} \in \mathcal{V}\}\) . The vertex set is partitioned into two disjoint subsets \(\mathcal{V}_{1}\) and \(\mathcal{V}_{2}\) , with edges restricted to connections between nodes in distinct partitions: \(\mathcal{E} \subseteq \{(v_{i}, v_{j}) | v_{i} \in \mathcal{V}_{1}, v_{j} \in \mathcal{V}_{2}\}\) . In the context of VCGs, a CNF formula with \(n\)
<center>Figure 6: The convergence curve of training GNN for SAT domain. </center>  

literals and \(m\) clauses induces a bipartitioned graph where \(\mathcal{V}_{1} = \{l_{1},\dots,l_{n}\}\) denotes the literal nodes and \(\mathcal{V}_{2} = \{c_{1},\dots,c_{m}\}\) represents the clause nodes.  

GNN Structure. The process of using GNN to extract combinatorial optimization problem can be formulated as:  

\[\mathbf{v}_{i}^{(k + 1)}\leftarrow \mathbf{f}_{\mathbf{v}}(\mathbf{v}_{i}^{(k)},\sum_{j,i\neq j}^{(i,j)\in \mathcal{E}}\mathbf{g}_{\mathbf{v}}(\mathbf{v}_{i}^{(k)},\mathbf{v}_{j}^{(k)},\mathbf{e}_{i j})),\quad (k = 0,1,\dots,K - 1) \quad (12)\]  

\[\begin{array}{r}{\pmb{h}_{q} = P o o l(\{\mathbf{v}_{i}^{(K)}\}),} \end{array} \quad (13)\]  

where \(\mathbf{f}_{\mathbf{v}}\) and \(\mathbf{g}_{\mathbf{v}}\) are perceptrons for node representation; \(K\) represents the total number of times that we perform the convolution; \(P o o l\) denotes pooling function that aggregates the embedding of each node in the graph, obtaining the structure embedding \(\pmb{h}_{q}\in \mathbb{R}^{d}\) of the instance \(q\) . We denote the parameters of GNN as \(\theta_{G}\) .  

GNN Training. The loss function w.r.t. \(\theta_{G}\) is given below:  

\[\mathcal{L}(\theta_{G}) = -\frac{1}{N}\sum_{i = 1}^{N}\sum_{c = 1}^{C}y_{i,c}\log p_{\theta_{G}}(c|q_{i}), \quad (14)\]  

\[p_{\theta_{G}}(c|q_{i}) = \frac{\exp\left(W_{c}^{T}\pmb{h}_{q_{i}} + \pmb{b}_{c}\right)}{\sum_{j = 1}^{C}\exp\left(\pmb{W}_{j}^{T}\pmb{h}_{q_{j}} + \pmb{b}_{j}\right)}, \quad (15)\]  

where \(N\) is the total number of CO problems in the training procedure; \(C\) denotes the number of classes; \(y_{i,c}\in \{0,1\}\) is the ground- truth label of problem instance \(q_{i}\) ; and \(\pmb{W}_{j}\in \mathbb{R}^{d}\) , \(\pmb{b}_{j}\in \mathbb{R}\) , \((j = 1,\dots,C)\) is the parameters of final classifying layer, which are also part of \(\theta_{G}\) .  

Implementation & Training Details. Following the training protocol outlined above, we implement separate GNN models for the SAT and MILP domains using the dataset described in Appendix D. Our GNN architecture consists of three convolutional layers ( \(K = 3\) ) followed by a global mean pooling operation. We leverage the graph convolution operator from the torch_geometric library, with node embedding dimensions of 16, 32, and 64 for successive layers \(^3\) . To capture global graph structure, we apply mean pooling to the final convolutional layer's node embeddings. For classification, a softmax layer processes the pooled representation to produce final predictions. The models are trained using the loss function defined in Eq.(14) with AdamW optimizer and cosine decay learning rate. Figure 6 illustrates the training convergence for SAT instance classification (5- way classification task). Upon model convergence, we compute structural prior representations for combinatorial optimization problem instances by processing their bipartite graph representations through the above GNN. The final convolutional layer's output embeddings are extracted as the structural prior for each instance in the target domain.