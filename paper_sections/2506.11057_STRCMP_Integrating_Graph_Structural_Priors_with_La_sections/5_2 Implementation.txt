Model Architecture. The proposed composite model comprises two components: a GNN and a structure- prior- aware LLM. We implement the GNN using graph convolution operators from torch_geometric, structured with three sequential convolutional layers terminated by global mean
<center>Figure 3: Optimization performance over different CO domains. Aligned with [29], we normalize each benchmark's indicator values using \(1 - \frac{value - min}{2\times (max - min)}\) , where \(value\) represents the method's measured indicator, with \(min\) and \(max\) denoting respectively the minimum and maximum values observed during evaluation. A larger shaded area corresponds to superior performance. </center>  

pooling. We adapt the LLM component based on Qwen2.5- Coder- 7B- Instructor model through modifying its architecture, forward propagation dynamics and inference paradigm. Comprehensive implementation details including hyperparameter configurations, adaptations, and software dependencies are provided in Appendix B.1 and B.2.  

Training, Inference and Used Hardware. We first train domain- specific GNNs for SAT and MILP problems using aforementioned benchmark dataset. The GNNs are trained for three epochs on SAT instances and four epochs on MILP instances, followed by conducting post- training of the adapted LLM with a specialized corpus. All post- training data is collected via queries to Qwen2.5- Coder- 7B- Instructor, aligned with the model used in the post training. The adapted LLM undergoes three epochs of post training per domain. Both GNN and LLM selecting optimal checkpoints based on validation prediction loss. We then integrate the composite model (i.e. the trained GNN and adapted LLM) into an EA- based framework to discover solver- specific algorithm configurations optimized for training instances. Finally, we evaluate the performance of identified algorithms on held- out test sets. All experiments are conducted on hardware platform with dual AMD EPYC 9534 64- core processors @ 2.45GHz and two NVIDIA H800 80GB GPUs connected via PCIe. More training specifics and data curation are detailed in Appendix B.1 and B.2 respectively.