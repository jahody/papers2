Data Curation. We begin by assembling a curated collection of mathematical models for CO problems, which are directly compatible with corresponding CO solvers, alongside their natural language descriptions. Subsequently, for each CO problem, we compile a prompt that integrates the natural language description with specific code generation requirements. For instance, the code generation requirements may include details such as the function name, input/output parameters, expected function behavior, relevant background knowledge, etc. This prompt is then fed into an LLM to generate a code snippet. Note that to maximize the diversity of collected code snippets, we employ multiple LLM queries per prompt under high temperature settings to sample distinct candidate implementations. Each generated code snippet is evaluated by embedding it into the target solver and solving the corresponding CO problem, thereby obtaining performance metrics for the code snippet. Note that based on the principles of data curation, we collect the needed data via querying Qwen2.5- Coder- 7B- Instructor same as the model adopted in the following post training procedure.  

Post Training. Specifically, we first curate the collected data by processing each CO problem \(Q_{i}\) with its associated prompt \(x_{i}\) and corresponding multiple generated code snippets \(y_{i,j}(j = 1,\dots,M)\) These code snippets are systematically ranked based on previously obtained performance metrics to establish quality ordering. The highest- performing code- prompt pairs are selected to form the SFT dataset \(\mathcal{D}_{S F T} = \{(x_{i},y_{i}^{*})\}_{i = 1}^{N}\) . Additionally, by leveraging pairwise comparisons extracted from the established ranking hierarchy, we derive the preference dataset \(\mathcal{D}_{D P O} = \{(x_{i},y_{w},y_{l})\}_{i = 1}^{N}\) where \(y_{w},y_{l}\) are preferred/dispreferred code snippets. The training process can be formulated as follows:  

\[\mathcal{L}_{\mathrm{DPO}} = -\mathbb{E}_{(x,y_{w},y_{l})\sim \mathcal{D}_{D P O}}\left[\log \sigma \left(\beta \ln \frac{\pi_{\theta_{L}}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)} -\beta \ln \frac{\pi_{\theta_{L}}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right)\right], \quad (16)\]  

where \(\sigma\) is the Sigmoid function; \(\beta\) is the hyperparameter that governs the trade- off between reward maximization and KL divergence minimization; \(\pi_{\mathrm{ref}}\) is the reference model trained on \(\mathcal{D}_{S F T}\) . Note that both \(\pi_{\mathrm{ref}}\) and \(\pi_{\theta_{L}}\) updates only the parameter \(\theta_{L}\) of the composite model. Through above posttraining, we obtain a generative model capable of structure- aware code generation that simultaneously respects combinatorial optimization problems' inherent topological constraints and solver- specific syntactic requirements. Note that based on above principle of data curation and post- training, we collect 8k and 4k post- training instances for MILP and SAT instances respectively.  

Implementation & Training Details. To implement the proposed composite model, we first design a structure- prior- aware forward propagation mechanism and corresponding adapted inference framework based on the Qwen2.5- Coder- 7B- Instructor model via the Transformers library.  

\(\bullet\) Structure- Prior- Aware Forward Propagation Mechanism. Specifically, we process the input prompt through the tokenizer and the model's embedding layer to obtain \(i n p u t\_ e m b e d s\) , which are then merged with structural feature vectors of combinatorial optimization problems extracted by the previous graph neural network.  

First, we align the dimensionality of the combinatorial optimization problem feature vector \(\pmb{h}_{q}\in \mathbb{R}^{d}\) with the hidden layer dimensions of the large language model via zero- padding. Given the text embedding shape \(e m b e d s\in \mathbb{R}^{B\times S\times H}\) , where \(B\) is the batch size, \(S\) is the sequence length (number of tokens), and \(H\) is the hidden dimension, the dimension- adapted feature vector is obtained as:  

\[\mathbf{H}_{q} = Z e r o P a d d i n g(\mathbf{h}_{q}\oplus \mathbf{0}^{(H - d)})\in \mathbb{R}^{B\times H} \quad (17)\]  

where ZeroPadding denotes zero- padding, and \(\mathbf{H}_{q}\) represents the padded graph structural feature vector. Subsequently, we fuse text and structural features between the embedding layer and decoder layer by prepending the graph feature vector to the text embedding sequence, forming a hybrid input:  

\[\mathcal{E}(e m b e d s,\mathbf{H}_{q}) = [\mathrm{CLS}]\oplus \mathbf{H}_{q}\oplus e m b e d s[1:] \in \mathbb{R}^{B\times (1 + S)\times H} \quad (18)\]  

Here, \(e m b e d s\) is the input_embeds obtained from processing the textual prompt via the tokenizer and embedding layer, enabling the self- attention mechanism to jointly model textual semantics and graph structural features. A mask of all True values is constructed and merged with the attention_mask to match the shape of the combined input_embeds, ensuring \(\mathbf{H}_{q}\) participates in attention computation. Let the original attention_mask shape be \(M = \{0,1\}^{B\times S}\) ; the merged attention_mask becomes:  

\[M_{i n} = [M_{0:1};\mathbf{1}^{B};M_{1:S}]\in \{0,1\}^{B\times (1 + S)} \quad (19)\]
<center>Figure 7: The convergence curve of post-training composite model for SAT domain. </center>  

The resulting input_embeds and attention_mask are then passed to the decoder layers and subsequent model structures for standard forward propagation.  

\(\bullet\) Parameter- Efficient Finetuning. We adopt LoRA for parameter- efficient fine- tuning on an autoregressive language model task. Key hyperparameters are configured as: rank dimension \(r = 16\) controlling the latent dimension of low- rank adaptation matrices; scaling factor \(l o r a\_ a l p h a = 32\) for output normalization; dropout probability 0.05 for regularization; trainable low- rank adaptation layers injected exclusively into query \((q\_ p r o j)\) and value \((v\_ p r o j)\) projection submodules of the Transformer architecture while keeping other parameters frozen; bias parameters set to "none" to preserve original model biases. The adapted LLM is trained with AdamW optimizer and cosine decay learning rate. An illustrative training curve of the model for SAT domain is given in Figure 7.  

\(\bullet\) Adapted Inference Framework. Based on this modified architecture, we further adapt the LLM inference framework with structure- prior feature vector integration. During each autoregressive forward pass, logits are obtained via the feature- enhanced forward propagation, ensuring persistent influence of the structure- prior feature vector throughout the generation process rather than only affecting the initial output. We employ a hybrid sampling strategy with Top- k set to 20, Top- p to 0.8, and repetition penalty to 1.0.