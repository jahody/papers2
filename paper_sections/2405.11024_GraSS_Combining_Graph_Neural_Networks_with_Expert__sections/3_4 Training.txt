We train in a supervised fashion on a dataset of training SAT instances, for which runtimes have been collected ahead of time on each solver of interest. Since our model produces a distribution over the \(K\) possible solvers, we could treat the problem as simple classification with a cross- entropy loss. However, this would not be well- aligned with our objective of minimizing solving runtime, since it would equally penalize incorrect predictions, irrespective of the amount of additional runtime induced by the selection of a  

  

suboptimal solver. Instead, we propose the regret- like loss  

\[\mathcal{L} = \frac{1}{N}\sum_{i = 1}^{N}\left(\sum_{k = 1}^{K}p_{i}^{k}t_{i}^{k} - t_{i}^{*}\right)^{2}, \quad (1)\]  

where \(p_{i}^{k}\) and \(t_{i}^{k}\) are the model probability and runtime for instance \(i = 1,\ldots ,N\) and solver \(k = 1,\ldots ,K\) , respectively, and \(t_{i}^{*} = \min_{k}t_{i}^{k}\) is the best time achieved by any solver on the instance. This has the advantage of more directly optimizing final runtime, taking into account that not all mistakes are equally impactful on solving time. We minimize this loss using the Adam [26] algorithm with early stopping.