We compare our approach with the following baselines.  

Best Base Solver. The individual solver among the portfolio of seven that had the best performance on the training data, measured in average runtime over all instances. In practice, this was the bulky solver for both datasets.  

SATzilla07 [46]. We adapt this landmark SAT solving machine learning model, based on a linear ridge regression model trained to predict runtimes based on global handcrafted features that summarize SAT instance characteristics. Since our work focuses on SAT solver selection, we omit the presolving process in the original SATzilla pipeline. We also remove features in the original model that require probing. This leaves 33 global features (#1- 33 in the original article). The model is trained from the Ridge class in the scikit- learn [36] library with default settings. We convert the approach into a SAT solver selection model by selecting the solver with shortest predicted runtime.  

SATzilla12 [45]. We also adapt the updated SATzilla model from 2012, which was based on random forest classification. Again, we remove the presolving process, and only use the features that do not require probing (features #1- 55 in the original article). We train a random forest model between each pair of solvers, weighting each training instance by the absolute difference in runtime between the two solvers, for \(7(7 - 1) / 2 = 21\) models in total. Each model is trained from the RandomForestClassifier class in scikit- learn with 99 trees and \([log_2(55)] + 1 = 7\) sampled features in each tree. At test- time, each model is used to vote which solver it prefers in its pair for solving an instance, and the final solver choice is made from the solver that has received the most votes.  

ArgoSmArT [34]. This is an approach based on a \(k\) - nearest neighbors model trained for classification. We used the same 29 features as in the original paper, which form a subset of the 33 features used in our adaptation of SATzilla07. We use the KNeighborsClassifier class in scikit- learn with \(k = 9\) neighbors.  

CNN [31]. We reimplement the approach of Loreggia et al. [31], where the CNF formula is interpreted as text, converted to its ASCII values and then to a grayscale image, before being resized to 128x128 pixels and fed to a Convolutional Neural Network (CNN). We use the same architecture as in the paper, which we implement in the pytorch [35] library, and train it over a cross- entropy loss with the Adam [26] algorithm, a learning rate of 1e- 3 and early stopping.