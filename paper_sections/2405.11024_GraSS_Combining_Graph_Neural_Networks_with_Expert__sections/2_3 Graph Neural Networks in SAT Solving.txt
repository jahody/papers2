Several works have explored applications of GNNs to various aspects of SAT solving in the past, even if not specifically to the problem of solver selection. In all these works, some kind of graphical representation of SAT instances is used. Multiple suggestions have appeared in the literature: these include lossless representations like literal- clause graphs (LCGs) and variable- clause graphs (VCGs), and lossy representations like literal- incidence graphs (LIGs) and variable- incidence graphs (VIGs) [19]. These different representations strike a balance between graph size and information content, and have found success in various SAT- related tasks.  

Some prior works have explored the use of GNNs to learn local search heuristics in SAT solvers [28, 48]. Yolcu and PÃ³czos [48] represent SAT formulas as variable- clause graphs (VCGs) and a GNN model is trained to select variables whose sign to flip at every step through a Markov decision process (MDP). The learned heuristic is shown to reduce the number of steps required to solve the problem. Graph- Q- SAT [28] uses a deep Q- network (DQN) with GNN architecture to learn branching heuristics in conflict driven clause learning (CDCL) solvers. Each SAT formula is converted into a VCG, and GNN layers are used to predict the \(Q\) - value of each variable. The variable with the highest \(Q\) - value for the specific assignment is selected for branching. The learned heuristic is shown to significantly reduce the number of iterations required for SAT solving.  

Instead of relying on existing SAT solvers, NeuroSAT [38] uses a GNN- based model to predict satisfiability of an instance in an end- to- end manner. Each SAT formula is represented by a literal- clause graph (LCG), as in our work. After several steps of message- passing, the updated embedding of each literal is projected to a scalar "vote" to indicate the confidence that the formula is satisfiable. The votes are averaged together and passed through a sigmoid function to produce the model's probability that the instance is satisfiable. On randomly generated instances from a SR(40) distribution, NeuroSAT solved \(70\%\) of SAT problems with an accuracy of \(85\%\) . A subsequent work, NeuroCore [37], uses a lighter NeuroSAT model to predict the "core" of instance, which is the smallest unsatisfiable subset of clauses. This prediction is then used to guide variable selection in SAT solver algorithms.  

Finally, graph neural networks have also been widely used for SAT instance generation. For example, G2SAT [49] and HardSATGEN [29] represent instances as LCGs, and generate new variants from an iterative splitting and merging process driven by a GNN. Furthermore, W2SAT [44] extends this approach by representing instances as weighted graphs encoding literal co- occurrence among clauses, while using a similar generation mechanism.