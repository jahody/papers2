Exact solvers for combinatorial optimization problems are generally based on backtracking search. To improve this framework, branching heuristics are applied to give the most promising direction during the search, and branching heuristics are designed by the properties of specific problems. To improve branching heuristics by our GNN model, our idea is to define a function \(f(\mathrm{Var}_{b},\Theta)\) to measure the quality of branches in the search tree where \(\mathrm{Var}_{b}\) is the set of unsigned variables of a branch \(b\) and \(\Theta\) is the probability space from the output of our GNN model. We next utilize this idea for the dominating- clique problem.  

Culberson et al. [9] propose an efficient solver for the dominating- clique problem. From their experiments, the solver performs better than other general SAT solvers, including BerkMin, MarchEq, and SATzilla. The solver is a backtracking- based algorithm. They encode a given graph \(G = (V,E)\) with \(V = \{v_{1},v_{2},\dots,v_{n}\}\) to a CNF formula as follows.  

- There are \(n\) variables \(\{X_{i}\}_{i \in [n]}\) where \(X_{i} = 1\) means that the corresponding \(v_{i}\) is in the solution;
- Define a clause \(C_{i} = \{X_{j}\}_{v_{j}\in N[v_{i}]}\) for each vertex \(v_{i}\) to indicate that the vertex is in the solution or at least one of its neighbors is in the solution;  

With the encoded CNF formula, the solver works as Algorithm 1.  

Algorithm 1 [9] An algorithm for the dominating- clique problem  

1: procedure DoMCLq \((D,S,U,G(V,E))\)  

2: if \(U\) is \(\emptyset\) then  

3: DOMCLQ ‚Üê D  

4: return TRUE  

5: else  

6: Find \(C\in U\) such that \(|C\cap S| = \min_{C^{\prime}\in U}|C^{\prime}\cap S|\)  

7: if \(C\cap S\) is not \(\emptyset\) then  

8: \(S^{\prime \prime}\gets S\)  

9: for \(X_{i}\in C\cap S\) do  

10: \(X_{i}\gets 1\) ; \(D\gets D\cup \{v_{i}\}\)  

11: \(S^{\prime}\leftarrow \{X_{j}\mid X_{j}\in S^{\prime \prime},\{v_{i},v_{j}\} \in E\} ;U^{\prime}\leftarrow U\setminus \{C^{\prime}\mid C^{\prime}\in\)  

\(U,X_{i}\in C^{\prime}\}\)  

12: if \(\mathrm{DomClq}(D,S^{\prime},U^{\prime},G(V,E))\) then  

13: return TRUE  

14: else  

15: \(X_{i}\gets 0\) ; \(D\gets D\setminus \{v_{i}\} ;S^{\prime \prime}\gets S^{\prime \prime}\setminus \{X_{i}\}\)  

16: return FALSE  

The solver has three parameters: a potential dominating clique \(D\) , a set \(S\) of the unassigned variables such that \(S = \{X_{u}\mid \forall v\in D,\{u,v\} \in E\}\) , and a set \(U\) of unsatisfied clauses. The three parameters are initialized as \(\emptyset\) , \(\{X_{i}\}_{i\in [n]}\) , and \(\{C_{i}\}_{i\in [n]}\) respectively.  

From the perspective of CSP solvers, this algorithm uses the MRV heuristic. This heuristic is optimal if every sub- problem by adding one vertex into \(D\) has the same amount of search space. However, it is false for most cases. To improve it, we can see \(\{X_{i}\}_{i\in [n]}\) as random variables under the probability distributions output from our probabilistic- method GNN. We use the information entropy of these random variables to predict the amount of the search space of unsatisfied clauses. In particular, instead of the cardinality of \(C^{\prime}\cap S\) , we measure the joint information entropy of the random variables in \(C^{\prime}\cap S\) . We therefore replace Line 6 in Algorithm 1 by finding \(C\in U\) such that \((C\cap S)\) has the minimum joint information entropy. In other words, we define the function \(f\) as the joint information entropy of unassigned variables of a branch to measure the quality of branches. Note that the probability distributions of unassigned variables should not be fixed during the backtracking search. The reason is that \(S\) , \(D\) , and \(U\) are changing during the search, so the correspondingly unexplored subgraph which consists of \(S\) and \(U\) is also changing. We apply the softmax function on the distributions of the unassigned variables to re- weigh them during the backtracking search. We also try the \(Z\) - score normalization, but its performance
is worse than the softmax function.  

To calculate the joint information entropy of random variables in \(C^{\prime}\cap S =\) \(\{X_{1},\dots,X_{m}\}\) , we try two ways. The first one, called the fast version because the calculation of this way is fast but not accurate, is \(\begin{array}{r}{\sum_{i = 1}^{m}\mathbb{H}(X_{i}) - \Big(\prod_{i = 1}^{m}(1 - } \end{array}\) \(p_{i})\Big)\log_{2}\Big(\prod_{i = 1}^{m}(1 - p_{i})\Big)\) . \(X_{i}\) 's are mutually independent, so their joint information entropy is the sum of each one's information entropy. In addition, if there is a dominating clique, at least one variable in \(C^{\prime}\cap S\) is 1; we thus minus the information entropy of the case that all variables in \(C^{\prime}\cap S\) are 0.  

The second way, called the accurate version but with a slow calculation, is similar to the improvement on Equation (6). Instead of calculating the joint information entropy of random variables in \((C^{\prime}\cap S)\) , we calculate the joint information entropy under a more precise case. Given a clause \(C^{\prime}\cap S = \{X_{1},\dots,X_{m}\}\) , for \(i = 1\) to \(m\) , we iteratively set \(X_{1},\dots,X_{i - 1}\) as 0 and \(X_{i}\) as 1. We know that \(\{X_{i + 1},\dots,X_{m}\} \setminus \{X_{j}\}_{v_{j}\in N(v_{i})}\) must be 0 from the properties of dominating cliques. Thus, we only need to consider the joint information entropy of the random variables in \(N[v_{i}]\setminus \{v_{1},\dots,v_{i - 1}\}\) . Similar to Equation (7), we calculate the joint information entropy as  

\[\sum_{i = 1}^{m}\Big(p\big(-\log_{2}(p) + \sum_{v_{r}\in N(v_{i})\setminus \{v_{1},\dots,v_{i - 1}\}}\mathbb{H}(X_{r})\big)\Big) \quad (8)\]  

where \(\begin{array}{r}{p = p_{i}\prod_{j = 1}^{i - 1}(1 - p_{j})\prod_{v_{k}\in \{v_{i + 1},\dots,v_{m}\}\setminus N(v_{i})}(1 - p_{k}).} \end{array}\)