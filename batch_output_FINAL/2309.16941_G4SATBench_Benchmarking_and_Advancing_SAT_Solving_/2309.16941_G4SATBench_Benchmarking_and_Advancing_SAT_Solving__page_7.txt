demonstrate limited generalization to larger formulas beyond their training data, they perform relatively better on smaller instances. These observations suggest that the generalization performance of GNN models for satisfiability prediction is influenced by the distinct nature and complexity of its training data. Training on more challenging instances could potentially enhance their generalization ability.  

<center>Figure 3: Classification accuracy of satisfiability across different datasets. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>  

<center>Figure 4: Classification accuracy of satisfiability across different difficulty levels. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>  

Due to the limited space, Figure 4 exclusively displays the performance of NeuroSAT and GGNN on the SR and 3- SAT datasets. Comprehensive results on the other five datasets, as well as the experimental results on different massage passing iterations, are provided in Appendix C.2.  

### 5.2 Satisfying Assignment Prediction  

Evaluation with different training losses. Table 2 presents the results of NeuroSAT (on \(\mathrm{LCG}^{*}\) ) and GGNN (on \(\mathrm{VCG}^{*}\) ) across three different training objectives. The results of other GNN models are listed in Table 10 in Appendix C.3. Interestingly, the unsupervised training methods outperform the supervised learning approach across the majority of datasets. We hypothesize that this is due to the presence of multiple satisfying assignments in most satisfiable instances. Supervised training tends to bias GNN models towards learning a specific satisfying solution, thereby neglecting the exploration of other feasible ones. This bias may compromise the models' ability to generalize effectively. Such limitations become increasingly apparent when the space of satisfying solutions is much larger, as seen in the medium CA and PS datasets. Additionally, it is noteworthy that employing \(\mathrm{UNS}_1\) as the loss function can result in instability during the training of some GNN models, leading to a failure to converge in some cases. Conversely, using \(\mathrm{UNS}_2\) loss demonstrates strong and stable performance across all datasets.  

In addition to evaluating the performance of GNN models under various training loss functions, we extend our analysis to explore how these models perform across different data distributions and under various inference algorithms. Furthermore, we assess the robustness of these GNN models when trained on noisy datasets that include unsatisfiable instances in an unsupervised fashion. For detailed results of these evaluations across different GNN baselines, please refer to Appendix C.3.