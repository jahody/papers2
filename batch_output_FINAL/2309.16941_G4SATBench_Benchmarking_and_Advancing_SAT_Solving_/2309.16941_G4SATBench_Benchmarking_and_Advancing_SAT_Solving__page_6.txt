where \(c^{+}\) and \(c^{- }\) are the sets of variables that occur in the clause \(c\) in positive and negative form respectively. Note that these two losses reach the minimum only when the prediction \(x\) is a satisfying assignment, thus minimizing such losses could help to construct a possible satisfying assignment.  

Inference algorithms. Beyond the standard readout process like training, G4SATBench offers two alternative inference algorithms for satisfying assignment prediction (Selsam et al., 2019; Amizadeh et al., 2019b). The first method performs 2- clustering on the literal embeddings to obtain two centers \(\Delta_{1}\) and \(\Delta_{2}\) and then partitions the positive and negative literals of each variable into distinct groups based on the predicate \(||x_{i} - \Delta_{1}||^{2} + ||\neg x_{i} - \Delta_{2}||^{2}< ||x_{i} - \Delta_{2}||^{2} + ||\neg x_{i} - \Delta_{1}||^{2}\) (Selsam et al., 2019). This allows the construction of two possible assignments by mapping one group of literals to true. The second approach is to employ the readout function at each iteration of message passing, resulting in multiple assignment predictions for a given instance (Amizadeh et al., 2019b).  

Evaluation metrics. For satisfiability prediction and unsat- core variable prediction, we report the classification accuracy of each GNN model in G4SATBench. For satisfying assignment prediction, we report the solving accuracy of the predicted assignments. If multiple assignments are predicted for a SAT instance, the instance is considered solved if any of the predictions satisfy the formula.  

## 5 Benchmarking Evaluation on G4SATBench  

In this section, we present the benchmarking results of G4SATBench. To ensure a fair comparison, we conduct a grid search to tune the hyperparameters of each GNN baseline. The best checkpoint for each GNN model is selected based on its performance on the validation set. To mitigate the impact of randomness, we use 3 different random seeds to repeat the experiment in each setting and report the average performance. Each experiment is performed on a single RTX8000 GPU and 16 AMD EPYC 7502 CPU cores, and the total time cost is approximately 8,000 GPU hours. For detailed experimental setup and hyperparameters, please refer to Appendix C.1.  

### 5.1 Satisfiability Prediction  

Evaluation on the same distribution. Table 1 shows the benchmarking results of each GNN baseline when trained and evaluated on datasets possessing identical distributions. All GNN models exhibit strong performance across most easy and medium datasets, except for the medium SR dataset. This difficulty can be attributed to the inherent characteristic of this dataset, which includes satisfiable and unsatisfiable pairs of medium- sized instances distinguished by just a single differing literal. Such a subtle difference presents a substantial challenge for GNN models in satisfiability classification. Among all GNN models, the different graph constructions do not seem to have a significant impact on the results, and NeuroSAT (on LCG\*) and GGNN (on VCG\*) achieve the best overall performance.  

Table 1: Classification accuracy of satisfiability on identical distribution.   

<table><tr><td rowspan="2">Graph</td><td rowspan="2">Method</td><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td><td>k-Vercov</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Clique</td><td>k-Domest</td></tr><tr><td rowspan="4">LCG*</td><td>NeuroSAT</td><td>96.00</td><td>96.33</td><td>98.83</td><td>96.59</td><td>97.92</td><td>99.77</td><td>99.99</td><td>78.02</td><td>84.90</td><td>99.57</td><td>96.81</td><td>89.39</td><td>99.67</td></tr><tr><td>GCN</td><td>94.43</td><td>94.47</td><td>98.79</td><td>97.53</td><td>98.24</td><td>99.59</td><td>99.98</td><td>69.39</td><td>82.67</td><td>99.53</td><td>96.16</td><td>85.72</td><td>99.16</td></tr><tr><td>GGNN</td><td>96.36</td><td>95.70</td><td>98.81</td><td>97.47</td><td>98.80</td><td>99.77</td><td>99.97</td><td>71.44</td><td>83.45</td><td>99.50</td><td>96.21</td><td>81.20</td><td>99.69</td></tr><tr><td>GIN</td><td>95.78</td><td>95.37</td><td>98.14</td><td>96.98</td><td>97.60</td><td>99.71</td><td>99.97</td><td>70.54</td><td>82.80</td><td>99.49</td><td>95.80</td><td>83.87</td><td>99.61</td></tr><tr><td rowspan="3">VCG*</td><td>GCN</td><td>93.19</td><td>94.92</td><td>97.82</td><td>95.79</td><td>98.72</td><td>99.54</td><td>99.99</td><td>66.35</td><td>83.75</td><td>99.49</td><td>95.48</td><td>82.99</td><td>99.42</td></tr><tr><td>GGNN</td><td>96.75</td><td>96.25</td><td>98.77</td><td>96.44</td><td>98.88</td><td>99.68</td><td>99.98</td><td>77.12</td><td>85.11</td><td>99.57</td><td>96.48</td><td>83.63</td><td>99.62</td></tr><tr><td>GIN</td><td>96.04</td><td>95.71</td><td>98.47</td><td>96.95</td><td>97.33</td><td>99.59</td><td>99.98</td><td>73.56</td><td>85.26</td><td>99.49</td><td>96.55</td><td>89.41</td><td>99.38</td></tr></table>  

Evaluation across different distributions. To assess the generalization ability of GNN models, we evaluate the performance of NeuroSAT (on LCG\*) and GGNN (on VCG\*) across different datasets and difficulty levels. As shown in Figure 3 and Figure 4, NeuroSAT and GGNN struggle to generalize effectively to datasets distinct from their training data in most cases. However, when trained on the SR dataset, they exhibit better generalization performance across different datasets. Furthermore, while both GNN models