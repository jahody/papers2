Statistics. To provide a comprehensive understanding of our generated datasets, we compute several characteristics across three difficulty levels. These statistics include the average number of variables and clauses, as well as graph measures such as average clustering coefficient (in VIG) and modularity (in VIG, VCG, and LCG). The dataset statistics are summarized in Table 8. 

Table 8: Dataset statistics across difficulty levels in G4SATBench. 

<table><tr><td rowspan="2">Dataset</td><td colspan="6">Easy Difficulty</td><td colspan="6">Medium Difficulty</td><td colspan="6">Hard Difficulty</td></tr><tr><td>#Variables</td><td>#Clauses</td><td>C.C.(VIG)</td><td>Mod.(VIG)</td><td>Mod.(VCG)</td><td>Mod.(LCG)</td><td>#Variables</td><td>#Clauses</td><td>C.C.(VIG)</td><td>Mod.(VIG)</td><td>Mod.(LCG)</td><td>#Variables</td><td>#Clauses</td><td>C.C.(VIG)</td><td>Mod.(VCG)</td><td>Mod.(LCG)</td></tr><tr><td>SR</td><td>25.00</td><td>148.35</td><td>0.98</td><td>0.00</td><td>0.25</td><td>0.33</td><td>118.36</td><td>646.54</td><td>0.62</td><td>0.06</td><td>0.31</td><td>0.37</td><td>299.64</td><td>1613.86</td><td>0.32</td><td>0.09</td><td>0.32</td><td>0.37</td></tr><tr><td>3-SAT</td><td>25.05</td><td>113.69</td><td>0.72</td><td>0.06</td><td>0.36</td><td>0.46</td><td>120.00</td><td>513.14</td><td>0.27</td><td>0.16</td><td>0.43</td><td>0.51</td><td>250.44</td><td>1067.34</td><td>0.14</td><td>0.17</td><td>0.45</td><td>0.52</td></tr><tr><td>CA</td><td>31.66</td><td>303.48</td><td>0.65</td><td>0.19</td><td>0.73</td><td>0.73</td><td>120.27</td><td>1661.07</td><td>0.54</td><td>0.38</td><td>0.80</td><td>0.80</td><td>299.68</td><td>4195.50</td><td>0.59</td><td>0.57</td><td>0.80</td><td>0.80</td></tr><tr><td>PS</td><td>25.41</td><td>176.68</td><td>0.98</td><td>0.00</td><td>0.27</td><td>0.32</td><td>118.75</td><td>822.78</td><td>0.86</td><td>0.05</td><td>0.35</td><td>0.37</td><td>249.61</td><td>1728.34</td><td>0.77</td><td>0.08</td><td>0.38</td><td>0.28</td></tr><tr><td>A-Clique</td><td>34.85</td><td>592.89</td><td>0.90</td><td>0.03</td><td>0.45</td><td>0.49</td><td>69.56</td><td>2220.05</td><td>0.91</td><td>0.03</td><td>0.48</td><td>0.49</td><td>112.87</td><td>5543.26</td><td>0.88</td><td>0.04</td><td>0.49</td><td>0.50</td></tr><tr><td>k-Domest</td><td>41.90</td><td>369.40</td><td>0.70</td><td>0.26</td><td>0.47</td><td>0.53</td><td>90.64</td><td>1736.22</td><td>0.70</td><td>0.21</td><td>0.49</td><td>0.51</td><td>137.31</td><td>4022.48</td><td>0.70</td><td>0.20</td><td>0.49</td><td>0.51</td></tr><tr><td>k-Verox</td><td>45.41</td><td>484.28</td><td>0.66</td><td>0.16</td><td>0.48</td><td>0.53</td><td>107.40</td><td>2634.14</td><td>0.69</td><td>0.16</td><td>0.49</td><td>0.51</td><td>190.24</td><td>8190.94</td><td>0.69</td><td>0.16</td><td>0.50</td><td>0.51</td></tr></table>

## B GNN Models 

Message-passing schemes on VCG*. Recall that VCG* incorporates two distinct edge types, G4SATBench employs different functions to execute heterogeneous message-passing in each direction of each edge type. Formally, we define a d-dimensional embedding for each variable and clause node, denoted by \(h_l\) and \(h_c\), respectively. These embeddings are initialized to two learnable vectors \(h_v^0\) and \(h_c^0\), depending on the node type. At the \(k\)-th iteration of message passing, these hidden representations are updated as follows: 

\[h_v^{(k)} = \mathrm{UPD}\left(\mathrm{AGG}\left(\left\{\mathrm{MLP}_v^+ \left(h_v^{(k-1)}\right)\right\}\right), \mathrm{AGG}\left(\left\{\mathrm{MLP}_v^- \left(h_v^{(k-1)}\right)\right\}\right), h_c^{(k-1)}\right), \quad (7)\] \[h_v^{(k)} = \mathrm{UPD}\left(\mathrm{AGG}\left(\left\{\mathrm{MLP}_c^+ \left(h_c^{(k-1)}\right)\right\}\right), \mathrm{AGG}\left(\left\{\mathrm{MLP}_c^- \left(h_c^{(k-1)}\right)\right\}\right), h_v^{(k-1)}\right),\]

where \(c^+\) and \(c^-\) denote the sets of variable nodes that occur in the clause \(c\) with positive and negative polarity, respectively. Similarly, \(v^+\) and \(v^-\) denote the sets of clause nodes where variable \(v\) occurs in positive and negative form. \(\mathrm{MLP}_v^+\), \(\mathrm{MLP}_v^-\), \(\mathrm{MLP}_c^+\), and \(\mathrm{MLP}_c^-\) are four MLPs. \(\mathrm{UPD}(\cdot)\) is the update function, and \(\mathrm{AGG}(\cdot)\) is the aggregation function. 

GNN baselines. Table 9 summarizes the message-passing algorithms of the GNN models used in G4SATBench. We adopt heterogeneous versions of GCN (Kipf & Welling, 2017), GGNN (Li et al., 2016), and GIN (Xu et al., 2019) on both LCG* and VCG*, while maintaining the original NeuroSAT (Selsam et al., 2019) only on LCG*. 

## C Benchmarking Evaluation 

### C.1 Implementation Details 

In G4SATBench, we provide the ground truth of satisfiability and satisfying assignments by calling the state-of-the-art modern SAT solver CaDiCaL (Fleury & Heisinger, 2020) and generate the truth labels for unsat- core variables by invoking the proof checker DRAT-trim (Wetzler et al., 2014). All neural networks in our study are implemented using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). For all GNN models, we set the feature dimension \(d\) to 128 and the number of message passing iterations \(T\) to 32. The MLPs in the models consist of two hidden layers with the ReLU (Nair & Hinton, 2010) activation function. To select the optimal hyperparameters for each GNN baseline, we conduct a grid search over several settings. Specifically, we explore different learning rates from \(\{10^{-3}, 5 \times 10^{-4}, 10^{-4}, 5 \times 10^{-5}, 10^{-5}\}\), training epochs from \(\{50, 100, 200\}\), weight decay values from \(\{10^{-6}, 10^{-7}, 10^{-8}, 10^{-9}, 10^{-10}\}\), and gradient clipping norms from \(\{0.1, 0.5, 1\}\). We employ Adam (Kingma & Ba, 2015) as the optimizer and set the batch size to 128, 64, or 32 to fit within the maximum GPU memory (48G). For the parameters \(\tau\) and \(\kappa\) of the unsupervised loss in Equation 4 and Equation 5, we try the default settings (\(\tau = t^{-0.4}\) and \(\kappa = 10\), where \(t\) is the global step during training) as the original paper (Amizadeh et al., 2019a) as well as other values (\(\tau \in \{0.05, 0.1, 0.2, 0.5\}\), \(\kappa \in \{1, 2, 5\}\)) and empirically find \(\tau = 0.1\), \(\kappa = 1\) yield the best results. Furthermore,