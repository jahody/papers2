Evaluation with different datasets. Figure 9 illustrates the performance of NeuroSAT across different datasets. For easy datasets, we observe that NeuroSAT demonstrates a strong generalization ability to other datasets when trained on the SR, 3- SAT, CA, and PS datasets. However, when trained on the \(k\) - Clique, \(k\) - Domset, and \(k\) - Vercov datasets, which involve specific graph structures inherent to their combinatorial problems, NeuroSAT struggles to generalize effectively. This observation indicates that the GNN model may overfit to leverage specific graph features associated with these combinatorial datasets, without developing a generalized solving strategy that can be applied to other problem domains for satisfying assignment prediction. For medium datasets, NeuroSAT also faces challenges in generalization, as its performance is relatively limited. This can be attributed to the difficulty of these datasets, where finding satisfying assignments is much harder than easy datasets.  

<center>Figure 9: Solving accuracy of NeuroSAT across different datasets (with \(\mathrm{UNS}_2\) as the training loss). The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>  

Evaluation with different inference algorithms. Figure 10 illustrates the results of NeuroSAT using various decoding algorithms (with \(\mathrm{UNS}_2\) as the training loss). Notably, all three decoding algorithms demonstrate similar performances across all datasets. This observation indicates that utilizing the standard readout after message passing is sufficient for predicting a satisfying assignment. Also, the GNN model has successfully learned to identify potential satisfying assignments within the latent space, which can be extracted by clustering the literal embeddings.  

<center>Figure 10: Solving accuracy of NeuroSAT with different inference algorithms. </center>  

## Evaluation with unsatisfiable training in-  

Evaluation with unsatisfiable training instances. Following previous works (Amizadeh et al., 2019a;b; Ozolins et al., 2022), our evaluation of GNN models focuses solely on satisfiable instances. However, in practical scenarios, the satisfiability of instances may not be known before training. To address this gap, we explore the effectiveness of training NeuroSAT using the unsupervised loss \(\mathrm{UNS}_2\) on noisy datasets that contain unsatisfiable instances. Table 11 presents the results of NeuroSAT when trained on such datasets, where \(50\%\) of the instances are unsatisfiable. Interestingly, incorporating unsatisfiable instances for training does not significantly affect the performance of the GNN model. This finding highlights the potential utility of training GNN models using \(\mathrm{UNS}_2\) loss on new datasets, irrespective of any prior knowledge regarding their satisfiability.  

Table 11: Solving accuracy of NeuroSAT when trained on noisy datasets. Values in parentheses indicate the performance difference compared to the model trained without unsatisfiable instances. The \(k\) -Clique dataset is excluded as NeuroSAT fails during training.   

<table><tr><td colspan="6">Easy Datasets</td><td colspan="6">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Domset</td><td>k-Vercov</td><td>SR</td><td>3-SAT</td><td>CA</td><td>PS</td><td>k-Domset</td><td>k-Vercov</td></tr><tr><td>78.84<br>(-0.95)</td><td>80.48<br>(-0.11)</td><td>87.01<br>(-2.33)</td><td>88.66<br>(-0.13)</td><td>98.00<br>(-0.85)</td><td>95.24<br>(-4.49)</td><td>37.21<br>(-0.04)</td><td>41.75<br>(+0.14)</td><td>76.49<br>(+5.64)</td><td>72.52<br>(+1.46)</td><td>94.93<br>(-1.25)</td><td>96.18<br>(+0.19)</td></tr></table>  

## C.4 Unsat-core Variable Prediction  

Evaluation across different difficulty levels. The results across different difficulty levels are presented in Figure 11. Remarkably, both NeuroSAT and GGNN exhibit a strong generalization ability when trained on easy or medium datasets. This suggests that GNN models can effectively learn and generalize from the