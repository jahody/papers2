Table 9: Supported GNN models in G4SATBench.   

<table><tr><td>Graph</td><td>Method</td><td>Message-passing Algorithm</td><td>Notes</td></tr><tr><td rowspan="2">NeuroSAT</td><td>h(c), s(c) = LayerNormLSTM1</td><td>h(c), s(c) = LayerNormLSTM1</td><td>s, s1 are the hidden states which are initialized to zero vectors.</td></tr><tr><td>h(c), s(c) = LayerNormLSTM2</td><td>h(c), s(c) = LayerNormLSTM2</td><td></td></tr><tr><td rowspan="2">LCG*</td><td>h(c) = Linear1</td><td>h(c) = Linear1</td><td>d, d1 are the degrees of clause node c and literal node l in LCG respectively.</td></tr><tr><td>h(c) = Linear2</td><td>h(c) = Linear2</td><td></td></tr><tr><td rowspan="2">GGNN</td><td>h(c) = GRU1</td><td>h(c) = GRU1</td><td></td></tr><tr><td>h(c) = GRU2</td><td>h(c) = GRU2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = MLP1</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = MLP2</td><td></td></tr><tr><td rowspan="2">GCN</td><td>h(c) = Linear1</td><td>h(c) = Linear1</td><td>d, d1 are the degrees of clause node c and variable node v in VCG respectively.</td></tr><tr><td>h(c) = Linear2</td><td>h(c) = Linear2</td><td></td></tr><tr><td rowspan="2">VCG*</td><td>h(c) = GRU1</td><td>h(c) = GRU1</td><td></td></tr><tr><td>h(c) = GRU2</td><td>h(c) = GRU2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = MLP1</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = MLP2</td><td></td></tr><tr><td rowspan="2">GIN</td><td>h(c) = MLP1</td><td>h(c) = GIN</td><td></td></tr><tr><td>h(c) = MLP2</td><td>h(c) = GIN</td><td></td></tr></table>  

it is important to note that we use three different random seeds to benchmark the performance of different GNN models and assess the generalization ability of NeuroSAT and GGNN using one seed for simplicity.  

## C.2 Satisfiability Prediction  

Evaluation across different difficulty levels. The complete results of NeuroSAT and GGNN across different difficulty levels are presented in Figure 6. Consistent with the findings on the SR and 3- SAT datasets, both GNN models exhibit limited generalization ability to larger instances beyond their training data, while displaying relatively better performance on smaller instances. This observation suggests that training these models on more challenging instances could potentially enhance their generalization ability and improve their performance on larger instances.  

<center>Figure 6: Classification accuracy of satisfiability across different difficulty levels. The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>