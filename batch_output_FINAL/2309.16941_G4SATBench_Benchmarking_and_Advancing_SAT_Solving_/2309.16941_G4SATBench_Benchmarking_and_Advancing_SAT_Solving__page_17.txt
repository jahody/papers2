Evaluation with different message passing iterations. To investigate the impact of message-passing iterations on the performance of GNN models during training and testing, we conducted experiments with varying iteration values. Figure 7 presents the results of NeuroSAT and GGNN trained and evaluated with different message passing iterations. Remarkably, using a training iteration value of 32 consistently yielded the best performance for both models. Conversely, employing too small or too large iteration values during training resulted in decreased performance. Furthermore, the models trained with 32 iterations also demonstrated good generalization ability to testing iterations 16 and 64. These findings emphasize the critical importance of selecting an appropriate message-passing iteration to ensure optimal learning and reasoning within GNN models.  

<center>Figure 7: Classification accuracy of satisfiability across different message passing iterations \(T\) . The x-axis denotes testing iterations and the y-axis denotes training iterations. </center>  

## C.3 Satisfying Assignment Prediction  

Evaluation with different training losses. Table 10 presents the complete results of each GNN baseline across three different training objectives. Like the results of NeuroSAT and GGNN, all other GNN models with unsupervised training outperform their supervised training counterparts.  

Table 10: Solving accuracy on identical distribution with different training losses. The top and bottom 7 rows represent the results for easy and medium datasets, respectively. SUP denotes the supervised loss, \(\mathrm{UNS}_1\) and \(\mathrm{UNS}_2\) correspond to the unsupervised losses defined in Equation 5 and Equation 6, respectively. The symbol "-" indicates that some seeds failed during training. Note that only satisfiable instances are evaluated in this experiment.  

<table><tr><td rowspan="2">Graph</td><td rowspan="2">Method</td><td colspan="3">SR</td><td colspan="3">3-SAT</td><td colspan="3">CA</td><td colspan="3">PS</td><td colspan="3">k-Clique</td><td colspan="3">k-Domest</td><td colspan="3">k-Vercov</td></tr><tr><td>SUP</td><td>UNS1</td><td>UNS2</td><td>SUP</td><td>UNS1</td><td>UNS2</td><td>SUP</td><td>UNS1</td><td>UNS2</td><td></td><td>SUP</td><td>UNS1</td><td>UNS2</td><td>SUP</td><td>UNS1</td><td>UNS2</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="4">LCG*</td><td>NeuroSAT</td><td>88.47</td><td>82.30</td><td>79.79</td><td>78.39</td><td>80.23</td><td>80.59</td><td>0.27</td><td>82.17</td><td>89.34</td><td>39.18</td><td>89.23</td><td>88.79</td><td>66.30</td><td>88.34</td><td>63.43</td><td>69.61</td><td>96.74</td><td>98.85</td><td>85.15</td><td>99.36</td><td>99.73</td></tr><tr><td>GCN</td><td>83.74</td><td>73.09</td><td>77.02</td><td>70.34</td><td>74.79</td><td>75.31</td><td>0.17</td><td>75.30</td><td>82.41</td><td>39.66</td><td>82.75</td><td>84.89</td><td>63.85</td><td>82.60</td><td>86.17</td><td>59.29</td><td>97.50</td><td>97.55</td><td>76.83</td><td>99.16</td><td>99.28</td></tr><tr><td>GGNN</td><td>84.13</td><td>76.39</td><td>78.75</td><td>72.87</td><td>76.55</td><td>76.42</td><td>0.29</td><td>78.13</td><td>84.08</td><td>38.82</td><td>84.44</td><td>86.29</td><td>60.80</td><td>84.60</td><td>87.12</td><td>68.36</td><td>97.49</td><td>98.06</td><td>82.06</td><td>-</td><td>99.34</td></tr><tr><td>GIN</td><td>83.81</td><td>81.45</td><td>80.39</td><td>73.99</td><td>78.47</td><td>76.24</td><td>0.20</td><td>78.44</td><td>85.15</td><td>39.13</td><td>85.31</td><td>85.43</td><td>56.85</td><td>84.48</td><td>85.11</td><td>68.93</td><td>96.99</td><td>97.43</td><td>81.49</td><td>99.28</td><td>99.38</td></tr><tr><td rowspan="3">VCG*</td><td>GCN</td><td>83.38</td><td>84.19</td><td>78.00</td><td>76.60</td><td>84.42</td><td>79.23</td><td>14.98</td><td>76.64</td><td>83.79</td><td>51.48</td><td>85.88</td><td>83.06</td><td>56.27</td><td>85.28</td><td>86.91</td><td>66.32</td><td>97.62</td><td>96.74</td><td>78.67</td><td>-</td><td>93.51</td></tr><tr><td>GGNN</td><td>86.30</td><td>87.16</td><td>81.00</td><td>77.96</td><td>88.97</td><td>79.32</td><td>15.11</td><td>76.32</td><td>83.12</td><td>47.67</td><td>86.85</td><td>87.17</td><td>66.86</td><td>86.31</td><td>87.48</td><td>66.42</td><td>-</td><td>98.42</td><td>82.61</td><td>-</td><td>99.52</td></tr><tr><td>GIN</td><td>84.61</td><td>89.56</td><td>83.27</td><td>79.23</td><td>87.65</td><td>81.72</td><td>17.81</td><td>83.28</td><td>86.03</td><td>48.92</td><td>91.21</td><td>85.65</td><td>60.67</td><td>86.12</td><td>88.09</td><td>67.67</td><td>-</td><td>81.01</td><td>99.38</td><td>99.41</td><td></td></tr><tr><td rowspan="4">LCG*</td><td>NeuroSAT</td><td>34.97</td><td>25.00</td><td>37.25</td><td>20.07</td><td>30.40</td><td>41.61</td><td>0.00</td><td>35.45</td><td>70.83</td><td>3.64</td><td>60.28</td><td>71.03</td><td>56.61</td><td>41.45</td><td>-</td><td>52.09</td><td>95.06</td><td>96.18</td><td>74.77</td><td>67.44</td><td>95.99</td></tr><tr><td>GCN</td><td>13.19</td><td>13.76</td><td>19.21</td><td>8.87</td><td>20.50</td><td>24.58</td><td>0.00</td><td>30.20</td><td>54.04</td><td>14.45</td><td>45.16</td><td>56.29</td><td>55.36</td><td>61.82</td><td>66.33</td><td>43.50</td><td>92.86</td><td>94.89</td><td>67.83</td><td>-</td><td>93.84</td></tr><tr><td>GGNN</td><td>14.15</td><td>16.55</td><td>21.18</td><td>7.96</td><td>22.84</td><td>25.68</td><td>0.00</td><td>28.12</td><td>50.66</td><td>2.33</td><td>44.89</td><td>57.96</td><td>52.35</td><td>54.29</td><td>68.91</td><td>49.07</td><td>-</td><td>92.26</td><td>69.21</td><td>66.37</td><td>94.30</td></tr><tr><td>GIN</td><td>15.36</td><td>18.60</td><td>22.17</td><td>9.66</td><td>21.38</td><td>24.93</td><td>0.00</td><td>35.76</td><td>57.81</td><td>2.02</td><td>43.43</td><td>57.62</td><td>53.07</td><td>44.60</td><td>66.32</td><td>44.39</td><td>93.3</td><td>93.82</td><td>70.59</td><td>55.59</td><td>95.69</td></tr><tr><td rowspan="3">VCG*</td><td>GCN</td><td>20.59</td><td>9.21</td><td>22.44</td><td>12.48</td><td>17.00</td><td>29.53</td><td>0.44</td><td>39.04</td><td>48.99</td><td>2.29</td><td>35.99</td><td>55.46</td><td>46.09</td><td>25.90</td><td>68.62</td><td>46.96</td><td>-</td><td>92.68</td><td>69.15</td><td>-</td><td>96.46</td></tr><tr><td>GGNN</td><td>28.04</td><td>27.72</td><td>33.37</td><td>16.46</td><td>29.65</td><td>35.95</td><td>0.56</td><td>48.13</td><td>49.93</td><td>3.12</td><td>51.73</td><td>65.11</td><td>44.26</td><td>48.92</td><td>56.43</td><td>51.01</td><td>-</td><td>71.97</td><td>-</td><td>95.23</td></tr><tr><td>GIN</td><td>26.73</td><td>26.48</td><td>31.97</td><td>14.64</td><td>26.86</td><td>35.81</td><td>0.64</td><td>44.06</td><td>63.84</td><td>3.38</td><td>58.03</td><td>64.66</td><td>55.47</td><td>56.97</td><td>67.78</td><td>46.98</td><td>-</td><td>95.28</td><td>69.40</td><td>-</td><td>96.96</td></tr></table>  

Evaluation across different difficulty levels. The performance of NeuroSAT across different difficulty levels is shown in Figure 8. Notably, training on medium datasets yields superior generalization performance compared to training on easy datasets. This suggests that training on more challenging SAT instances with larger sizes can enhance the model's ability to generalize to a wider range of problem complexities.  

<center>Figure 8: Solving accuracy of NeuroSAT across different difficulty levels (with \(\mathrm{UNS}_2\) as the training loss). The x-axis denotes testing datasets and the y-axis denotes training datasets. </center>