Table 4 shows the testing results on augmented SAT datasets. Notably, training on the augmented instances leads to significant improvements in both satisfiability prediction and satisfying assignment prediction. These improvements can be attributed to the presence of "learned clauses" that effectively modify the structure of the original formulas, thereby facilitating GNNs to solve with relative ease. However, despite the augmented instances being easily solvable using the backtracking search within a few search steps, GNN models fail to effectively handle these instances when trained on the original instances. These findings suggest that GNNs may not implicitly learn the CDCL heuristic when trained for satisfiability prediction or satisfying assignment prediction.  

Table 4: Results on augmented datasets. Values inside/outside parentheses denote the results of models trained on augmented/original instances.   

<table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="2">Easy Datasets</td><td colspan="2">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>SR</td><td>3-SAT</td></tr><tr><td rowspan="2">T1</td><td>NeuroSAT</td><td>100.00 (96.78)</td><td>100.00 (96.06)</td><td>100.00 (84.57)</td><td>96.78 (84.85)</td></tr><tr><td>GGNN</td><td>100.00 (97.66)</td><td>100.00 (95.46)</td><td>100.00 (84.01)</td><td>96.29 (85.80)</td></tr><tr><td rowspan="2">T2</td><td>NeuroSAT</td><td>85.05 (83.28)</td><td>83.50 (81.04)</td><td>51.95 (45.51)</td><td>39.00 (16.52)</td></tr><tr><td>GGNN</td><td>85.35 (83.42)</td><td>81.56 (79.99)</td><td>44.18 (40.09)</td><td>34.67 (14.75)</td></tr></table>  

Table 5: Results using contrastive pretraining. Values in parentheses denote the difference between the results without pretraining.   

<table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="2">Easy Datasets</td><td colspan="2">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>SR</td><td>3-SAT</td></tr><tr><td rowspan="2">T1</td><td>NeuroSAT</td><td>96.68 (+0.68)</td><td>96.23 (+0.10)</td><td>78.31 (+0.29)</td><td>85.02 (+0.12)</td></tr><tr><td>GGNN</td><td>96.46 (+0.29)</td><td>96.45 (+0.20)</td><td>76.34 (+0.78)</td><td>85.17 (+0.06)</td></tr><tr><td rowspan="2">T2</td><td>NeuroSAT</td><td>80.54 (+0.75)</td><td>79.71 (-0.88)</td><td>36.42 (-0.83)</td><td>41.23 (-0.38)</td></tr><tr><td>GGNN</td><td>80.66 (-0.34)</td><td>79.23 (-0.09)</td><td>33.44 (+0.07)</td><td>36.39 (+0.44)</td></tr></table>  

Evaluation with contrastive pretraining. Observing that GNN models exhibit superior performance on clause- learning augmented SAT instances, there is potential to improve the performance of GNNs by learning a latent representation of the original formula similar to its augmented counterpart. Motivated by this, we also experiment with a contrastive learning approach (i.e., SimCLR (Chen et al., 2020)) to pretrain the representation of CNF formulas to be close to their augmented ones (Duan et al., 2022), trying to explicitly embed the CDCL heuristic in the latent space through representation learning.  

The results of contrastive pretraining are presented in Table 5. In contrast to the findings in Duan et al. (2022), our results show limited performance improvement through contrastive pretraining, indicating that GNN models still encounter difficulties in effectively learning the CDCL heuristic in the latent space. This observation aligns with the conclusions drawn in Chen & Yang (2019), which highlight that static GNNs may fail to exactly replicate the same search operations due to the dynamic changes in the graph structure introduced by the clause learning technique.  

### 6.2 Comparison with the LS Heuristic  

Evaluation with random initialization. LS- based SAT solvers typically begin by randomly initializing an assignment and then iteratively flip variables guided by specific heuristics until reaching a satisfying assignment. To compare the behaviors of GNNs with this solving procedure, we first conduct an evaluation of GNN models with randomized initial embeddings in both training and testing, emulating the initialization of LS SAT solvers.  

The results presented in Table 6 demonstrate that using random initialization has a limited impact on the overall performances of GNN- based SAT solvers. This suggests that GNN models do not aim to learn a fixed latent representation of each formula for satisfiability prediction and satisfying assignment prediction. Instead, they have developed a solving strategy that effectively exploits the inherent graph structure of each SAT instance.  

Table 6: Results using random initialization. Values in parentheses denote the difference between the results with learned initialization.   

<table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="2">Easy Datasets</td><td colspan="2">Medium Datasets</td></tr><tr><td>SR</td><td>3-SAT</td><td>SR</td><td>3-SAT</td></tr><tr><td rowspan="2">T1</td><td>NeuroSAT</td><td>97.24 (+1.24)</td><td>96.44 (+0.11)</td><td>77.29 (-0.91)</td><td>84.85 (-0.05)</td></tr><tr><td>GGNN</td><td>96.78 (+0.03)</td><td>96.38 (+0.13)</td><td>76.97 (-0.15)</td><td>85.80 (+0.69)</td></tr><tr><td rowspan="2">T2</td><td>NeuroSAT</td><td>79.09 (-0.70)</td><td>80.79 (+0.20)</td><td>37.27 (+0.02)</td><td>40.75 (-0.86)</td></tr><tr><td>GGNN</td><td>80.10 (-0.90)</td><td>79.83 (+0.51)</td><td>32.85 (-0.52)</td><td>36.59 (+0.64)</td></tr></table>  

Evaluation on the predicted assignments. Under random initialization, we further analyze the solving strategies of GNNs by evaluating their predicted assignments decoded from the latent space. For the task of satisfiability prediction, we employ the 2- clustering decoding algorithm to extract the predicted assignments from the literal embeddings of NeuroSAT at each iteration of message passing. For satisfying assignment