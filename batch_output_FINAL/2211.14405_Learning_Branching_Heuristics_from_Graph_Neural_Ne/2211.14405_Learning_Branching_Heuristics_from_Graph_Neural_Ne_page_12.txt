average number \(^2\) of their branches on the instances that dominating cliques do not exist. Figure 1d shows such ratios on the instances that dominating cliques exist. It is reasonable that the GNN- enhanced solvers perform better on the instances that dominating cliques do not exist. For such case, solvers go through the whole search tree, so the learned branching heuristic can prune more search space. Instead, for the case of solving instances that dominating cliques exist, the original heuristic might be equally effective since solvers return once a dominating clique is found.  

### 4.3 Finding Minimum Dominating Cliques  

To find the minimum dominating clique, we add the following features to our dominating- clique solvers. We record the currently minimum dominating clique and update it once we find a dominating clique with lower cardinality, and so on until completing the whole backtracking search. In addition, we apply backjumping to prune the search space. Our evaluation order of the random variables in \(C \cap S\) is the decreasing order of the numbers of the unsatisfied clauses that these random variables involve. Thus, we can backjump three levels whenever we get the first dominating clique or find a dominating clique with lower cardinality than the currently minimum solution. Also, we can backjump two levels whenever the current recursive depth is equal to the cardinality of the currently minimum dominating clique.  

We prepare three GNN models for the experiments. Their loss functions are (5), (6) with \(\mathbb{E}(|S|) = \sum_{i = 1}^{n} p_{i}\) , and (6) with \(\mathbb{E}(|S|)\) as (7) respectively. Combined with the two calculations of joint information entropy, we have six solvers. Figure 2 shows the experimental results on test data for finding the minimum dominating clique. In particular, Figure 2a shows the winning ratios of the four solvers: the original solver and the other three GNN- enhanced solvers using the fast calculation of joint information entropy. We use (5) in the figure to represent the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (5). The (6) in the figure represents the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (6) where \(\mathbb{E}(|S|) = \sum_{i = 1}^{n} p_{i}\) . The (7) in the figure represents the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (6) where \(E(|S|)\) as (7). Figure 2b shows the winning ratios of these four solvers using the accurate calculation of joint information entropy. Figure 2c shows the ratios of these six GNN- enhanced solvers to the original solver in terms of the geometric average of branches. From Figure 2a and Figure 2b, we can see that the loss functions that have the term \(E(|S|)\) performs better than the loss function (5). Another observation from Figure 2c is that either the accurate calculation of joint information entropy or the accurate calculation of \(\mathbb{E}(|S|)\) improves the solver's performance.