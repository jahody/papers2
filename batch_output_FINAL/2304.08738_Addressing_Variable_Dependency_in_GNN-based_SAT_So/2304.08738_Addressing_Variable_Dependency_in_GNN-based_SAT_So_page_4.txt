<center>Figure 1: (a) XOR implemented by AIG; (b) the DAG representation of (a); (c) the equi-satisfiable CNF with additional variable \(a\) and \(b\) , and the corresponding bipartite graph of XOR (here dotted line means the variable is negated in the clause). </center>  

predecessors) from those that leaves \(v\) (the successors). We only use the messages from predecessors in the forward pass, and likewise, the successors in the backward pass. The incoming messages are aggregated by an aggregator function \(\mathcal{A}\) , which is invariant to permutation of the elements in the input set. Finally, the aggregated message is used to update the hidden state of \(v\) by a standard GRU function \(GRU(\cdot)\) [Cho et al., 2014].  

In AsymSAT, message passing follows the topological order. In the forward pass, messages flow from circuit input nodes (which have no predecessors) to the only circuit output node (which has no successors). The hidden state vectors are updated sequentially. In the backward pass, messages flow from the circuit output node to the circuit input nodes. In each pass, the hidden state vectors are updated according to the following rule:  

\[x_{v}^{(k + 1)}\coloneqq GRU\left(p_{v},\mathcal{A}\left(\left\{m_{n}^{(k)}|n\in \mathcal{N}(v)\right\}\right)\right) \quad (1)\]  

Initially, \(p_{v} = N_{G}(v)\) , which is the node type vector of node \(v\) . So in the first forward pass, the type of a node is encoded into the hidden state vector. In all remaining passes, \(p_{v} = x_{v}^{(k)}\) , which is the hidden state vector resulted from the previous pass. We use three separate GRUs: \(GRU_{init}(\cdot)\) , \(GRU_{f}(\cdot)\) , \(GRU_{b}(\cdot)\) . Among the three, \(GRU_{init}(\cdot)\) is only used in the first forward pass. \(GRU_{f}(\cdot)\) is used for all remaining forward passes. \(GRU_{b}(\cdot)\) is used in the backward passes. We call one forward pass followed by one backward pass as an iteration. In Equation 1, \(\mathcal{N}(v)\) is either the predecessors or the successors of \(v\) . Their hidden state vectors are encoded into messages \(m_{n}^{(k)}\) by a learnable function \(\mathcal{M}: x_{n}^{(k)} \to m_{n}^{(k)}\) .  

Our graph embedding layers share some similarities with [Amizadeh et al., 2018] as both are built upon DAG- RNN. The difference here is mainly in the computation between two iterations. We use two GRUs for forward passes, because the size of a hidden state vector is different from that of the node type vector, whereas [Amizadeh et al., 2018] introduced a function to project hidden state vectors into the space  

of node type vectors after each iteration to keep the same dimensionality and use the same GRU. We argue that the projection could potentially introduce a loss of information and therefore, we employ two separate GRUs in the forward pass: \(GRU_{init}(\cdot)\) , \(GRU_{f}(\cdot)\) to handle either the node type vector or a hidden state vector from the previous pass.  

SAT assignment decoding layers. As discussed earlier, we would like to predict the Boolean assignments on the circuit input nodes sequentially. In this node- level prediction, we map a sequence of hidden state vectors of the circuit input nodes \(X = (x_{i_{1}}, x_{i_{2}}, x_{i_{3}}, \ldots)\) to a sequence of input assignment \(L\) . After iterations of message passing, these hidden state vectors encode the information related to the structure of the graph. If two input nodes are symmetric with respect to each other, we expect that their hidden state vectors will be the same. If we individually use each of these vectors to decode a 0- 1 assignment (namely, the concurrent prediction), the symmetric nodes will certainly map to the same variable assignment. As we have discussed in Section 3, SAT solutions must take variable dependency into consideration, therefore, in our model, we need to associate variable assignments of the same SAT problem.  

In our AsymSAT, we use a recurrent neural network (referred to as the \(\mathcal{R}\) layer) to generate sequential predictions on variable assignments, so that the model output on a certain circuit input node depends on the predictions of other nodes. We make this \(\mathcal{R}\) layer bi- directional to account for dependencies from both sides. A subsequent MLP will work as a selector to decide which direction is more preferred. Sequential prediction mimics classic (non- machine- learning- based) SAT solvers. These classic SAT solvers like GRASP [Marques- Silva and Sakallah, 1999] or MiniSAT [Sorensson and Een, 2005] pick decision variables one after another. Regarding the aforementioned XOR example, we expect this RNN layer will be able to learn to predict different variable assignments for the two symmetric variables after training with such examples.  

As a summary, we show the overall architecture of our