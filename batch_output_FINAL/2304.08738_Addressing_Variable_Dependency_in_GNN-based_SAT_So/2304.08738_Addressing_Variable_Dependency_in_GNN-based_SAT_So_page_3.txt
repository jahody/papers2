gate or a circuit input) are encoded as one- hot vectors, which are the inputs to the GNN model. Similar to NeuroSAT, message- passing generates node embeddings which are used to predict variable assignments. A message- passing iteration consists of one forward pass from circuit input nodes to the only circuit output node and one backward pass in the reversed order. Compared to NeuroSAT, message- passing in DG- DAGRNN follows the topological order and sequentially update node embeddings, whereas NeuroSAT updates all node embeddings concurrently.  

Both NeuroSAT and DG- DAGRNN predict SAT solutions concurrently without considering dependency among variables. This results in a fundamental weakness: they are unable to predict the correct solutions for certain symmetric graphs, as explained in the next section.  

## 3 Variable Dependency in SAT Solving  

Generally speaking, SAT and Circuit- SAT solving must consider variable dependency. In other words, they must "remember" what predictions have been made so far. A simple example is the 2- input XOR \((x \oplus y)\) . Here, \(x\) and \(y\) are symmetric — if we swap them, we will get exactly the same formula because XOR is commutative. However, we must assign different values to \(x\) and \(y\) in order to get a 1 as the result. If \(x\) has been assigned as 1, then \(y\) must be 0. This is the dependency between these two variables.  

Symmetry naturally exists in many SAT problems. Sometimes, it is part of the formula that is symmetric — for example, \((x \oplus y) \wedge z\) . When converted to AIG or CNF, a symmetric formula like \(x \oplus y\) will result in a symmetric AIG or CNF, as shown by Figure 1. It is not hard to see, symmetric nodes have symmetric predecessors and successors. Therefore, when GNN- based SAT solvers use message- passing to encode the graph structure, symmetric nodes will have the same node embeddings, unless they are distinguished by initialization. However, pure random initialization for all nodes provides no extra information for the neural network to distinguish the symmetric ones. On the other hand, a bias in initialization would introduce artefact that does not generalize. Therefore, prior works [Selsam et al., 2018; Amizadeh et al., 2018; Zhang et al., 2020] all used equal initial embeddings and therefore, they would not be able to distinguish symmetric nodes when predicting SAT assignments. We accompany our argument on random initialization with experimental results in the appendix.  

When individual node embeddings are directly used to predict variable assignments without considering the dependency among them, the inferred assignments will always be the same for the pair of symmetric nodes. As we have shown by the 2- input XOR example, some symmetric formulas reject equal variable assignments as their satisfying solutions. Therefore, NeuroSAT and DG- DAGRNN in [Selsam et al., 2018] and [Amizadeh et al., 2018] are unable to deal with these symmetric SAT or Circuit- SAT problems. We argue that a GNN- based SAT solver should sequentially predict variable assignments in order to take variable dependency into consideration. This is achieved by a recurrent neural network added in our model, explained in the next section.  

## 4 Our Methods  

In this section, we explain our approach where RNN is used for dependent predictions. Specifically, we focus on the Circuit- SAT problem because a CNF formula for SAT problems can be converted into a circuit structure. We formulate solving Circuit- SAT problems as a supervised learning process as the following.  

### 4.1 Problem formulation  

Problem input. We expect the problem input to be a DAG representing the structure of the circuit. As discussed in Section 2, we only need to consider circuits made from AND gates and NOT gates. Each node in the DAG has an one- hot input feature vector that indicates the type of the node. There are in total three types: the primary inputs, AND gates and NOT gates. Formally, we expect the problem input to be the form of \(G = < V_G, N_G, E_G >\) , where \(V_G\) is a set of circuit nodes, \(N_G\) is a function that maps each node to its type, and \(E_G\) is the set of directed edges of the circuit graph. An edge between two nodes means that there is a wire connection from a logic gate or a circuit input to another gate.  

Problem output. The machine learning model should predict a 0- 1 assignment for each circuit input node. We denote the assignments as \(L \in \{0, 1\}^i\) and \(i\) is the number of circuit input nodes. Each instance in the dataset is in the form of \((G, L)\) , where the 0- 1 assignments are generated by an external SAT solver that works as the oracle.  

### 4.2 The Proposed GNN Architecture  

In the high- level, we would like to build a machine- learning model that learns the mapping from a circuit graph to the 0- 1 assignment on input nodes: \(f: G \to L\) . There are plenty of existing GNN models that are designed to handle input data organized as a graph [Yolcu and Póczos, 2019; Selsam et al., 2018; Selsam and Björner, 2019]. There, each graph node is associated with a vector (the hidden state vector) which eventually represents some structural information around the node. Nodes exchange their knowledge of the graph structure by sending messages to their neighbors, and the hidden states will be gradually updated. The propagation of information is referred to as the message- passing mechanism, which essentially embeds the information about the graph structure into the hidden states.  

Graph embedding layers. When it comes to the implementation of message passing, there are various choices, for example, which direction the message flows towards, how to aggregate messages from several nodes, what is the order of hidden state updates. Therefore, different variants of message- passing can be implemented. In this work, we build upon the DAG- RNN framework [Shuai et al., 2016] to create a GNN architecture for sequential variable assignment prediction.  

To better explain our GNN architecture, we introduce the following notations. Each graph node \(v \in V_G\) is associated with a \(d\) - dimensional hidden state vector \(x_v\) , which is iteratively updated based on the messages from neighboring nodes. During message- passing, we distinguish the nodes that reach \(v\) following a directed edge (in other words, the