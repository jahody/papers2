<center>Figure 2: (a) An example of an AIG circuit, (b) the graph embedding layers (for simplicity, we only draw the connections for two nodes.) (c) the SAT assignment decoding layers </center>  

Table 1: Percentage of the symmetric circuit problem solved   

<table><tr><td>AsymSAT w. LSTM</td><td>AsymSAT w. GRU</td><td>AsymSAT w.o. R layer</td><td>NeuroSAT DG-DAGRNN</td></tr><tr><td>100.00%</td><td>100.00%</td><td>0.00%</td><td>0.00%</td></tr></table>  

AsymSAT model in Figure 2.  

### 4.3 Training  

For AsymSAT, we apply the supervised learning method. We consider SAT solution prediction as a labeling problem â€” giving 0- 1 labels to each of the circuit input nodes. We use Cross- Entropy for the loss function, denoted as:  

\[\begin{array}{c}{Loss = -\Sigma_{i = 1}^{n}[g(v_{i})log(P(v_{i} = 1)) + }\\ {(1 - g(v_{i}))log(P(v_{i} = 0))]} \end{array} \quad (2)\]  

where \(g(v_{i})\) is the ground truth of the SAT assignment on \(v_{i}\) generated by an oracle SAT solver, \(P(v_{i} = 1)\) and \(P(v_{i} = 0)\) are the predicted probability of \(v_{i}\) to be 1 or 0.  

## 5 Experimental Evaluation  

### 5.1 Data preparation  

We prepare three datasets in total: the small- scale symmetric circuit examples, medium- size CNF formulas, and large random circuits with more than 1K logic gates.  

Small- scale symmetric AIG with asymmetric solutions. We manually construct 10 circuits with no more than 3 inputs. Within each circuit, there are at least two input nodes that are symmetric but require distinct assignments. We intentionally keep this training set small. If NeuroSAT and DG- DAGRNN are capable of handling symmetric circuits with asymmetric SAT solutions, they should easily reach a high training accuracy on this small dataset. However, our experiment result later will show that they are unable to predict any SAT solutions for this dataset.  

Medium- size randomly generated CNF formulas. We generate random CNF formulas in the same way as described by [Selsam et al., 2018]. We refer to this dataset as the \(SR(n)\) problem, where \(n\) is the number of variables. CNF formula  

for \(SR(n)\) problems can be converted into the circuit form using the principle of Shannon's Decomposition as suggested by [Amizadeh et al., 2018].  

Large randomly generated AIGs. We generate random AIGs using the AIGEN tool [Jacobs and Sakr, 2021], which was designed to create random test circuits to check and profile the EDA tools. By default, AIGEN generates sequential logic circuits (those with storage elements). We extract the combinational logic circuits from the sequential logic circuits. We refer to this dataset as the \(V(n)\) problem, where \(n\) stands for the number of circuit input nodes. \(V(n)\) problems can be converted into CNF using Tseitin transformation. Compared to \(SR(n)\) problems, \(V(n)\) is a nontrivial dataset even when \(n\) is relatively small. For example, each \(V(10)\) problem has more than 1K logic gates on average. The corresponding CNF formulas contain more than 1K variables, which is much larger than the largest dataset \(SR(40)\) used in the prior work [Selsam et al., 2018].  

### 5.2 Experimental setup and result  

The dimension on the outcome of the \(\mathcal{R}\) layer is 10 and we use the Adam optimizer during training process. For NeuroSAT, and DG- DAGRNN, we follow the same configurations as described in [Selsam et al., 2018] and [Amizadeh et al., 2018]. To our best knowledge, the source code for the original DG- DAGRNN model is not publicly available. We build this model following the instructions in [Amizadeh et al., 2018]. We train and test all three models on a server with two NVIDIA GeForce RTX 3090 GPUs.  

## Experiments for AsymSAT Configurations  

Effectiveness of the RNN decoding layer. In this experiment, we train our AsymSAT model, the NeuroSAT model and the DG- DAGRNN model on the same 10 symmetric circuits and measure the training accuracy. We use two con