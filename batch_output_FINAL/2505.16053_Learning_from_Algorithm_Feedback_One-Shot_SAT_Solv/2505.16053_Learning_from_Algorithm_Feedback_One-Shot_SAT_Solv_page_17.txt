## B.4 Supervised UNSAT-Core and Backbone Prediction  

For UNSAT- core and backbone prediction in Section 3.2, we train supervised GNN models that are identical in architecture and size to the models used for our RLAF- based policies. The used hyperparameters are specified in Table 4. In the following, we provide a detailed description of how these models are trained and evaluated.  

## UNSAT-Core  

Selsam and Bjørner (2019) propose to train supervised models that predict the UNSAT- core membership of variables and then use the model prediction to guide branching heuristics. Following their methodology, we phrase the task of predicting whether or not a variable occurs in an UNSAT- core as a variable- level binary classification task and train a GNN for this problem in a supervised manner using a standard cross- entropy loss. The ground- truth on training and validation instances is computed by extracting the cores from DRAT UNSAT proofs generated by the CaDiCaL Biere et al. (2024) solver. Note that these cores are not minimal, as computing such would not be feasible. As discussed in Section 3.2, the extracted cores on our unsatisfiable 3SAT training instances contain all variables for almost all instances and are therefore not a meaningful training target. We therefore only train UNSAT- core prediction models for 3COL and CRYPTO. We train a separate model for each distribution and restrict training to the unsatisfiable instances.  

Note that Selsam and Bjørner (2019) integrate their prediction by periodically resetting the VSIDS scores of the guided CDCL- solver to prediction logits of the GNN. This requires careful tuning of the reset frequency. It is also specific to solvers based on the VSIDS heuristic and would, for example, not be applicable to the March solver. Furthermore, in later ablation experiments, Selsam and Bjørner (2019) report that the performance improvement obtained with a trained GNN is barely distinguishable from when an untrained, randomly initialized model is used, further questioning the effectiveness of guiding solvers with this strategy. To facilitate a direct and fair comparison with RLAF- trained policies, we instead combine the UNSAT- core predictions with our own solver guidance based on multiplicative weights. For a variable \(x\) , let \(p_{\mathrm{core}}(x)\) be the predicted probability of \(x\) being in an UNSAT- core according to the trained GNN model. Then we transform these probabilities to variable weights through the following transformation:  

\[w(x) = 1 + \alpha \cdot p_{\mathrm{core}}(x). \quad (21)\]  

Here, \(\alpha \geq 0\) is a parameter that determines how the variable weight scales with the raw model predictions. For this experiment, we found weights of \(w(x) \geq 1\) to perform better, hence the offset of 1 in Equation (21). The value of \(\alpha\) is tuned on the corresponding validation dataset in the range \(\{10^{- 4}, 10^{- 3}, 10^{- 2}, 10^{- 1}, 10^{0}, 10^{1}, 10^{2}, 10^{3}, 10^{4}\}\) . We tune \(\alpha\) separately for both Glucose and March. The polarities are simply set to \(p(x) = 1\) as the prediction of UNSAT- core membership has no clear implication for the sign of the branching literal. Using this methodology, we found that the UNSAT- core predictions can significantly accelerate both base solvers, although by a smaller margin than RLAF- trained policies.  

## Backbone  

Wang et al. (2024) suggests using the backbone membership of literals as a supervised training target and then setting variable polarities using the model predictions. We follow their methodology and train a GNN on the literal- level binary classification task using cross- entropy loss. As discussed in Section 3.2, we only train a model for the 3SAT instances and only use the satisfiable problems for training. The backbone of coloring problems is always empty due to the permutation symmetry of the colors, and some distributions, such as CRYPTO, predominantly consist of UNSAT instances.  

When evaluating, we set the polarity of a variable \(x\) as \(p(x) = 0\) if \(p_{\mathrm{backbone}}(-\alpha) > p_{\mathrm{backbone}}(x)\) and \(p(x) = 1\) otherwise. Here, \(p_{\mathrm{backbone}}(\ell)\) is the predicted probability of literal \(\ell\) belonging to the backbone. We further assign variable weights \(w(x)\) under the assumption that correctly assigning backbone literals in early search steps positively affects the runtime. To this end, we apply the transformation from Equation (21) to the mean backbone probability \(\overline{p}_{\mathrm{backbone}}(x) = 0.5(p_{\mathrm{backbone}}(-\alpha) + p_{\mathrm{backbone}}(x))\) to obtain a weight for each variable. Again, we tune the transformation parameter \(\alpha\) for both base solvers on the validation set.