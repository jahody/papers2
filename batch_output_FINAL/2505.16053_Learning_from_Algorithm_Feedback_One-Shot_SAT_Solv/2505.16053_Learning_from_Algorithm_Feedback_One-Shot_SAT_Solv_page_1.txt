# Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs  

Jan TÃ¶nshoff \* RWTH Aachen University  

Martin Grohe RWTH Aachen University  

## Abstract  

Boolean Satisfiability (SAT) solvers are foundational to computer science, yet their performance typically hinges on hand- crafted heuristics. This work introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm for learning to guide SAT solver branching heuristics with Graph Neural Networks (GNNs). Central to our approach is a novel and generic mechanism for injecting inferred variable weights and polarities into the branching heuristics of existing SAT solvers. In a single forward pass, a GNN assigns these parameters to all variables. Casting this one- shot guidance as a reinforcement learning problem lets us train the GNN with off- the- shelf policy- gradient methods, such as GRPO, directly using the solver's computational cost as the sole reward signal. Extensive evaluations demonstrate that RLAF- trained policies significantly reduce the mean solve times of different base solvers across diverse SAT problem distributions, achieving more than a 2x speedup in some cases, while generalizing effectively to larger and harder problems after training. Notably, these policies consistently outperform expert- supervised approaches based on learning handcrafted weighting heuristics, offering a promising path towards data- driven heuristic design in combinatorial optimization.  

## 1 Introduction  

Solving computationally hard combinatorial problems, such as Boolean satisfiability (SAT), remains a cornerstone of computer science and is critical to diverse domains such as verification, planning, and cryptography (Biere et al., 2021). Complete search algorithms are of particular importance, as they are guaranteed to find a solution if one exists or prove unsatisfiability otherwise. The runtime of these classical algorithms heavily depends on hand- crafted heuristics to navigate the solution space, for example, by determining variable assignments during the search. Such heuristics are often rigid and hard to adapt to specific instance distributions without extensive expert knowledge and tuning. Machine learning offers a compelling alternative: Augmenting the heuristic components of classical search algorithms with trainable functions allows us to construct adaptable solvers. Specifically, reinforcement learning (RL) can train these extended solvers to learn improved, distribution- specific heuristics in a data- driven manner without direct expert supervision.  

In this work, we study how to leverage RL- trained Graph Neural Network (GNNs) to improve branching heuristics of SAT solvers. Our main contributions are as follows: First, we introduce a novel and generic method for integrating variable- wise weights into the branching heuristics of existing SAT solvers. Secondly, we construct a GNN- based policy that assigns a weight and polarity to each variable in one forward pass. This one- shot setting enables a single GNN pass to influence every branching decision, avoiding costly repeat passes. Thirdly, we phrase the task of inferring weights and polarities that reduce the solver's cost as an RL problem. The reward signal is directly obtained from the observed computational cost of the guided SAT solver, requiring no expert supervision. We refer