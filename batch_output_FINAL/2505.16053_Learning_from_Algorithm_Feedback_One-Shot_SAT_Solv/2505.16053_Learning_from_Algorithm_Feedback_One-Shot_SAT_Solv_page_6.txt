Here, \(\epsilon \in (0,1)\) is a hyperparameter, and \(r_{i,j}(\theta)\) is defined as the probability ratio of the new policy and the policy learned in the previous GRPO iteration:  

\[r_{i,j}(\theta) = \frac{\pi_{\theta}(\mathcal{W}_{i,j}|\phi_{i})}{\pi_{\theta_{k - 1}}(\mathcal{W}_{i,j}|\phi_{i})} \quad (9)\]  

This objective aims to adjust the policy such that actions (e.g., variable parameterizations) with high advantage become more likely while avoiding excessively large distribution shifts by clipping the objective at a probability ratio determined by \(\epsilon\) . The full training objective combines \(\mathcal{L}_{\mathrm{PPO}}\) with an additional term that penalizes the KL divergence relative to the previous model weights \(\theta_{k - 1}\) to stabilize training further:  

\[\mathcal{L}(\theta \mid \phi_{i}) = \mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) - \beta \cdot \mathrm{KL}\left(\pi_{\theta}(\phi_{i}),\pi_{\theta_{k - 1}}(\phi_{i})\right). \quad (10)\]  

Here, the weight \(\beta \geq 0\) is an additional hyperparameter. Starting from the previous model weights \(\theta_{k - 1}\) , we learn updated model weights \(\theta_{k}\) by performing stochastic gradient ascent for a fixed number of steps to maximize this objective function for all training instances. This overall process repeats in the next round of GRPO. In the appendix, Algorithm 4 provides a complete formal specification of our training.  

### 2.5 Training Setup  

We are utilizing GRPO as an online RL algorithm to learn the parameters of our policy GNN directly from observed solver costs. As a consequence, we train with the SAT solver in- the- loop and make \(M\cdot N\) calls to the solver per GRPO iteration. With our default parameters ( \(N = 100\) , \(M = 40\) ) we make 4000 SAT solver calls in each iteration. This imposes the practical constraint to train on a distribution \(\Omega\) of SAT problems where this number of solver calls is possible in an acceptable time on the underlying hardware. The work presented here intends to be a small- scale demonstration of RLAF as a training paradigm, and all training is performed on machines with one (multi- core) CPU and one GPU. Therefore, the training data in our experiments is chosen so that each instance is solvable by the given base solvers within a fraction of a second. In future work, the hardness and size of the training problems can be scaled up substantially by leveraging a distributed compute cluster for collecting the SAT solver feedback. Crucially, we demonstrate in Section 3 that after training, the learned policies do generalize to significantly harder and larger problems. The reliance on comparatively easy training problems is therefore not a significant limitation for learning effective GNN- guidance with RLAF.  

## 3 Experiments  

In our experiments<sup>2</sup>, we aim to answer two primary research questions: (i) Can RLAF train GNN- based guidance policies that shorten solver runtimes and generalize to harder formulas? (ii) How do RLAF- trained policies fare against guidance based on learning predefined notions of variable importance in a supervised manner? Furthermore, we want to understand whether the learned policies capture known variable properties after training and whether the policies learned with different solvers are related or solver- specific.  

Solvers We conduct experiments with two distinct base solvers: The well- known CDCL solver Glucose (Audemard and Simon, 2017) and the DPLL solver March (Heule et al., 2005). Glucose uses the VSIDS branching heuristic and is comparatively strong on structured problems, while March uses a look- ahead branching heuristic and is among the best- known solvers for random instances. We provide more technical details about how RLAF is integrated into both solvers in Appendix A.3.  

Data We consider three well- known classes of SAT problems with significantly different structures to study how well RLAF can adapt the base solvers to each of them. In Appendix B.1 we provide full details on the data generation and dataset statistics. Random 3SAT: We define 3SAT \((n)\) as the distribution of uniformly random 3SAT instances with \(n\) variables and clause- to- variable ratio of 4.26, which is approximately the critical density where the instances transition from SAT to UNSAT. The training data consists of 20K instances sampled from 3SAT(200). We test on larger instances with \(n \in \{300, 350, 400\}\) , where we sample 200 instances for each size \(n\) . Graph Coloring: We consider