to this training paradigm as Reinforcement Learning from Algorithm Feedback (RLAF). Finally, we demonstrate empirically that modern RL techniques, such as GRPO (Shao et al., 2024), are capable of training effective RLAF policies for different base solvers. The learned policies substantially reduce solver runtimes, generalize to harder problems after training, outperform supervised baselines, and appear to capture solver- agnostic structural properties of SAT problems.  

### 1.1 Background  

SAT Solving A Boolean formula in Conjunctive Normal Form (CNF) is a conjunction of clauses \(\phi = C_{1} \wedge \dots \wedge C_{m}\) , each clause being a disjunction of one or more literals \(C_{j} = (\ell_{j,1} \vee \dots \vee \ell_{j,k})\) . We denote by \(\mathrm{Var}(\phi) = \{x_{1}, \ldots , x_{n}\}\) the set of Boolean variables of \(\phi\) . The Boolean SAT problem is to decide whether or not there exists a satisfying assignment \(\alpha : \mathrm{Var}(\phi) \to \{0, 1\}\) that satisfies all clauses of a given formula \(\phi\) . This problem is well- known to be NP- complete and naturally arises in a wide range of applications (Biere et al., 2021). Modern SAT solvers predominantly stem from the Davis- Putnam- Logemann- Loveland (DPLL) algorithm, a backtracking search approach enhanced by unit propagation and pure literal elimination. Algorithm 1 provides a pseudocode description of a DPLL SAT solver. Many extensions of this general idea have been proposed to scale SAT solvers to larger, industrial instances. In particular, Conflict- Driven Clause Learning (CDCL) solvers significantly extend the DPLL framework by introducing clause learning and non- chronological backtracking. A common property of DPLL- derived solvers is the importance of the branching heuristic that picks the next branching literal in each search step (line 11 in Algorithm 1). Various branching heuristics have been proposed, and which heuristic performs best often depends on the structure of the given SAT formula \(\phi\) (Kullmann, 2021). Customizing branching heuristics towards a specific distribution of inputs generally requires expert knowledge and significant trial and error.  

Reinforcement Learning Reinforcement learning (RL) aims to learn policies for sequential decision- making problems where an agent interacts with an environment to learn through trial and error. An RL problem is usually formalized as a Markov Decision Process (MDP), which is defined as a tuple \((\mathcal{S}, \mathcal{A}, P, R)\) , where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of possible actions, \(P\) denotes the transition probabilities between states and \(R\) is the reward function. In probabilistic settings, policies \(\pi\) are stochastic mappings from states to distributions over actions, i.e., \(\pi (a|s)\) indicates the probability of selecting action \(a\) in state \(s\) . More generally, for continuous action spaces, i.e. \(\mathcal{A} = \mathbb{R}\) , \(\pi (a|s)\) is the probability density that a policy assigns to an action \(a\) . The primary objective in RL is to determine an optimal policy \(\pi^{*}\) that maximizes the expected cumulative discounted reward \(\mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^{t} R(s_{t}, a_{t}) \right]\) , where \(\gamma \in [0, 1)\) represents a discount factor that emphasizes earlier rewards. This formulation is the basis for various RL algorithms. Recently, RL algorithms based on policy gradients, such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) have been used extensively to fine- tune LLMs from human feedback (RLHF, Christiano et al. (2017); Ouyang et al. (2022)) and from verifiable rewards (RLVR, Lambert et al. (2024); Guo et al. (2025)).  

### 1.2 Related Work  

Leveraging deep learning in the context of combinatorial optimization (CO) problems has emerged as a major area of research (Cappart et al., 2021) and has been applied to a wide range of problems such as combinatorial graph problems (Khalil et al., 2017), SAT solving (Selsam et al., 2019), Mixed- Integer Programming (Khalil et al., 2022), and Constraint Satisfaction Problems (Tönshoff et al., 2023). Here, we primarily focus on work that aims to enhance SAT solvers with (graph) neural networks. One line of work suggests using predictions of predefined variable properties to guide SAT solver branching heuristics. Selsam and Björner (2019) train a GNN to predict whether variables belong to an UNSAT core. The branching heuristic is then guided by periodically resetting the solver's VSIDS scores to the GNN's predictions, thus making the guidance specific to VSIDS- based CDCL solvers and dependent on careful tuning of the reset frequency. Wang et al. (2024) predict whether literals occur in the backbone of satisfiable formulas and use these predictions to set the polarity of variables. Another line of work explores purely RL- based training for enhancing branching heuristics, eliminating the need for expert supervision. Kurin et al. (2020) uses Q- learning to train GNNs end- to- end as branching policies to minimize solver runtime, and Cameron et al. (2024) propose Monte Carlo Forest Search for guiding early branching decisions in SAT Solvers on UNSAT problems. Both methods require one GNN forward pass per guided branching decision, which creates a significant bottleneck as the GNN usually requires orders of magnitude more runtime than classical branching