<center>Figure 3: Learning to accelerate a SAT solver with GRPO: a) For a given training formula \(\phi\) sample multiple variable parameterizations i.i.d. from the current policy \(\pi_{\theta}(\phi)\) . b) Run the SAT solver on \(\phi\) with each parameterization. c) Map the cost of each solver run (i.e. the number of decisions) to the normalized group-relative advantage \(\hat{A} (\phi ,\mathcal{W})\) . d) Optimize the model weights \(\theta\) to maximize \(\mathcal{L}_{\mathrm{PPO}}\) to shift the policy towards faster parameterizations. </center>  

Note that all trainable weights of the GNN model have a partial derivative with respect to \(\pi_{\theta}(\mathcal{W}|\phi)\) for a given \(\mathcal{W}\) , which enables us to train with policy- gradient methods such as GRPO. During training, we sample multiple \(\mathcal{W}\) i.i.d. from \(\pi_{\theta}(\phi)\) and use the variance of the observed solver runtimes to compute our training signal, as explained in Section 2.4. At test time, we do not sample randomly from \(\pi_{\theta}(\phi)\) but simply use the mode \(\hat{\mathcal{W}}\) , which deterministically chooses the most probable weight and polarity for each variable \(x\) . This eliminates a source of variance when testing and, on average, yields better results than sampling at random from the learned policy.  

### 2.4 Policy Optimization  

Our aim is to learn a policy GNN that guides the SAT solver towards lower computational costs on a given distribution of SAT instances. Formally, let \(\Omega\) be some training distribution of SAT problems. The objective is to learn model weights \(\theta\) that minimize the expected solver cost when applying the learned policy to instances sampled from \(\Omega\) :  

\[\theta^{*} = \underset {\theta}{\arg \min}\underset {\phi \sim \Omega ,\mathcal{W}\sim \pi_{\theta}(\phi)}{\mathbb{E}}[\mathrm{Cost}(\phi ,\mathcal{W})]. \quad (6)\]  

Here, \(\mathrm{Cost}(\phi ,\mathcal{W})\) is defined as the number of decisions required when running \(\mathrm{Solve}(\phi ,\mathcal{W})\) , which is the primary target metric we aim to minimize. We can view this objective as an RL problem by modeling the process of choosing \(\mathcal{W}\) as a single- step Markov Decision Process (MDP) where the input formula \(\phi\) is viewed as the state, and a single- step episode unfolds by choosing a variable parameterization \(\mathcal{W}\) as the action. Once the action is taken, the environment transitions immediately to a terminal state, yielding a reward \(R(\phi ,\mathcal{W}) = - \mathrm{Cost}(\phi ,\mathcal{W})\) that is the negative of the solver's cost (e.g., number of decisions). Note that we also experimented with directly using CPU time as a cost measure, but found this to yield less stable training due to the performance variance caused by noisy CPU utilization.  

We leverage Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to learn a policy for this RL problem. GRPO is a simplification of Proximal Policy Optimization (PPO) (Schulman et al., 2017) that eliminates the need for learning an additional value network. The initial model weights \(\theta_{0}\) are sampled at random. GRPO updates these model weights in iterations \(k\in \{1,\ldots ,K\}\) In iteration \(k\) , we first sample a batch of training instances from \(\mathcal{F} = \{\phi_{1},\ldots ,\phi_{N}\} \sim \Omega^{N}\) from the given training distribution. For each such formula \(\phi_{i}\) we sample \(M\) variable parameterizations \(\mathcal{W}_{i,1},\ldots ,\mathcal{W}_{i,M}\sim \pi_{\theta_{k - 1}}(\phi_{i})\) i.i.d. from the current policy. We then run \(\mathrm{Solve}(\phi_{i},\mathcal{W}_{i,j})\) for all \(i,j\in [N]\times [M]\) and measure the corresponding cost and reward. The group- relative advantage is then defined as  

\[\hat{A}_{i,j} = \frac{R(\phi_{i},\mathcal{W}_{i,j}) - \mathrm{mean}(\mathbf{R}_{i})}{\mathrm{std}(\mathbf{R}_{i})} \quad (7)\]  

where \(\mathbf{R}_{i} = \{R(\phi_{i},\mathcal{W}_{i,j}) \mid j\in \{1,\ldots ,M\} \}\) is the set of all rewards collected for the same instance \(\phi_{i}\) . The main objective is to maximize the clipped policy update function for each training instance \(\phi_{i}\) :  

\[\mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) = \frac{1}{M}\sum_{j}\left[\min \left(r_{i,j}(\theta)\hat{A}_{i,j},\mathrm{clip}(r_{i,j}(\theta),1 - \epsilon ,1 + \epsilon)\hat{A}_{i,j}\right)\right]. \quad (8)\]