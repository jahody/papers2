cost classifier to predict if the entire set of features can be computed within a time threshold and an algorithm selector for instances with feature cost less or equal to this threshold. Whereas the first version [46] applies ridge regression, the latest version [45] uses a cost- sensitive decision tree for the algorithm selection model.  

In addition to the SATzilla family, hand- picked features have also been used with \(k\) - nearest neighbor or clustering methods to select the best algorithm [23, 32, 34]. CSHC [32] uses a cost- sensitive hierarchical clustering algorithm to iteratively partition the feature space into clusters in a supervised top- down fashion. In combination with the static algorithm schedule of 3S [23], it achieved better performance than SATzilla in the 2011 SAT Competition. ArgoS- mArT [34] uses a combination of a \(k\) - Nearest Neighbors (KNN) model and a multi- armed bandit algorithm. The KNN performs solver selection based on the nearest instance to the queried instance in a metric space.  

Finally, Loreggia et al. [31] proposed a deep learning approach based on convolutional neural networks (CNNs). They take textual representations of SAT instances in a standard format, and convert them into grayscale images by replacing each character with its corresponding ASCII code. These images are then rescaled to \(128 \times 128\) pixels, and fed to a CNN, which is trained to predict the fastest solver in the pool. Although employing deep learning, this approach is not lossless, as the rescaling required by the CNN architecture leads to information loss, unlike our approach.  

### 2.2 Algorithm Selection  

More generally, SAT solver selection is a special case of algorithm selection, which aims to select, for a given input, the most efficient algorithm from a set of candidate algorithms. This is particularly important for computationally hard problems, where there is typically no single algorithm that outperforms all others for all inputs. Besides SAT solving, algorithm selection techniques have achieved remarkable success in various applications such as Answer Set Programming (ASP) [15] and the Traveling Salesperson Problem (TSP) [1]. A thorough literature review is provided in Kerschke et al. [25].  

Algorithm selection methods can be roughly divided between offline and online methods. Offline methods, such as this work, rely on training ahead of time on a labeled dataset, whereas online algorithms attempt to improve selection performance as more and more cases are run. Although providing worse initial performance, online methods avoid the computational cost of the initial training phase and the problem of distribution shift between training and test data, and methods based on based on reinforcement learning or multi- armed bandits have been proposed for this purpose [10, 15].  

The many design choices in algorithm selection systems pose new challenges for efficient system development. For example, AutoFolio [30] automatically configures the entire framework, including budget allocation for pre- solving schedules, pre- processing procedures (such as transformations and filtering) and algorithm component selection. It achieved competitive performance in multiple scenarios from the ASLib [4] benchmark.  

### 2.3 Graph Neural Networks in SAT Solving  

Several works have explored applications of GNNs to various aspects of SAT solving in the past, even if not specifically to the problem of solver selection. In all these works, some kind of graphical representation of SAT instances is used. Multiple suggestions have appeared in the literature: these include lossless representations like literal- clause graphs (LCGs) and variable- clause graphs (VCGs), and lossy representations like literal- incidence graphs (LIGs) and variable- incidence graphs (VIGs) [19]. These different representations strike a balance between graph size and information content, and have found success in various SAT- related tasks.  

Some prior works have explored the use of GNNs to learn local search heuristics in SAT solvers [28, 48]. Yolcu and PÃ³czos [48] represent SAT formulas as variable- clause graphs (VCGs) and a GNN model is trained to select variables whose sign to flip at every step through a Markov decision process (MDP). The learned heuristic is shown to reduce the number of steps required to solve the problem. Graph- Q- SAT [28] uses a deep Q- network (DQN) with GNN architecture to learn branching heuristics in conflict driven clause learning (CDCL) solvers. Each SAT formula is converted into a VCG, and GNN layers are used to predict the \(Q\) - value of each variable. The variable with the highest \(Q\) - value for the specific assignment is selected for branching. The learned heuristic is shown to significantly reduce the number of iterations required for SAT solving.  

Instead of relying on existing SAT solvers, NeuroSAT [38] uses a GNN- based model to predict satisfiability of an instance in an end- to- end manner. Each SAT formula is represented by a literal- clause graph (LCG), as in our work. After several steps of message- passing, the updated embedding of each literal is projected to a scalar "vote" to indicate the confidence that the formula is satisfiable. The votes are averaged together and passed through a sigmoid function to produce the model's probability that the instance is satisfiable. On randomly generated instances from a SR(40) distribution, NeuroSAT solved \(70\%\) of SAT problems with an accuracy of \(85\%\) . A subsequent work, NeuroCore [37], uses a lighter NeuroSAT model to predict the "core" of instance, which is the smallest unsatisfiable subset of clauses. This prediction is then used to guide variable selection in SAT solver algorithms.  

Finally, graph neural networks have also been widely used for SAT instance generation. For example, G2SAT [49] and HardSATGEN [29] represent instances as LCGs, and generate new variants from an iterative splitting and merging process driven by a GNN. Furthermore, W2SAT [44] extends this approach by representing instances as weighted graphs encoding literal co- occurrence among clauses, while using a similar generation mechanism.  

## 3 APPROACH  

We now describe our proposed approach. The workflow of our method is shown in Figure 1.  

### 3.1 Problem  

Our SAT solver selection problem can be formally described as follows. We have a fixed pool of SAT solvers \(S_{1}, \ldots , S_{K}\) and a SAT instance \(a\) , and we must select a solver so as to solve the instance in the smallest runtime possible.