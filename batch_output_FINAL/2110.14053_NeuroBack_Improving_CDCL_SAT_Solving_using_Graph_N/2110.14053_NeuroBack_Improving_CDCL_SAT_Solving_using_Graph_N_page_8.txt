<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table>  

Table 2: The performance on the validation set of both pre- trained and fine- tuned NeuroBack models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy.  

Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included DataBack- PT.  

The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. DataBack- PT thus contains a diverse set of formulas.  

DataBack- FT Given that NeuroBack will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, DataBack- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While DataBack- FT is considerably smaller in size compared to DataBack- PT, its individual formulas are distinctly larger than those in DataBack- PT.  

## 6 EXPERIMENTS  

Platform All experiments were run on an ordinary commodity computer with one NVIDIA GeForce RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM.  

Research Questions The experiments aim to answer two research questions:  

RQ1: How accurately does the NeuroBack model classify the phases of backbone variables? RQ2: How effective is the NeuroBack approach?  

RQ1: NeuroBack Model Performance The NeuroBack model was pre- trained on the entire DataBack- PT dataset, then fine- tuned on a random \(90\%\) of DataBack- FT samples, and evaluated on the remaining \(10\%\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \(75.1\%\) accuracy in classifying backbone variables, with a precision exceeding \(90\%\) and a recall rate of \(76.7\%\) . Considering the distinct data sources of DataBack- PT and DataBack- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \(4\%\) and \(15\%\) across all metrics, making precision, recall and F1 score all exceeding \(90\%\) . In conclusion, NeuroBack model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning.  

RQ2: NeuroBack Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. NeuroBack- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each NeuroBack solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and