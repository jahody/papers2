#### 4.2.1 GNN MODEL DESIGN  

The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, GraphTrans Wu et al. (2021). However, in our particular SAT application context, GraphTrans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task.  

To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph.  

Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \(L\) stacked GNN blocks, a transformer subnet with \(M\) GSA transformer blocks and \(N\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, ViT- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency.  

#### 4.2.2 IMPLEMENTATION  

The current implementation of NeuroBack model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer.  

GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the ViT transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. LayerNorm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \(L = 4\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \(M = 3\) and \(N = 3\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using PyTorch Paszke et al. (2019) and PyTorch Geometric Fey & Lenssen (2019).  

#### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING  

The NeuroBack model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable