<center>Figure 3: The architecture of NeuroBack model, consisting of three main components: a GNN subnet with \(L\) stacked GNN blocks, a transformer subnet with \(M\) GSA transformer blocks and \(N\) LSA transformer blocks, and a FFN layer for node classification. </center>  

represent the variables and clauses, respectively. Each edge connects a variable node to a clause node, representing that the clause contains the variable. Two edge types represent two polarities of a variable appearing in the clause, i.e., the variable itself and its complement. Although the representation is compact, its diameter might be substantial for large- scale SAT formulas, which could result in insufficient message passing during the learning process.  

To mitigate this issue, we introduce a meta node for each connected component in the graph, with meta edges connecting the meta node to all clause nodes in the component. With the added meta nodes and edges, every variable node not appearing in the same clauses can reach each other through their corresponding clause nodes and the meta node, thereby making the diameter at most four. Figure 2 shows an example of our graph representation for the CNF formula \((v_{1} \lor v_{2}) \land (v_{2} \lor v_{3}) \land (v_{3} \lor v_{4})\) . It includes one meta node, four variable nodes and three clause nodes \(c_{1}, c_{2}, c_{3}\) , representing clauses \(v_{1} \lor v_{2}, v_{2} \lor v_{3}\) and \(v_{3} \lor v_{4}\) , respectively. Without the added meta node and edges, the longest path in the graph runs from variable node \(v_{1}\) to variable node \(v_{4}\) , making the diameter six. However, by introducing the meta node and edges, the diameter is reduced to four.  

Formally, for a graph representation \(G = (V, E, W, H)\) of a CNF formula, the edge feature \(W_{u, v}\) of each edge \((u, v)\) is initialized by its edge type with the value 0 representing the meta edge, the value 1 representing positive polarity, and \(- 1\) negative polarity; the node feature \(H_{v}\) of each node \(v\) is initialized by its node type with 0 representing a meta node, 1 representing a variable node, and \(- 1\) representing a clause node.  

<center>Figure 2: An example graph representation of the CNF formula \((v_{1} \lor v_{2}) \land (v_{2} \lor v_{3}) \land (v_{3} \lor v_{4})\) . A meta node \(m\) is added along with meta edges (represented by dashed lines) connecting to all clause nodes in the connected component, reducing the graph diameter from six to four. </center>  

### 4.2 GNN-based PHASE PREDICTION  

Given that the phases of backbone variables remain consistent across all satisfying assignments, selecting the correct phase for a backbone variable prevents backtracking. Conversely, an incorrect choice inevitably leads to a conflict. Moreover, the proportion of backbone variables is typically significant. For instance, during our data collection from five notable sources (i.e., CNFgen, SATLIB, model counting competitions, SATCOMP random tracks, and SATCOMP main tracks), backbone variables constitute an average of \(27\%\) of the total variables. Therefore, accurately identifying the phases of the backbone variables is crucial for efficiently solving a SAT formula. Furthermore, identifying the phases of the non- backbone variables appearing in the majority of satisfying assignments is also important. Because such phases could prevent backtracking with high probabilities.  

With the converted graph representation, predicting the phases of all variables, including backbone and non- backbone variables, is a binary node classification problem, which can be addressed by a GNN model. Inspired by transfer learning Zhuang et al. (2020), we first train a GNN model to predict the phases of backbone variables, and then leverage the trained model to predict the phases of all variables. Our key insight is that the knowledge learned from predicting the phases of backbone variables can provide a valuable guidance on predicting the phases of non- backbone variables exhibiting in the majority of satisfying assignments. The following subsections introduce the design, implementation, and training of our GNN model.