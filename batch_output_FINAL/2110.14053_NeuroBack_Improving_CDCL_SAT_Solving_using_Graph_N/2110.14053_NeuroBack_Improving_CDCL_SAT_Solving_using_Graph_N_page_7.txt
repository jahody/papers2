<table><tr><td>DataBack-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>DataBack-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># BackboneVar</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># BackboneVar</td><td>48,266 (23%)</td></tr></table>  

Table 1: Details of DataBack. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set.  

phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5.  

Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, AdamW optimizer Loshchilov & Hutter (2017) with a learning rate of \(10^{- 4}\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer.  

### 4.3 APPLYING PHASE PREDICTIONS IN SAT  

The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with NeuroBack predictions. The resulting implementation is called NeuroBack- Kissat.  

## 5 DATABACK  

We created a new dataset, DataBack, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the NeuroBack model. Accordingly, there are two subsets in DataBack: the pre- training set, DataBack- PT, and the fine- tuning set, DataBack- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called CadiBack Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. DataBack includes formulas solved within the time limit with at least one backbone variable.  

We observe that there exists a significant label imbalance in both DataBack- PT and DataBack- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \(f\) with \(n\) backbone variables \(b_{1},\ldots ,b_{n}\) , let \(\mathcal{L}_{f}:\{b_{1},\ldots ,b_{n}\} \to \{1,0\}\) denote the mapping of each backbone variable to its phase. The dual formula \(f^{\prime}\) is obtained from \(f\) by negating each backbone variable: \(f^{\prime} = f[b_{1}\to\) \(\neg b_{1},\ldots ,b_{n}\mapsto \neg b_{n}]\) . The dual \(f^{\prime}\) is still satisfiable and retains the same backbone variables as \(f\) , but with the opposite phases \(\mathcal{L}_{f^{\prime}}(b_{i}) = \neg \mathcal{L}_{f}(b_{i}),i\in \{1,\ldots ,n\}\) . For the given CNF formula example in Section 2 \(\phi = (v_{1}\vee \neg v_{2})\wedge (v_{2}\vee v_{3})\wedge v_{2}\) , having \(v_{1}\) and \(v_{2}\) as its backbone variables with phases \(\{v_{1},v_{2}\} \to \{1\}\) , the dual formula is \(\phi^{\prime} = (\neg v_{1}\vee v_{2})\wedge (\neg v_{2}\vee v_{3})\wedge \neg v_{2}\) , still having \(v_{1}\) and \(v_{2}\) as the backbone variables but with opposite phases \(\{v_{1},v_{2}\} \to \{0\}\) . This data augmentation strategy doubles the size of DataBack with a perfect balance in positive and negative backbone labels. In the rest of the paper, DataBack- PT and DataBack- FT refer to the augmented, balanced datasets.  

DataBack- PT The CNF formulas in DataBack- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St√ºtzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.