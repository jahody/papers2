We propose NeuroBack, a novel approach to make CDCL SAT solving more effective and avoid frequent online model inferences, thus making the GNN approach more practical. The main idea of NeuroBack is to make offline model inference, i.e., prior to the solving process, to obtain instructive static information for improving CDCL SAT solving. Once trained, the offline model inference allows NeuroBack to execute solely on the CPU, thereby making it completely independent of GPU resources. In particular, NeuroBack seeks to refine the phase selection heuristics in CDCL solvers by leveraging offline neural predictions on variable phases appearing in the majority (or even all) of the satisfying assignments.  

The offline predictions on such phase information are based on the generalization of backbone variables, which are variables whose phases remain consistent across all satisfying assignments. Recent work Biere et al. (2021); Al- Yahya et al. (2022) has shown that backbone variables are crucial for enhancing CDCL SAT solving. Choosing the correct phase for a backbone variable prevents conflicts, while an incorrect choice inevitably leads to backtracking in the search. Moreover, predicting the correct phases of non- backbone variables appearing in the majority of satisfying assignments is also important, because such phases prevent backtracking with high probabilities. Our conjecture is that the knowledge learned from predicting the phases of backbone variables can be transferred to predicting the phases of non- backbone variables exhibited in the majority of satisfying assignments. Therefore, NeuroBack applies a GNN model, trained solely on predicting the phases of backbone variables, to predict the phases of all variables.  

NeuroBack converts the SAT formula with diverse scales into a compact and more learnable graph representation, turning the problem of predicting variable phases into a binary node classification problem. To make the GNN model both compact and robust, NeuroBack employs a novel Graph Transformer architecture with light- weight self- attention mechanisms. To train the model with supervised learning, a balanced dataset called DataBack containing 120,286 labeled formulas with diversity was created from five different sources: CNFgen Lauria et al. (2017), SATLIB Hoos & St√ºtzle (2000), model counting competitions from 2020 to 2022 MCC (2020; 2021; 2022), and main and random tracks in SAT competitions from 2004 to 2021.  

To evaluate the effectiveness of our approach, NeuroBack is incorporated into a state- of- the- art CDCL SAT solver called Kissat Biere & Fleury (2022), resulting in a new solver called NeuroBack- Kissat. The experimental results on all SAT problems from SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) show that NeuroBack allows Kissat to solve up to \(5.2\%\) and \(7.4\%\) more problems, respectively. The experiments thus demonstrate that NeuroBack is a practical neural approach to improving CDCL SAT solvers. The contributions of our paper are:  

1. Approach. To our knowledge, NeuroBack presents the first practical neural approach to make the CDCL SAT solving more effective, without requiring any GPU resource during its application.  

2. Dataset. A new dataset DataBack containing 120,286 data samples is created for backbone phase classification. DataBack is publicly available at https://huggingface.co/datasets/neuroback/DataBack  

3. Implementation. NeuroBack is incorporated into a state-of-the-art SAT solver, Kissat. The source code of NeuroBack model and NeuroBack-Kissat is publicly available at https://github.com/wenxiwang/neuroback.  

## 2 BACKGROUND  

This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer.  

Preliminaries of SAT In SAT, a propositional logic formula \(\phi\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \((\wedge)\) of clauses. Each clause is a disjunction \((\vee)\) of literals. A literal is either a variable \(v\) , or its complement \(\neg v\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \(\phi = (v_{1} \vee \neg v_{2}) \wedge (v_{2} \vee v_{3}) \wedge v_{2}\) consists of three clauses \(v_{1} \vee \neg v_{2}\) , \(v_{2} \vee v_{3}\) and \(v_{2}\) ; four literals \(v_{1}\) , \(\neg v_{2}\) , \(v_{2}\) and \(v_{3}\) ; and three variables \(v_{1}\) , \(v_{2}\) , and \(v_{3}\) . One satisfying assignment of \(\phi\) is \(v_{1} = 1\) , \(v_{2} = 1\) , \(v_{3} = 0\) . The goal of a SAT solver is to check if a formula \(\phi\) is sat or unsat. A complete solver either outputs a satisfying assignment for \(\phi\) , or proves that no such assignment exists.