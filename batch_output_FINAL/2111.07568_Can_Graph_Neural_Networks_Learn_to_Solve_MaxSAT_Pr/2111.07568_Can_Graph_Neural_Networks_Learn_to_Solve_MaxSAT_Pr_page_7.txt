Table 2: The accuracy of MS-NSFG on different combinations of training and testing sets.   

<table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.86 (99.8%)</td><td>1.42 (99.7%)</td><td>3.86 (98.6%)</td><td>1.15 (99.8%)</td><td>6.77 (98.5%)</td><td></td></tr><tr><td>91.9%</td><td>91.9%</td><td>76.1%</td><td>92.0%</td><td>75.4%</td><td></td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>1.19 (99.7%)</td><td>1.17 (99.8%)</td><td>4.59 (98.4%)</td><td>1.50 (99.7%)</td><td>7.96 (98.3%)</td><td></td></tr><tr><td>91.7%</td><td>92.4%</td><td>75.8%</td><td>91.6%</td><td>75.1%</td><td></td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>20.58 (96.0%)</td><td>29.10 (95.7%)</td><td>1.37 (99.5%)</td><td>27.92 (95.9%)</td><td>2.16 (99.5%)</td><td></td></tr><tr><td>78.1%</td><td>76.0%</td><td>83.3%</td><td>78.0%</td><td>82.2%</td><td></td></tr></table>  

Table 3: The accuracy of MS-ESFG on different combinations of training and testing sets.   

<table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.54 (99.8%)</td><td>1.12 (99.8%)</td><td>5.98 (97.9%)</td><td>0.69 (99.9%)</td><td>9.92 (97.9%)</td></tr><tr><td>92.3%</td><td>92.2%</td><td>74.5%</td><td>92.2%</td><td>73.9%</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.48 (99.9%)</td><td>0.78 (99.8%)</td><td>6.13 (97.8%)</td><td>0.62 (99.9%)</td><td>10.24 (97.8%)</td></tr><tr><td>92.3%</td><td>92.8%</td><td>74.7%</td><td>92.2%</td><td>74.2%</td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>14.83 (97.1%)</td><td>16.44 (97.5%)</td><td>1.32 (99.5%)</td><td>20.22 (97.0%)</td><td>1.98 (99.5%)</td></tr><tr><td>78.8%</td><td>79.5%</td><td>83.5%</td><td>78.6%</td><td>82.5%</td></tr></table>  

vice versa) can maintain almost the same prediction accuracy. The second is generalizing to larger and more difficult problems. We use two testing sets, R2 (80, 800) and R3 (50, 500), where the number of variables is larger than those appeared in the training sets, as well as more difficult to solve by MaxHS. It can be found that both models trained on Max2SAT datasets can generalize to work on R2 (80, 800) with a satisfactory accuracy. This also holds when the training set is R3 (30, 300) and the testing set is R3 (50, 500), which implies that GNN models are expected to be promising alternatives to help solve those difficult MaxSAT problems. The last is generalizing to other datasets with different parameter \(k\) . For example, the model is trained on Max2SAT but tested on Max3SAT problems. The results show that both models have limitations under this condition, since they cannot achieve an accuracy of assignments \(>80\%\) on every pair of training and testing sets, and the gap to optimal objective is not close enough, especially when trained on R3 (30, 300) and tested on Max2SAT datasets.  

## Related Work  

Although the mainstream approaches for solving combinatorial problems, not limited to SAT or MaxSAT, are based on the search algorithms from symbolism, there have always been attempts trying to tackle these problems through data- driven techniques. A class of research is to integrate machine learning model in the traditional search framework, which has made progress on a number of problems such as mixed integer programming (MIP) (Khalil et al. 2016), satisfiability modulo theories (SMT) (Balunovic, Bielik, and Vechev 2018) and quantified boolean formulas (QBF) (Lederman et al. 2020). Here, we work on building end- to- end models which do not need the aid of search algorithm. The earliest research work can be traced back to the Hopfield network (Hopfield and Tank 1985) to solve TSP problem. Recently, many variants of neural networks have been proposed, which directly learn to solve combinatorial problems. (Vinyals, Fortunato, and Jaitly 2015) introduces Pointer Net, a sequential model that performs well on solving TSP and convex hull problems. (Khalil et al. 2017) uses the com  

bination of reinforcement learning and graph embedding, and learns greedy- like strategies for minimum vertex cover, maximum cut and TSP problems.  

With the development of graph neural networks, there have been a series of work that uses GNN models to solve combinatorial problems. An important reason is that many of such problems are directly defined on graph, and in general the relation between variables and constraints can be naturally represented as a bipartite graph. Except for the mentioned NeuroSAT (Selsam et al. 2019) and its improvement, there have been some efforts learning to solve TSP (Prates et al. 2019), pseudo- Boolean (Liu et al. 2020) and graph coloring (Lemos et al. 2019) problems with GNN- based models. The results indicate that GNNs have become increasingly appealing alternatives in solving combinatorial problems. Moreover, there are also some well- organized literature reviews on this subject, such as (Bengio, Lodi, and Prouvost 2021), (Cappart et al. 2021) and (Lamb et al. 2020).  

## Conclusion and Future Work  

Graph neural networks (GNNs) have been considered as a promising technique that can learn to solve combinatorial problems in the data- driven fashion, especially for the Boolean Satisfiability (SAT) problem. In this paper, we further study the quality of solution predicted by GNNs in learning to solve Maximum Satisfiability (MaxSAT) problem, both from theoretical and practical perspectives. Based on the graph construction methods in the previous work, we build two kinds of GNN models, MS- NSFG and MS- ESFG, which can predict the solution of MaxSAT problem. The models are trained and tested on randomly generated benchmarks with different distributions. The experimental results show that both models have achieved pretty high accuracy, and also satisfactory generalization to larger and more difficult instances. In addition, this paper is the first that attempts to present an explanation of the capability of GNNs to solve MaxSAT problem from a theoretical point of view. On the basis of algorithmic alignment theory, we prove that even a single- layer GNN model can solve the MaxSAT problem with an approximation ratio of \(1 / 2\) .  

We hope the results in this paper can inspire future work from multiple perspectives. A promising direction is to integrate the GNN models into a powerful search framework to handle more difficult problems in the specific domain, such as weighted and partial MaxSAT problems. It is also interesting to further analyze the theoretical capability of multilayer GNNs to achieve better approximation.  

## References  

Ans√≥tegui, C.; Bonet, M. L.; and Levy, J. 2013. SAT- based MaxSAT algorithms. Artif. Intell., 196: 77- 105. Bacchus, F. 2020. MaxHS in the 2020 MaxSat Evaluation. MaxSAT Evaluation 2020, 19. Balunovic, M.; Bielik, P.; and Vechev, M. T. 2018. Learning to Solve SMT Formulas. In Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa- Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing