where \(p \in [0,1]\) is the predicted probability of a variable being assigned True, and \(y\) is the binary label from an optimal solution. By averaging the loss of each variable, we obtain the loss of a problem instance, which should be minimized. We will investigate the performance of these two models through experiments in Section 5.  

## Theoretical Analysis  

With the results and prospects revealed in practice, GNNs are considered to be a suitable choice in learning to solve SAT problem from benchmarks. However, the knowledge about why these models would work from a theoretical perspective is still limited so far. In this section, we are committed to explaining the possible mechanism of GNNs to solve MaxSAT problem. More specifically, we prove that a single- layer GNN can achieve an approximation ratio of \(1 / 2\) with the help of a distributed local algorithm.  

## Algorithmic Alignment  

Our theoretical analysis framework is inspired by the algorithmic alignment theory proposed by (Xu et al. 2020), which provides a new point of view to understand the reasoning capability of neural networks. Formally, suppose a neural network \(\mathcal{N}\) with \(n\) modules \(\mathcal{N}_i\) , if by replacing each \(\mathcal{N}_i\) with a function \(f_i\) it can simulate, and \(f_1, \ldots , f_n\) generate a reasoning function \(g\) , we say \(\mathcal{N}\) aligns with \(g\) . As shown in that paper, even if different models such as MLPs, deep sets and GNNs have the same expressive power theoretically, GNNs tend to achieve better performance and generalization stably in the experiments. This can be explained by the fact that the computational structure of GNNs aligns well with the dynamic programming (DP) algorithms. Therefore, compared with other models, each component of GNNs only needs to approximate a simpler function, which means the algorithmic alignment can improve the sample complexity.  

## Distributed Local Algorithm  

According to the algorithmic alignment theory, it is possible to reasonably infer the capability that a GNN model can achieve with the help of an algorithm aligned with its computational structure. However, this also raises an issue: as a polynomial- time procedure, any GNN model cannot align with an exact algorithm for NP- hard problems such as MaxSAT, under the assumption that \(\mathrm{P} \neq \mathrm{NP}\) . In this case, we turn to employ a weaker approximation algorithm to analyze the capability of GNNs. Although there has been a lot of research on the approximation algorithms of MaxSAT (Vazirani 2001), it is regrettable that most of them cannot align with the structure of GNNs in an intuitive way.  

In fact, there is a class of algorithms called distributed local algorithm (DLA) (Elkin 2004), which have been found to be a good choice to align with GNNs for combinatorial problems. DLA assumes a distributed computing system that there are a set of nodes in a graph, where any two nodes do not know the existence of each other at first. The algorithm then runs in a constant number of synchronous communication rounds. In each round, a node performs local computation, and sends one message to its neighboring nodes, while  

Algorithm 1: A distributed local algorithm for MaxSAT  Input: The set of literals \(L\) , the set of clauses \(C\)  Output: The assignments of literals \(\Phi\) 1: Set up a factor graph such as NSFG for \(L\) and \(C\) . 2: \(W(L_i) \leftarrow 0\) for each \(L_i \in L\) . 3: \(S(C_j) \leftarrow \{\}\) for each \(C_j \in C\) . 4: \(S(C_j) \leftarrow S(C_j) \cup \{L_i\}\) for each edge \((L_i, C_j)\) in the factor graph. 5: for each \(C_j \in C\) do 6: \(L^* \leftarrow\) Pick a literal from \(S(C_j)\) . 7: \(W(L^*) \leftarrow W(L^*) + 1\) . 8: end for 9: for each \(L_i \in L\) do 10: if \(W(L_i) > = W(\bar{L}_i)\) then 11: \(\Phi (L_i) \leftarrow \mathrm{True}\) . 12: else 13: \(\Phi (L_i) \leftarrow \mathrm{False}\) . 14: end if 15: end for 16: return \(\Phi\)  

receiving one from each of them. Finally, each node computes the output in terms of the information it holds. DLAs have been widely used in many applications, such as designing sublinear- time algorithms (Parnas and Ron 2007) and controlling wireless sensor networks (Kubisch et al. 2003).  

## Analyzing GNNs for MaxSAT  

Most DLAs are designed for solving combinatorial problems in graph theory. (Sato, Yamada, and Kashima 2019) has employed DLAs to clarify the approximation ratios of GNNs for several NP- hard problems on graphs such as minimum dominating set and minimum vertex cover. This also encourages us to analyze the capability of GNNs to solve MaxSAT problem with the help of a DLA. As the DLA for MaxSAT has not been studied in the literature, we design an algorithm that aligns well with the message passing process described in Section 3, and the pseudo code is shown in Algorithm 1.  

This algorithm accepts the description of a MaxSAT problem instance as input, and returns the assignment of literals, while the value of objective (i.e., the number of satisfied clauses) is easy to compute from it. Following the rules of DLA, the literals and clauses do not have any information about the problem initially. \(W\) and \(S\) correspond to the states of literals and clauses, respectively. In line 4, the message is passed from literals to clauses, so that each clause is aware of the literals it contains, and stores the information into \(S\) . Next, in lines 5- 8, the message is generated by clauses and sent to literals. Finally, the assignment of each literal is decoded from \(W\) by a greedy- like method. It can be seen that the algorithm only carries out one round of communication. It is interesting to know whether a single- layer GNN may also have some capabilities to solve the MaxSAT problem theoretically. This can be analyzed by aligning it with the algorithm.  

Theorem 1. There exists a single- layer GNN to solve the