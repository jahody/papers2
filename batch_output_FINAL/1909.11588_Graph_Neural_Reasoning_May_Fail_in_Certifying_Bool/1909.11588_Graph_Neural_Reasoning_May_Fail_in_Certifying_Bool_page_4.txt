aggregation and combine functions that encode literals and clauses respectively, which GNNs in Eq. 2 may learn if they attempt to simulate WalkSAT:  

\[\begin{array}{r l} & {m_{v}^{(k)} = \overline{{\mathrm{Aggregate}}}_{L}\Big(\{h_{\Psi^{(v)}}^{(k - 1)}:\Psi (v)\in \Phi \} \Big),}\\ & {\qquad = \left\{ \begin{array}{l l}{\epsilon^{(k)},} & {\prod_{\Psi (v)}||h_{\Psi^{(v)}}^{(k - 1)}|| = 0}\\ {\qquad 0,} & {\prod_{\Psi (v)}||h_{\Psi^{(v)}}^{(k - 1)}||\neq 0} \end{array} \right.} \end{array} \quad (4)\]  

where \(\overline{{\mathrm{Aggregate}}}_{L}(\cdot)\) denotes the optimal aggregation function to propagate literal messages and \(m_{v}^{(k)}\) denotes the optimally propagated message of literal \(v\) in the \(k\) iteration; \(\mathbf{0}\) is a zero- value vector; \(\epsilon^{(k)}\) denotes a bounded non- zero random vector generated in the \(k\) iteration; \(||\cdot ||\) indicates a vector norm.  

\[\begin{array}{r l} & {h_{v}^{(k)} = \overline{{\mathrm{Combine}}}_{L}\Big(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},m_{v}^{(k)}\Big)}\\ & {\qquad = \left\{ \begin{array}{l l}{h_{v - v}^{(k - 1)},} & {v = \arg \max \{\big||m_{u}^{(k)}\big||\} \mathrm{and} \big||m_{v}^{(k)}\big|| > 0}\\ {h_{v}^{(k - 1)},} & {\mathrm{otherwise}} \end{array} \right.} \end{array} \quad (5)\]  

where \(\overline{{\mathrm{Combine}}}_{L}(\cdot)\) denotes the optimal combine function that iteratively updates literal embeddings by the aid of the optimal message. Eq. 5 implies the local Boolean variable flipping in WalkSAT: if the norm of \(m_{v}^{(k)}\) is the maximum among all the optimal literal messages, its literal embedding would be replaced by the embedding of its negation, otherwise, keep the identical value. The maximization ensures only one literal embedding that would be "flipped" per iteration, which simulates the local search behavior. Besides, the literal embedding selected for update would not be \(\mathbf{0}\) , which implies all the clauses containing this literal are satisfied (see the condition 2 in Eq. 4 ). Since all the satisfied clauses would not be selected in WalkSAT, this literal also would not be selected to update in this iteration. Finally, if a literal has been included by a clause that is unsatisfied, it would be randomly picked in some probability. The uncertainty is implied by the randomness of \(\epsilon^{(k)}\) .  

\[\begin{array}{r l} & {m_{\Psi^{(v)}}^{(k)} = \overline{{\mathrm{Aggregate}}}_{C}\big(\{h_{u}^{(k - 1)}:u\in \Psi (v)\} \big)}\\ & {\qquad = \left\{ \begin{array}{l l}{h_{\Psi^{(v)}}^{(0)},} & {\mathrm{Sigmoid}\Big(\mathrm{MLP}_{2}^{*}\Big(\sum_{u\in \Psi (v)}\mathrm{MLP}_{1}^{*}(h_{u}^{(k - 1)})\Big)\Big)\geq 0.5}\\ {0,} & {\mathrm{Sigmoid}\Big(\mathrm{MLP}_{2}^{*}\Big(\sum_{u\in \Psi (v)}\mathbf{MLP}_{1}^{*}(h_{u}^{(k - 1)})\Big)\Big)< 0.5} \end{array} \right.} \end{array} \quad (6)\]  

where \(\overline{{\mathrm{Aggregate}}}_{C}(\cdot)\) denotes the optimal aggregation function that conveys the clause embedding messages during reasoning. Note that \(\mathrm{MLP}_{2}\Big(\sum_{u\in \Psi (v)}\mathrm{MLP}_{1}(h_{u}^{(k - 1)})\Big)\) indicates Deep Sets [18], a neural network that encodes a literal embedding set \(\{h_{u}^{(k - 1)}\}_{u\in \Psi (v)}\) whose literals are included by a clause \(\Psi (v)\) . The reduced feature would be fed into the sigmoid clause predictor. We use \(\mathrm{MLP}_{1}^{*}\) and \(\mathrm{MLP}_{2}^{*}\) to denote the implicit optimal prediction to each clause: given the arbitrarily initiated literal embeddings that denote the Boolean value assignment of literals, the optimal Deep Sets can predict whether the literal- derived clause is satisfied \((\geq 0.5)\) or not \((< 0.5)\) . Since the predictor is permutation- invariant to the input, Propositions 3.1 in [15] promises that it can be approximated arbitrarily closely by graph convolution, which exactly corresponds to the parameterized clause aggregation functions in Eq.2. On the other hand, Eq. 5 promises the literal embeddings staying in their initiated values over iterations, hence the optimal Deep Sets may always judge whether a clause (the set of literals as the input of Deep Sets) is satisfied or not.  

\[\begin{array}{r l} & {h_{\Psi^{(v)}}^{(k)} = \overline{{\mathrm{Combine}}}_{C}\Big(h_{\Psi^{(v)}}^{(k - 1)},m_{\Psi^{(v)}}^{(k)}\Big)}\\ & {\qquad = \left\{ \begin{array}{l l}{h_{\Psi^{(v)}}^{(k - 1)},} & {h_{\Psi^{(v)}}^{(k - 1)} = m_{\Psi^{(v)}}^{(k)}}\\ {h_{\Psi^{(v)}}^{(0)},} & {||h_{\Psi^{(v)}}^{(k - 1)}||< ||m_{\Psi^{(v)}}^{(k)}||}\\ {0,} & {||h_{\Psi^{(v)}}^{(k - 1)}||\geq ||m_{\Psi^{(v)}}^{(k)}||} \end{array} \right.} \end{array} \quad (7)\]  

where \(\overline{{\mathrm{Combine}}}_{C}(\cdot)\) denotes the optimal clause combine function. Based on the propagated messages conveyed by Eq. 2 , it determines how to iteratively update clause embeddings to simulate WalkSAT.