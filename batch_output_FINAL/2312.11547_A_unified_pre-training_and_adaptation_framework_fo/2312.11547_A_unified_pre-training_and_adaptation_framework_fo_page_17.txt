model achieved optimal performance when \(d\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \(k\) was 10. The optimal value of weight \(\lambda\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three.  

## 5 Conclusion  

In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs.  

Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future.  

Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503).  

## References  

1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023.  2 Nathan Krislock, Jérôme Malick, and Frédéric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014.  3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020.  4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982.  5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006.  6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966.  7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000.  8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017.  9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.  10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.  11 Quentin Cappart, Didier Chételat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023.  12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017.  13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.