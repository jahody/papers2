were generated and the MaxHS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \(^{1)}\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually.  

Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \(p\) values [51] were used to evaluate the number of graph cuts. Concretely, \(p\) can be calculated by  

\[p(z) = \frac{z / n - \gamma / 4}{\sqrt{\gamma / 4}}, \quad (4.1)\]  

where \(z\) is the predicted cut size for a \(\gamma\) - regular graph with \(n\) nodes and \(\gamma\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation.  

Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems.  

Experimental Settings The proposed framework was implemented in Python with PyTorch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \(d\) was set as 128. The number of GNN layers \(L\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \(10^{- 5}\) and a weight decay of \(10^{- 10}\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB.  

Table 1 Results of solving Max-Cut with different approaches on random graphs.   

<table><tr><td rowspan="2">Methods</td><td colspan="4">n=100</td><td colspan="4">n=200</td><td colspan="4">n=500</td><td colspan="4">n=800</td><td colspan="4">n=1000</td></tr><tr><td>γ=3</td><td>γ=5</td><td>γ=10</td><td>γ=3</td><td>γ=5</td><td>γ=10</td><td>γ=3</td><td>γ=5</td><td>γ=10</td><td></td><td>γ=5</td><td>γ=10</td><td></td><td>γ=5</td><td>γ=10</td><td></td><td>γ=5</td><td></td><td>γ=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>