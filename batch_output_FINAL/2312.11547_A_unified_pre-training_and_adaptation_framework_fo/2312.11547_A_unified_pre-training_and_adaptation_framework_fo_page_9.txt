After obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \(\mathbf{X}_{ini}\) and \(\mathbf{C}_{ini}\) into low- dimensional features \(\mathbf{X}^{(0)}\) and \(\mathbf{C}^{(0)}\) ; the bipartite GNN backbone \(\mathrm{Bip - GNN}(\cdot)\) from section 3.3 extracts high- level node features for variables \(\mathbf{X}^{(L)}\) and clauses \(\mathbf{C}^{(L)}\) where \(L\) is the number of layers; and fully connected prediction network \(\mathrm{FC}(\cdot)\) maps node features to their labels.  

To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification:  

\[\mathcal{L}_{c} = \sum_{i = 1}^{n}\mathrm{BCE}(p_{i},y_{i}), \quad (3.14)\]  

where \(y_{i}\in \{0,1\}\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \(p_{i}\in \{0,1\}\) is the prediction of the pre- training model from the prediction network \(\mathrm{FC}(\cdot)\) . It is worth noting that we only consider the classification loss under \(n\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables.  

#### 3.4.2 Fine-Tuning with Domain Adaptation  

After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem.  

The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \(\mathrm{Bip - GNN}(\cdot)\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \(\mathrm{Dis}(\cdot)\) is to classify the domain labels for each sample.  

The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as  

\[\mathcal{L}_{ft} = \mathcal{L}_{c} + \lambda \mathcal{L}_{d}, \quad (3.15)\]  

where \(\mathcal{L}_{c}\) and \(\mathcal{L}_{d}\) are the losses for classification and discrimination, and \(\lambda\) is a positive hyper- parameter that controls the weight of losses.  

The discriminative loss that classifies nodes based on their domain labels can be denoted as  

\[\mathcal{L}_{d} = (\mathbf{X}_{S},\mathbf{X}_{T}) = \mathbb{E}_{\mathbf{x}_{S}\in \mathcal{D}_{S}}[\log \left(1 - \mathrm{Dis}(\mathrm{Bip - GNN}(\mathbf{x}_{S}))\right)] + \mathbb{E}_{\mathbf{x}_{T}\in \mathcal{D}_{T}}[\log \left(1 - \mathrm{Dis}(\mathrm{Bip - GNN}(\mathbf{x}_{T}))\right)], \quad (3.16)\]  

where \(\mathbf{X}_{S}\) and \(\mathbf{X}_{D}\) are the node features from source and target domains, \(\mathcal{D}_{S}\) and \(\mathcal{D}_{T}\) represent the data distribution of source and target domains, \(\mathrm{Dis}(\cdot)\) is the domain classifier, and \(\mathrm{Bip - GNN}(\cdot)\) is the feature extractor.