Test- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \(\alpha_{c}\) , a random \(k\) - SAT dataset consisting of mixed \(k\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems.  

We train both a Graph Convolutional Network (GCN)- solver [25] and NeuroSAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory.  

### 4.2 A New Hardness Heuristic for GNN-Based Solvers  

Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \(\alpha\) . Given an input graph \(G\) , we define the heuristics as:  

\[\omega (G) = -\mathbb{E}_{(i\sim j)}[Ric(i,j)]*\mathbb{E}[\alpha ],\quad \omega^{*}(G) = \frac{\omega(G)}{\mathbb{V}_{(i\sim j)}[Ric(i,j)]}, \quad (11)\]  

with the expectations being taken over the edges \(G\) . The averages of both heuristics, which we denote by \(\bar{\omega}\) and \(\bar{\omega}^{*}\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \(G\) is on average \((\omega (g))\) and how much this quantity concentrates \((\omega^{*}(G))\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of NeuroSAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \(\rho_{\bar{\alpha}} = 0.32\) , \(\rho_{\bar{\omega}} = 0.86\) and \(\rho_{\bar{\omega}^{*}} = 0.98\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper.  

## 5 Conclusions  

Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and NeuroSAT solvers in Table 1.