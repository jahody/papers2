### 2.2 Graph Neural Networks  

GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \(G\) with \(N\) nodes, represented by a symmetric binary adjacency matrix \(A\in \{0,1\}^{N\times N}\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \(X\in \mathbb{R}^{N\times d}\) to the graph, we can describe a GNNs as a convolution of the graph signal with \(A\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]:  

\[x_{i}^{(k)} = \theta^{(k)}\left(x_{i}^{(k - 1)},\bigoplus_{j\in \mathcal{N}(i)}\phi^{(k)}\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\right)\right), \quad (1)\]  

where \(x_{i}^{(k)}\) denotes node features of node \(x_{i}\) at layer \(k\) \(e_{j i}\) the (optional) edge features from node \(j\) to node \(i\) \(\mathcal{N}(\cdot)\) the set of (1- hop) neighbor nodes, \(\bigoplus\) a differentiable, permutation invariant function, (e.g., sum, mean), and \(\phi ,\theta\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs).  

A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \(i\) be a literal node and \(j\) be a clause node, then:  

\[\begin{array}{r l} & {h_{j}^{(k)} = \theta_{c}^{(k)}\left(h_{j}^{(k - 1)},\bigoplus_{i\in \mathcal{N}(j)}\left(\left\{\phi_{i}^{(k)}\left(h_{i}^{(k - 1)}\right)\right\}\right)\right),}\\ & {h_{i}^{(k)} = \theta_{l}^{(k)}\left(h_{i}^{(k - 1)},\bigoplus_{j\in \mathcal{N}(i)}\left(\left\{\phi_{c}^{(k)}\left(h_{j}^{(k - 1)}\right)\right\}\right)\right),} \end{array} \quad (2)\]  

where the subscripts \(c\) and \(l\) refer to NNs specialized on the clause and literal partitions respectively.  

### 2.3 Ricci Curvature of Graphs  

In Riemannian geometry, RC quantifies the local deviation of a manifold \(\mathcal{M}\) from flat Euclidean space, as a result of the metric defined on \(\mathcal{M}\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature.  

Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].