Table 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Variation</td><td colspan="4">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan="3">GCN</td><td>Vanilla</td><td>0.510 ± 0.012</td><td>0.180 ± 0.048</td><td>0.470 ± 0.031</td><td>0.650 ± 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 ± 0.018</td><td>0.154 ± 0.017</td><td>0.422 ± 0.013</td><td>0.664 ± 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 ± 0.014</td><td>0.170 ± 0.010</td><td>0.422 ± 0.016</td><td>0.662 ± 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 ± 0.012</td><td>0.176 ± 0.031</td><td>0.416 ± 0.027</td><td>0.654 ± 0.027</td></tr><tr><td rowspan="3">NeuroSAT</td><td>Vanilla</td><td>0.690 ± 0.022</td><td>0.436 ± 0.032</td><td>0.734 ± 0.017</td><td>0.746 ± 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 ± 0.030</td><td>0.436 ± 0.015</td><td>0.742 ± 0.027</td><td>0.758 ± 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 ± 0.020</td><td>0.416 ± 0.046</td><td>0.724 ± 0.022</td><td>0.726 ± 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 ± 0.018</td><td>0.438 ± 0.028</td><td>0.742 ± 0.025</td><td>0.758 ± 0.020</td></tr></table>  

<center>Figure 3: Low dimensional visualization of the literal embeddings produced by NeuroSAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center>  

[49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \(k\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.