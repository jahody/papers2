<center>Figure 2: (a) Average BFC as a function of \(\alpha\) for random 3-SAT problems with \(N = 256\) . The color is used as a representation for the average solvability of the problems at a given \(\alpha\) by the NeuroSAT model [46], with a group labeled as solvable if \(50\%\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \(\alpha_{c} \approx 4.267\) [32]. The average BFC drops monotonically with \(\alpha\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \(\alpha\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with NeuroSAT as a function of the variance and average of the BFC. Notice how as \(\alpha\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \(\alpha\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center>  

results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using PyTorch [41], PyTorch- Geometric [17] and PyTorch Lightning [15]. The networks were trained for 100 epochs using the AdamW optimizer [29], with learning rate \(\eta = 0.0001\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs.  

### 4.1 The Relationship Between Curvature and Satisfiability  

We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \(\alpha \in [3,5]\) in steps of \(\Delta \alpha = 0.1\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \(\alpha_{c} \approx 4.267\) . Considering its widespread use and downstream performance, we train the NeuroSAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \(2N\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \(N = 256\) , with the results being summarized in Figure 2.  

Our results show that by considering the probability of finding a solution at a given \(\alpha\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \(k\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \(\alpha_{d} \approx 3.927\) [32].