of satisfied clauses at each step. These local updates occur in continuous space and can therefore be viewed as gradient updates with respect to an implicit objective function measuring clause satisfaction. Variables are also represented in a high- dimensional vector space, similar to semi- definite programming as explained in B.  

From this perspective, Equations 6, 7, and 11 can be interpreted as a gradient descent algorithm searching for an optimal assignment over a high- dimensional unit sphere (due to unit normalization), while the final classification layer corresponds to a rounding step to Boolean values. In future work, we aim to manually derive these equations from a trained GNN using a primal- dual approach, interleaving gradient updates of primal and dual variables associated with constraints. We believe that by utilizing suitable proximal operators and an appropriate metric in the relaxed solution space, the GNN can be effectively interpreted as a primal- dual algorithm optimizing a continuous relaxation of the MaxSAT objective in a high- dimensional space. This points out to another major advantage of using the RNN update function because its simple form is suitable for such derivation.  

Deriving equations for such algorithms applicable to arbitrary combinatorial optimization problems would be highly beneficial in practice, allowing these equations to be parameterized by learnable matrices and fine- tuned for specific problem distributions. Such data- driven solvers would be analogous to physics- informed neural networks Cai et al. [2021], where substantial domain knowledge is embedded within the model, followed by fine- tuning to approximate the dynamics of a particular physical system. This approach results in fast numerical solvers tailored to specific domains. We believe that the development of data- drive numerical solvers represents an exciting future direction for combinatorial optimization research. To make these numerical solvers practical, it will still be necessary to integrate them into more complex systems, where they would function as guessing or bounding heuristic.  

Another limitation of our work is that the model was tested exclusively on random problems. This decision is justified by the findings of Li et al. Li et al. [2023], who demonstrated that models trained on random problem instances exhibit superior generalization to other distributions. Since Li et al. already provided experimental results demonstrating the transferability of models across different problem distributions, we chose not to repeat those experiments here.  

## 8 Conclusion  

This work provides a comprehensive analysis of graph neural networks for Boolean satisfiability problems. Our evaluation identified key design choices that enhance performance: variable- clause graph representation with RNN updates offers an effective balance of accuracy and efficiency, while our novel closest assignment supervision method significantly improves performance on problems with large solution spaces. The recurrent architecture enables flexible scaling during inference through additional message- passing iterations and resampling. Our diffusion model extension demonstrates another approach to inference- time adaptation, with further improvements possible by integrating classical techniques like unit propagation.  

Our analysis of embedding space patterns and optimization trajectories supports the interpretation that these models implicitly implement continuous relaxation algorithms for MaxSAT, explaining their ability to generalize to novel problem instances. This connection provides a theoretical framework for understanding neural reasoning capabilities in structured domains, with implications for the design of hybrid solving approaches.  

## References  

Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit- sat: An unsupervised differentiable approach. In International conference on learning representations, 2018.