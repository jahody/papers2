Table 3: Performance analysis of VCG+RNN with assignment prediction across different datasets and training methodologies. Our novel "Closest" supervision method (which dynamically selects assignments closest to current model predictions) consistently outperforms training with precalculated assignments. For SR40, SAT-only training with closest assignment supervision achieves the highest SAT accuracy (76%), while SAT+UNSAT training with closest assignment supervision yields the lowest average gap (0.98). The missing data for 3SAT100 with SAT+UNSAT closest supervision is due to prohibitive computational costs. Bold values indicate best results per dataset.  

<table><tr><td>Dataset</td><td>Training Mode</td><td>Assignment Type</td><td>Avg. Gap ↓</td><td>Gap on SAT ↓</td><td>Gap on UNSAT ↓</td><td>SAT Acc. ↑</td><td>Dec. Acc. ↑</td></tr><tr><td rowspan="4">SR40</td><td>SAT only</td><td>Precalculated</td><td>2.93</td><td>1.11</td><td>4.75</td><td>68.2 %</td><td>84.1 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>2.68</td><td>0.88</td><td>4.48</td><td>76 %</td><td>88 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>1.95</td><td>0.8</td><td>3.05</td><td>68.8 %</td><td>84.4 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>0.98</td><td>0.48</td><td>1.49</td><td>71.2 %</td><td>85.6 %</td></tr><tr><td rowspan="4">SR100</td><td>SAT only</td><td>Precalculated</td><td>4.42</td><td>2.36</td><td>6.48</td><td>47.4 %</td><td>73.7 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>3.57</td><td>1.67</td><td>5.48</td><td>59.6 %</td><td>79.8 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>3.81</td><td>2.34</td><td>5.28</td><td>44.8 %</td><td>72.4 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>1.43</td><td>0.92</td><td>1.94</td><td>53.2 %</td><td>76.6 %</td></tr><tr><td rowspan="4">3SAT100</td><td>SAT only</td><td>Precalculated</td><td>5.93</td><td>3.40</td><td>9.27</td><td>25.7 %</td><td>57.2 %</td></tr><tr><td>SAT only</td><td>Closest</td><td>5.23</td><td>2.33</td><td>9.11</td><td>48.4 %</td><td>70 %</td></tr><tr><td>SAT+UNSAT</td><td>Precalculated</td><td>4.22</td><td>2.84</td><td>6.00</td><td>23.9 %</td><td>55.8 %</td></tr><tr><td>SAT+UNSAT</td><td>Closest</td><td>—</td><td>—</td><td>—</td><td>—</td><td>—</td></tr></table>  

- Cross-distribution applicability: The model trained on SR40 maintains reasonable effectiveness on SR100 and 3SAT100, though with expected performance decrease. This aligns with findings from Li et al. Li et al. [2023], who demonstrated that models trained on SR distributions generally transfer well to other SAT problem structures.  

#### 5.3.2 Train-time vs Test-time Scaling  

Tables 4 and 5 present the performance of models trained on SR40 and SR100 distributions when evaluated across benchmarks of varying sizes. The SR40- trained model achieves reasonable generalization to larger instances, though with decreasing effectiveness as problem size increases. For SR100, the model achieves 74.2% decision accuracy despite being trained on smaller instances, showing good generalization capabilities.  

The SR100- trained model demonstrates better performance on larger instances compared to the SR40- trained model, as expected. On SR200, it achieves 83.0% decision accuracy compared to 58.5% for the SR40 model. This suggests that while test- time scaling can improve performance on larger problems, there are limits to this approach, and training models on larger instances might be necessary for optimal performance on very large problems.  

These results highlight that recurrent GNN architectures allow for a flexible computation- performance tradeoff that can be adjusted at inference time based on available computational resources and desired solution quality.  

### 5.4 Diffusion Model Extension  

As we mentioned in Section 3.4.4, one can use the GNN as a function approximator \(f_{\theta}(\cdot)\) inside a diffusion model. This enables another way of scaling the test- time compute. We adapt the diffusion model used by Sun et al. Sun and Yang [2023] where the function approximator is trained to predict the ground truth solution \(\mathbf{x_0} = f_{\theta}(\mathbf{x_t},t)\) conditioned on a sample \(\mathbf{x_t}\) at time \(t\) . The predicted assignment is then used to obtain a sample at time \(t - 1\) and this process is repeated again \(\mathbf{x_0} = f_{\theta}(\mathbf{x_{t - 1}},t - 1)\) until we reach \(t = 0\) . One application of the function approximator together with the sampling is called a diffusion step. The number of diffusion steps \(T\) used for inference is a parameter which can be adapted after the model was already