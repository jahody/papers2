variable assignments. This metric has the same definition for both SAT and UNSAT instances; it simply counts how many clauses remain unsatisfied with random assignments on average. Larger gaps indicate more challenging problems where random guessing performs poorly.  

### 5.2 Quantitative Evaluation  

We conducted a comprehensive evaluation that compares different architectural choices and supervision methods. Our evaluation focuses on five key performance metrics:  

- Average Gap: The average number of unsatisfied clauses across all test instances. Lower values indicate better performance, with 0 representing perfect satisfaction (i.e., no unsatisfied clauses) on satisfiable instances. For unsatisfiable instances, this metric reflects how close the model gets to minimizing unsatisfied clauses.- Gap on SAT: The average number of unsatisfied clauses computed only over satisfiable instances.- Gap on UNSAT: The average number of unsatisfied clauses computed only over unsatisfiable instances.- SAT Accuracy: The percentage of satisfiable instances for which the model correctly finds a satisfying assignment, computed only over satisfiable instances.- Decision Accuracy: The percentage of instances for which the model correctly predicts whether the formula is satisfiable. Since our approach does not formally refute unsatisfiable instances, we classify an instance as unsatisfiable when the model fails to find a satisfying assignment. This means unsatisfiable instances are always classified correctly under this assumption. This applies specifically in the case of assignment-based evaluation.  

#### 5.2.1 Comparison of Graph Representations, Update Functions and Training Methods  

Table 2 presents a comprehensive comparison of different architectural configurations trained exclusively on the SR40 dataset. This comparison includes different graph representations (LiteralClause Graph vs. Variable- Clause Graph), update functions (RNN vs. LSTM), and supervision approaches (SAT/UNSAT classification, assignment supervision, and unsupervised objective training), all evaluated on instances with 40 variables. All models were evaluated using Exponential Moving Average (EMA) of parameters during validation only, as detailed in A.1.2, which helps reduce fluctuations in validation metrics and provide more reliable model selection. Importantly, curriculum learning ( A.1.1) was employed only for training models with SAT/UNSAT classification objectives, as it proved unnecessary for models trained with assignment prediction or unsupervised learning approaches.  

Graph Representation Impact: Our results demonstrate that Literal- Clause Graph (LCG) and Variable- Clause Graph (VCG) representations exhibit different strengths. VCG shows better performance for assignment- based training with RNN updates, achieving a SAT accuracy of 68.8% compared to 48.6% for LCG. Additionally, VCG's more compact representation (using one node per variable rather than two for positive and negative literals) provides computational advantages for larger formulas, making it our preferred choice for scaling to more complex problems.