<center>Figure 2: Percentage of SAT instances solved as message passing iterations increase for a model trained on SR40 with SAT+UNSAT closest assignment supervision. Left: Performance on SR100, showing rapid initial improvement. Right: Comparison across benchmarks, demonstrating effectiveness decreases with problem size but benefits from additional iterations, highlighting the recurrent architecture's inference-time scaling capability. </center>  

<center>Figure 3: Average gap (unsatisfied clauses) reduction with increasing message passing iterations for a model trained on SR40 with SAT+UNSAT closest assignment supervision. Left: Comparison across benchmarks showing extremely rapid gap reduction in early iterations for all problem sizes, with all benchmarks achieving remarkably low average gaps despite varying SAT-solving performance. Right: Individual instance trajectories revealing different convergence patterns between SAT and UNSAT instances, with occasional fluctuations suggesting potential benefit from monitoring solution quality during inference. </center>  

trained and therefore, in this setting we have two types of iterations. One is the number of message- passing iterations and the second is the number of diffusion steps. In Table 6 we report the tradeoff between the number of message- passing steps (referred to as GNN_Steps) and the number of diffusion steps. The reported numbers correspond to the dataset SR100 with 100 variables in each problem. The model was trained on the SR40 distribution and the tested combinations use around 300 iterations in total distributed between the two types of steps.  

The experiments revealed a consistent trend: increasing the number of message- passing steps is generally more important for improving metrics such as Accuracy and Avg. Gap.  

#### 5.4.1 Connection to Assignment Prediction Training  

We also report an interesting finding which allows to simplify the function approximator used in the diffusion model. Notice that in the expression \(\mathbf{x}_0 = f_\theta (\mathbf{x}_t, t)\) it is also conditioned on the timestep \(t\) . This conditioning is dictated by the theory of diffusion models Nakkiran et al. [2024] and most of the models, including the one by Sun et al. Sun and Yang [2023] blindly follow this design choice. In our experiments, we found out that this conditioning is not needed and that the model sometimes works even better without it. Therefore, in all reported results, the