that satisfiability classification requires understanding the logical structure rather than statistical properties. As reported in the review by Li et al. Li et al. [2023], the models trained on problems from this generator transfer the best to other problem distributions.  

3- SAT Generator We also employ a 3- SAT generator configured at the critical clause- to- variable ratio of 4.26, known as the phase transition point where SAT problems are empirically the most challenging to solve [Crawford and Auton, 1996]. At this ratio, approximately half of the generated instances are satisfiable. Each clause contains exactly 3 literals selected uniformly from the available variables, with each literal negated with 0.5 probability. Unlike the SR generator, 3- SAT focuses on generating naturally difficult problems rather than explicitly preventing superficial feature learning.  

## 5 Experimental Results  

### 5.1 Training and Evaluation Methodology  

For training, we generate 50,000 instances: 25,000 pairs for SR and 50,000 instances for 3- SAT. We annotate each dataset by the maximum number of variables appearing in the training formulas. For SR, we test two variations, SR40 for which the training examples are sampled with 3- 40 variables and SR100 for which the training examples contain 10- 100 variables. For 3- SAT, the training samples contain 10- 100 variables (3SAT100). The SR dataset is well suited for training SAT/UNSAT prediction models due to its design that prevents learning from superficial features, making it harder for models to exploit statistical shortcuts rather than learning true logical reasoning. We also create versions of training data which contain only satisfiable instances (denoted SAT only). The size of these datasets is half of the original datasets (i.e. 25000 examples). To evaluate generalization, we validate exclusively on problems with exactly the maximum number of variables in each category, therefore SR40 for evaluation means that the problems have always exactly 40 variables (not a range of 3- 40), SR100 test contains only problems with exactly 100 variables, and so on.1  

Table 1 summarizes the key statistics of our evaluation datasets.  

Table 1: Statistics of benchmark test sets. SAT% indicates the percentage of satisfiable instances in each dataset. Avg. Gap represents the average number of unsatisfied clauses when using random variable assignments. SAT Gap and UNSAT Gap show this metric separated by instance satisfiability. SR datasets are generated using the SR generator with the indicated number of variables (e.g., SR40 contains instances with 40 variables), while 3SAT datasets contain instances near the phase transition point with the specified number of variables. All datasets maintain a balanced distribution of satisfiable and unsatisfiable instances.  

<table><tr><td>Dataset</td><td>SAT%</td><td>Avg. Gap</td><td>SAT Gap</td><td>UNSAT Gap</td><td>Avg. Clauses</td></tr><tr><td>SR40</td><td>50.0%</td><td>21.29</td><td>21.59</td><td>20.99</td><td>228.40</td></tr><tr><td>SR100</td><td>50.0%</td><td>51.31</td><td>50.64</td><td>51.98</td><td>547.49</td></tr><tr><td>SR200</td><td>50.0%</td><td>100.31</td><td>101.03</td><td>99.59</td><td>1083.81</td></tr><tr><td>SR400</td><td>50.0%</td><td>198.74</td><td>198.53</td><td>198.95</td><td>2152.32</td></tr><tr><td>3SAT100</td><td>53.5%</td><td>52.78</td><td>53.00</td><td>52.54</td><td>426.00</td></tr><tr><td>3SAT200</td><td>55.5%</td><td>107.65</td><td>107.45</td><td>107.90</td><td>852.00</td></tr></table>  

The Gap metric represents the average number of unsatisfied clauses when using random