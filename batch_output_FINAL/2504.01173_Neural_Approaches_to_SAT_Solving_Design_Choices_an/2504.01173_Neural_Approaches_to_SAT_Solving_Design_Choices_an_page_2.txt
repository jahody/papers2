their performance and on the theoretical side, understanding how a GNN can solve a CNF formula could help us to elucidate the reasoning ability of Transformers Vaswani et al. [2017] because Transformers can be viewed as GNNs in which the graph connectivity is given by the attention map and is learned from data Cai et al. [2023]. Our aim in this contribution is to provide an experimental evaluation of different design choices for GNNs in the context of Boolean satisfiability together with an intuitive explanation of the inner workings of these models. Our main contributions are as follows:  

- We provide an experimental comparison of different architectures and training regimes.- We introduce a novel supervision method based on the closest assignment, resulting in significant improvements.- We demonstrate that these architectures scale well at test time.- We extend the graph neural network to a diffusion model and show how it relates to the base model.- We provide an intuitive explanation for the inner workings of these models.  

The rest of the text has the following structure: Section 3 (Relevant Background) provides the necessary context on Boolean satisfiability problems, SAT solving approaches, graph neural networks, theoretical connection to approximation algorithms, and diffusion models. Section 4 (Experimental Setup) describes our methodology, including data representation choices, architecture variants, supervision methods, and benchmark generation. Section 5 (Experimental Results) presents our comprehensive evaluation, comparing different graph representations and training methods (Section 5.2), demonstrating test- time scaling capabilities (Section 5.3), and introducing our diffusion model extension (Section 5.4). Section 6 (Interpreting the Trained Model) offers analysis of the embedding space and explains the networks' behavior through the lens of approximation algorithms based on continuous relaxation. Section 2 (Related Work) positions our contribution within the broader research landscape, and Section 7 contains a discussion of our findings and directions for future research. We conclude in Section 8. Additional implementation details and mathematical derivations are provided in the Appendix.  

## 2 Related Work  

Our research builds directly upon NeuroSAT Selsam et al. [2018], which introduced the first end- to- end neural approach for SAT solving using a recurrent message- passing architecture. While we maintain the core iterative design of NeuroSAT (allowing variable numbers of message- passing iterations through weight sharing), we explore simplified variants using RNNs and LSTMs and incorporate techniques like curriculum learning to improve training efficiency.  

Several other works have explored different directions in neural SAT solving. Li et al. Li et al. [2023] developed G4SATBench to benchmark various GNN architectures (GCN, GGNN, GIN) across different graph representations and supervision objectives. Unlike their broader exploration across architecture types, our work focuses on the recurrent message- passing paradigm from NeuroSAT and investigates how different training objectives and graph representations affect performance within this specific framework. We also mention the work by Warde et al. Warde- Farley et al. [2023] who developed a recurrent architecture based on a Restricted Boltzmann Machine.  

Hybrid approaches that integrate neural networks with traditional solvers include NeuroCore by Selsam and Bjorner Selsam and Bjorner [2019], which uses neural predictions to guide variable branching in CDCL solvers. Similarly, Wang et al. Wang et al. [2021] proposed NeuroComb to enhance CDCL solvers through GNN- based identification of important variables and clauses.