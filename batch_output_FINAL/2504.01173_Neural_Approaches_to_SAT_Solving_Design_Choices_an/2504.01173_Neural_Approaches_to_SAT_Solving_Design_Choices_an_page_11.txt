approaches perform similarly in practice. Another training option would be to directly optimize a convex loss function derived from SDP relaxation, but this approach is limited because SDP formulations work well for MAX- 2- SAT and can be extended to MAX- 3- SAT, but become increasingly difficult to formulate for general MaxSAT problems with larger clauses.  

Assignment Prediction For satisfiable formulas, we can train the model to predict the satisfiable variable assignments directly. We tried to use either mean squared error or cross- entropy loss between the predicted assignments and the ground truth assignments: \(\mathcal{L}_{\mathrm{assign}}^{\mathrm{MSE}} =\) \(\| \hat{a} - x\|_{2}^{2}\) and \(\mathcal{L}_{\mathrm{assign}}^{\mathrm{CE}} = - \sum_{i}x_{i}\log \hat{x}_{i} + (1 - x_{i})\log (1 - \hat{x}_{i})\) where \(x\) is the ground truth assignment and \(\hat{a},\hat{x}\) are the predicted assignments which differ by application of softmax (i.e. \(\hat{a}\) are just logits without a softmax applied).  

Closest Assignment Training One problem with assignment prediction is that satisfiable formulas can have a lot of solution and the network is penalized even if it predicts satisfiable solution which differs from the one which is used as a ground truth. We therefore introduce a novel supervision method which uses a MaxSAT solver to always compute the solution which is closest to the solution predicted by the model. We then update then model with respect to this solution. In Section 5.2, we show that this method works particularly well when the solution space is large.  

For each formula in a batch, a valid assignments that minimize the Hamming distance to the model's current predictions is found by the RC2 MaxSAT solver. For satisfiable formulas it finds an assignment that satisfies all clauses while being closest to current prediction. For unsatisfiable formulas, it finds an assignment that maximizes the number of satisfied clauses while minimizing distance to prediction.  

This approach allows the model to explore different regions of the solution space while maintaining valid solutions for SAT instances or optimal partial solutions for UNSAT instances. The supervision signal adapts to the model's current state rather than forcing it toward a single pre- determined assignment. The disadvantage of this method is that the computation of the loss is slower then with the precomputed solution. This could be solved by pre- computing solutions or by using an approximate MaxSAT solver.  

SAT- Only Instance Filtering After initially training with both satisfiable and unsatisfiable instances, we experimented with formula- type specialization by restricting training to only satisfiable instances. In Table 3, we show that this filtering can lead to higher accuracy of the trained model.  

### 4.4 Benchmarks and Data Generation  

We utilize two complementary benchmark generators for evaluating the tested variants: the SR generator and a 3- SAT generator with the ratio between variable and clauses set close to the phase transition point.  

SR Generator The SR generator by Selsam et al. [Selsam et al., 2018] produces pairs of satisfiable and unsatisfiable formulas that differ by negating only a single literal. This design specifically prevents models from exploiting superficial features for classification. Intuitively, it works by iteratively sampling random clauses and adding them to a formula. After each addition, a SAT solver checks if the formula remains satisfiable. When adding a clause that finally makes the formula unsatisfiable, the generator saves this instance and creates its satisfiable counterpart by flipping a single literal in the last clause. To create each clause, it samples a small integer \(k\) based on a mix of Bernoulli and geometric distributions, then randomly selects \(k\) variables without replacement, negating each with 0.5 probability. This solver- driven approach ensures