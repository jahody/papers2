takes as input a concatenation of a term representing the Bethe average energy ( \(U\) of Equation 3) and two terms representing the components of the Bethe entropy ( \(H\) of Equation 3) across all iterations, summed across factors within each iteration. As already mentioned in Section 2.1, damping is a standard technique for improving convergence of BP. The BPNN architecture allows for possibly applying a learned operator \(\Delta : \mathbb{R}^{\sum_{i = 1}^{n} 2 \cdot \deg (x_{i})} \to \mathbb{R}^{\sum_{i = 1}^{n} 2 \cdot \deg (x_{i})}\) to the difference between iterations \(k + 1\) and \(k\) of every factor- to- variable message, and jointly modify it, in place of the scalar multiplier \(\alpha\) of Equation 4.  

### 3.2 Neural Belief Propagation with Attention  

Attention mechanisms, as seen in Section 2.2, are a technique that allows a deep learning model to combine input representations additively, while also enforcing permutation invariance. GNNs endowed with an attention mechanism are a promising research direction in the neuro- symbolic computing scenario [5,13,17], as they might enhance structured reasoning and efficient learning.  

This is the reason for modifying the BPNN architecture by augmenting it with a GAT- style attention mechanism (i.e. analogous to that of Equations 5, 6, 7). In what follows we will refer to our architecture as BPGAT, to underline the fact that it puts together the algorithmic structure of BP and the computational formalism of GATs.  

BPGAT works by taking as input a bipartite factor graph \(G = (V,E)\) , with factor nodes \(\{f_{j}\}_{j = 1}^{m}\) and variable nodes \(\{x_{i}\}_{i = 1}^{n}\) , representing a CNF SAT formula and outputs an approximation \(\ln \hat{Z}\) of the logarithm of the exact number of solutions of the input formula. As for BPNN, messages between nodes are partitioned into factor- to- variable messages and variable- to- factor messages, and computations are performed in the log- space.  

Considering variable- to- factor messages, at each iteration \(k + 1\) every variable node \(x_{i}\) contains the aggregated messages \(\hat{m}_{i \to j}^{(k)}\) received from its neighborhoods at the previous iteration \(k\) , and needs to receive messages from its adjacent clause nodes, and aggregate them. Such aggregation is done using a GAT- style multi- head attention mechanism: as for the GAT attentional layer, the model computes attention coefficients \(\alpha_{ij}\) for every factor node \(f_{j}\) in the neighborhood of the variable node \(x_{i}\) by projecting each message \(\hat{m}_{i \to j}^{(k)} \in \mathbb{R}^{2}\) and variable node hidden representation representation \(\hat{m}_{i \to j}^{(k)} \in \mathbb{R}^{2}\) via a weight matrix \(W \in \mathbb{R}^{2 \times 2}\) , then passing the concatenation of the projected messages through a single- layer feedforward neural network \(\mathbf{a} \in \mathbb{R}^{4}\) , then feeding the results to a LeakyReLU and finally applying softmax to normalize them across the neighborhood \(\mathcal{N}(x_{i})\) of \(x_{i}\) . More formally, attention coefficients are computed as:  

\[\alpha_{ij}^{(k + 1)} = \frac{\exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{i \to j}^{(k)},W\hat{m}_{j \to i}^{(k)})]))}{\sum_{k \in \mathcal{N}(x_{i})} \exp(\mathrm{LeakyReLU}(\mathbf{a}^{T}[\mathrm{concat}(W\hat{m}_{i \to j}^{(k)},\hat{m}_{k \to i}^{(k)})]))} \quad (10)\]