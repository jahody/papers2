in Section III. We compare DeepGate2 with the original DeepGate and another functionality- aware solution [8] in Section IV. Next, we apply DeepGate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper.  

## II. RELATED WORK  

### A. Circuit Representation Learning  

A prominent trend in the deep learning community is to learn a general representation from data first and then apply it to various downstream tasks, for example, GPT [16] and BERT [17] learn representations of natural language text that can be fine- tuned for a wide range of natural language processing tasks. Circuit representation learning has also emerged as an attractive research direction, which falls into two categories: structure- aware circuit representation learning [9], [11], [12] and functionality- aware circuit representation learning [7], [8].  

Since a circuit can be naturally formulated as a graph, with gates as nodes and wires as edge, the GNN is a powerful tool to capture the interconnections of logic gates and becomes a backbone model to learn circuit representations. For example, TAG [9] is a GNN- based model designed for analog and mixed- signal circuit representation learning and applied for several physical design applications, such as layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. ABGNN [12] learns the representation of digital circuits and handles the arithmetic block identification task. However, these models tend to focus on structural encoding and are not suitable for functionality- related tasks.  

Consequently, the functionality- aware circuit representation learning frameworks [7], [8] are designed to learn the underlying circuit functionality. For instance, FGNN [8] learns to distinguishes between functionally equivalent and inequivalent circuits by contrastive learning [18]. However, such self- supervised manner relies on data augmentation by perturbing the original circuit to logic equivalence circuit. If the perturbation is not strong and diverse, the model still identifies the functional equivalence circuits based on the invariant local structure, resulting a low generalization ability on capturing underlying functionality. DeepGate [7] leverages logic- 1 probability under random simulation as supervision, which approximates the statistic of the most direct representation of functionality, i.e. truth table. Despite achieving remarkable progress on testability analysis [5], there are limitations that affect the generalizability of DeepGate to other EDA tasks. We will elaborate on DeepGate in the next subsection.  

### B. DeepGate Framework  

DeepGate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. DeepGate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the DeepGate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability.  

<center>Fig. 1. An example of reconvergence structure </center>  

First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. DeepGate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, DeepGate cannot differentiate the functional difference between two circuits if they have the same probability.  

Second, DeepGate is not efficient enough to deal with large circuit. Specifically, DeepGate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in DeepGate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \(h_i\) is the embedding vector of node \(i\) . Since, DeepGate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation.  

We emphasize that the limitations of DeepGate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision.  

## III. METHODOLOGY  

### A. Problem Formulation  

The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous DeepGate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space.  

We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, DeepGate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by DeepGate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between