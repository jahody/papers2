### C. Comparison with other Models on Logic Equivalence Gates Identification  

This section compares the functionality- aware accuracy, as defined in Section IV- A2 of DeepGate2 with that of two other models: DeepGate [7] and FGNN [8]. The DeepGate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning.  

Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while DeepGate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \(99.15\%\) of logic equivalence gate pairs with a precision of \(97.48\%\) , indicating a low false positive rate. In contrast, DeepGate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although DeepGate has an average recall of \(91.46\%\) , its precision is only \(54.00\%\) , indicating a large number of false positive identifications. This is because DeepGate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \(80.83\%\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates.  

### D. Effectiveness of PI Encoding Strategy  

To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \(20.07\%\) .  

Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \(93.22\%\) , while the w/o model only achieve \(74.56\%\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately.  

### E. Effectiveness of Training Strategies  

To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural  

correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137.  

We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \((L_{prob})\) and identifying reconvergence structures \((L_{rc})\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \(L_{func} = 0.0594\) , which is \(51.47\%\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \(L_{func}\) value of 0.0594, which is \(51.47\%\) smaller than that of the latter.  

## V. DOWNSTREAM TASKS  

In this section, we combine our DeepGate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints.  

### A. Logic Synthesis  

This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs.  

To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency.  

1) Experiment Settings: We combine our DeepGate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our