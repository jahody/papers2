<center>Figure 3: Optimization performance over different CO domains. Aligned with [29], we normalize each benchmark's indicator values using \(1 - \frac{value - min}{2\times (max - min)}\) , where \(value\) represents the method's measured indicator, with \(min\) and \(max\) denoting respectively the minimum and maximum values observed during evaluation. A larger shaded area corresponds to superior performance. </center>  

pooling. We adapt the LLM component based on Qwen2.5- Coder- 7B- Instructor model through modifying its architecture, forward propagation dynamics and inference paradigm. Comprehensive implementation details including hyperparameter configurations, adaptations, and software dependencies are provided in Appendix B.1 and B.2.  

Training, Inference and Used Hardware. We first train domain- specific GNNs for SAT and MILP problems using aforementioned benchmark dataset. The GNNs are trained for three epochs on SAT instances and four epochs on MILP instances, followed by conducting post- training of the adapted LLM with a specialized corpus. All post- training data is collected via queries to Qwen2.5- Coder- 7B- Instructor, aligned with the model used in the post training. The adapted LLM undergoes three epochs of post training per domain. Both GNN and LLM selecting optimal checkpoints based on validation prediction loss. We then integrate the composite model (i.e. the trained GNN and adapted LLM) into an EA- based framework to discover solver- specific algorithm configurations optimized for training instances. Finally, we evaluate the performance of identified algorithms on held- out test sets. All experiments are conducted on hardware platform with dual AMD EPYC 9534 64- core processors @ 2.45GHz and two NVIDIA H800 80GB GPUs connected via PCIe. More training specifics and data curation are detailed in Appendix B.1 and B.2 respectively.  

### 5.3 Results  

## Optimization Performance (Answer to RQ1).  

The comparative optimization results are presented in Figure 3 and Table 1 ("CGD" and "ZAM" denotes CoinsGrid and Zamkeller dataset respectively) with respect to primary objective metrics (i.e. PAR- 2 and Number of Timeout for SAT and PD integral for MILP domains). Comprehensive results for all evaluation metrics are provided in Appendix G.1). As evidenced in Figure 3 and Table 1, our STRCMP with various post- training variants consistently matches or exceeds all baseline performance across both SAT and MILP domains. Specifically for SAT, STRCMP demonstrates universal superiority over its closest counterpart AutoSAT, particularly achieving significant reductions in terms of timeout (77.8% on Zamkeller: \(18 \rightarrow 4\) ; 66.7% on PRP: \(9 \rightarrow 3\) ) and solving time (on PRP: 22967 seconds \(\rightarrow 21146\) seconds; on Zamkeller: 20772 seconds \(\rightarrow 6929\) seconds). On MILP benchmarks, STRCMP maintains strong performance parity with NCO methods L2B/HEM and its direct competitor LLM4Solver. These  

Table 1: Optimization performance result w.r.t. Number of Timeout between different methods over SAT domain.   

<table><tr><td rowspan="2">Compared Methods</td><td colspan="3">Number of Timeout (â†“)</td></tr><tr><td>CNP CGD PRP</td><td>ZAM</td><td></td></tr><tr><td>AutoSAT [29]</td><td>32</td><td>16</td><td>9</td></tr><tr><td>EasySAT</td><td>32</td><td>25</td><td>50</td></tr><tr><td>NeuroSAT [40]</td><td>44</td><td>18</td><td>46</td></tr><tr><td>STRCMP</td><td>31</td><td>17</td><td>44</td></tr><tr><td>STRCMP (DPO Only)</td><td>32</td><td>16</td><td>3</td></tr><tr><td>STRCMP (SFT Only)</td><td>33</td><td>16</td><td>45</td></tr><tr><td>STRCMP w/o GNN</td><td>32</td><td>16</td><td>44</td></tr></table>  

Zamkeller: 20772 seconds \(\rightarrow 6929\) seconds). On MILP benchmarks, STRCMP maintains strong performance parity with NCO methods L2B/HEM and its direct competitor LLM4Solver. These