graph representation of MILPs – with variable and constraint nodes connected via edges representing their coefficients – to learn branching policies through imitation learning. Key innovations include encoding MILP states as bipartite graphs with constraint/variable features, designing permutation-invariant sum- based graph convolutions with prenormalization layers to handle variable- sized inputs, and training via behavioral cloning of strong branching decisions using cross- entropy loss. All configurations take the default value used in [14] for fair comparison.  

HEM [15, 16]. The paper addresses the challenge of improving cut selection in mixed- integer linear programming (MILP) solvers by simultaneously optimizing three critical aspects: which cuts to select (P1), how many to choose (P2), and their ordering (P3). Existing learning- based methods focus primarily on scoring individual cuts while neglecting dynamic cut count determination and sequence dependencies. To overcome these limitations, the authors propose a novel hierarchical sequence model (HEM) that leverages a two- level architecture: a higher- level policy predicts the ratio of cuts to select using a tanh- Gaussian distribution, while a lower- level pointer network formulates cut selection as a sequence- to- sequence learning problem to output ordered subsets. The model is trained via hierarchical policy gradient optimization within a reinforcement learning framework, where states encode MILP relaxation features and candidate cut characteristics, actions represent ordered cut subsets, and rewards correspond to solver performance metrics like primal- dual gap integral. All hyperparameters take the default value used in [15, 16] for fair comparison.  

NeuroSAT [40]. The paper proposes a message- passing neural network (MPNN) to solve the Boolean satisfiability problem by training solely on binary labels indicating satisfiability. The key innovation lies in representing SAT instances as bipartite graphs where literals and clauses are nodes connected via edges, and leveraging permutation- invariant neural architectures to process these graphs. NeuroSAT iteratively refines node embeddings through bidirectional message passing: clauses aggregate information from their constituent literals, while literals update their states based on connected clauses and complementary literals. Trained on a distribution of random SAT problems generated by incrementally adding clauses until unsat, the model learns to predict satisfiability via a cross- entropy loss on the final aggregated literal embeddings. Crucially, the architecture enforces structural symmetries (variable/clause permutation invariance, negation equivalence) and generalizes to larger instances and entirely unseen domains (e.g., graph coloring, vertex cover) at test time by simply extending the message- passing iterations, despite being trained only on small random \(n \leq 40\) problems. All configurations of this algorithm take the default used in [40] for fair comparison. Note that since NeuroSAT is a prediction framework for Boolean satisfiability problem, it is meaningless to measure the solving time for this framework over the SAT instances. Besides, the ground- truth dataset used to train the NeuroSAT model is those SAT instances can be proved/solved within timelimit \(\tau\) . Thus, we only count the number predicted to be correct of NeuroSAT (this number must be less than the number of ground- truth labels). Then, the number of timeout for NeuroSAT is equal to the total number of test instances minus the number predicted to be correct.  

## C.2.2 Evolutionary Code Optimization  

AutoSAT [29]. The paper introduces a framework that leverages Large Language Models (LLMs) to automatically optimize heuristics in Conflict- Driven Clause Learning (CDCL) SAT solvers. The authors address the challenge of manually designing and tuning heuristic functions in modern CDCL solvers, which is time- consuming and expert- dependent. Instead of generating solvers from scratch, AutoSAT operates within a modular search space comprising nine key heuristic functions (e.g., branching, restart, and clause management) derived from an existing CDCL solver. The framework employs LLMs to iteratively refine these heuristics through two search strategies: a greedy hill climber (GHC) and a \((1 + 1)\) Evolutionary Algorithm (EA), where LLMs generate candidate code modifications guided by performance feedback. All configurations of this algorithm take the default used in [29] for fair comparison.  

LLM4Solver [30]. The paper proposes a novel framework that integrates large language models (LLMs) with evolutionary search to automate the design of high- performance diving heuristics for exact combinatorial optimization (CO) solvers. The key challenge addressed is the inefficiency of traditional manual and learning- based approaches in navigating the vast, discrete algorithm space for CO solver components like diving heuristics, which require domain expertise and suffer from poor generalization. The core methodology leverages LLMs as prior- knowledge- guided generators to produce candidate algorithms encoded as interpretable score functions, while a derivative- free evolutionary framework (single- or multi- objective) optimizes these candidates through iterative