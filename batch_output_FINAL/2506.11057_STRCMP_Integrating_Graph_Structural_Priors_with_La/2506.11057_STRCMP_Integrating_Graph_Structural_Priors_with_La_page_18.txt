## B.2 Structure-Aware Code Generation  

Data Curation. We begin by assembling a curated collection of mathematical models for CO problems, which are directly compatible with corresponding CO solvers, alongside their natural language descriptions. Subsequently, for each CO problem, we compile a prompt that integrates the natural language description with specific code generation requirements. For instance, the code generation requirements may include details such as the function name, input/output parameters, expected function behavior, relevant background knowledge, etc. This prompt is then fed into an LLM to generate a code snippet. Note that to maximize the diversity of collected code snippets, we employ multiple LLM queries per prompt under high temperature settings to sample distinct candidate implementations. Each generated code snippet is evaluated by embedding it into the target solver and solving the corresponding CO problem, thereby obtaining performance metrics for the code snippet. Note that based on the principles of data curation, we collect the needed data via querying Qwen2.5- Coder- 7B- Instructor same as the model adopted in the following post training procedure.  

Post Training. Specifically, we first curate the collected data by processing each CO problem \(Q_{i}\) with its associated prompt \(x_{i}\) and corresponding multiple generated code snippets \(y_{i,j}(j = 1,\dots,M)\) These code snippets are systematically ranked based on previously obtained performance metrics to establish quality ordering. The highest- performing code- prompt pairs are selected to form the SFT dataset \(\mathcal{D}_{S F T} = \{(x_{i},y_{i}^{*})\}_{i = 1}^{N}\) . Additionally, by leveraging pairwise comparisons extracted from the established ranking hierarchy, we derive the preference dataset \(\mathcal{D}_{D P O} = \{(x_{i},y_{w},y_{l})\}_{i = 1}^{N}\) where \(y_{w},y_{l}\) are preferred/dispreferred code snippets. The training process can be formulated as follows:  

\[\mathcal{L}_{\mathrm{DPO}} = -\mathbb{E}_{(x,y_{w},y_{l})\sim \mathcal{D}_{D P O}}\left[\log \sigma \left(\beta \ln \frac{\pi_{\theta_{L}}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)} -\beta \ln \frac{\pi_{\theta_{L}}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right)\right], \quad (16)\]  

where \(\sigma\) is the Sigmoid function; \(\beta\) is the hyperparameter that governs the trade- off between reward maximization and KL divergence minimization; \(\pi_{\mathrm{ref}}\) is the reference model trained on \(\mathcal{D}_{S F T}\) . Note that both \(\pi_{\mathrm{ref}}\) and \(\pi_{\theta_{L}}\) updates only the parameter \(\theta_{L}\) of the composite model. Through above posttraining, we obtain a generative model capable of structure- aware code generation that simultaneously respects combinatorial optimization problems' inherent topological constraints and solver- specific syntactic requirements. Note that based on above principle of data curation and post- training, we collect 8k and 4k post- training instances for MILP and SAT instances respectively.  

Implementation & Training Details. To implement the proposed composite model, we first design a structure- prior- aware forward propagation mechanism and corresponding adapted inference framework based on the Qwen2.5- Coder- 7B- Instructor model via the Transformers library.  

\(\bullet\) Structure- Prior- Aware Forward Propagation Mechanism. Specifically, we process the input prompt through the tokenizer and the model's embedding layer to obtain \(i n p u t\_ e m b e d s\) , which are then merged with structural feature vectors of combinatorial optimization problems extracted by the previous graph neural network.  

First, we align the dimensionality of the combinatorial optimization problem feature vector \(\pmb{h}_{q}\in \mathbb{R}^{d}\) with the hidden layer dimensions of the large language model via zero- padding. Given the text embedding shape \(e m b e d s\in \mathbb{R}^{B\times S\times H}\) , where \(B\) is the batch size, \(S\) is the sequence length (number of tokens), and \(H\) is the hidden dimension, the dimension- adapted feature vector is obtained as:  

\[\mathbf{H}_{q} = Z e r o P a d d i n g(\mathbf{h}_{q}\oplus \mathbf{0}^{(H - d)})\in \mathbb{R}^{B\times H} \quad (17)\]  

where ZeroPadding denotes zero- padding, and \(\mathbf{H}_{q}\) represents the padded graph structural feature vector. Subsequently, we fuse text and structural features between the embedding layer and decoder layer by prepending the graph feature vector to the text embedding sequence, forming a hybrid input:  

\[\mathcal{E}(e m b e d s,\mathbf{H}_{q}) = [\mathrm{CLS}]\oplus \mathbf{H}_{q}\oplus e m b e d s[1:] \in \mathbb{R}^{B\times (1 + S)\times H} \quad (18)\]  

Here, \(e m b e d s\) is the input_embeds obtained from processing the textual prompt via the tokenizer and embedding layer, enabling the self- attention mechanism to jointly model textual semantics and graph structural features. A mask of all True values is constructed and merged with the attention_mask to match the shape of the combined input_embeds, ensuring \(\mathbf{H}_{q}\) participates in attention computation. Let the original attention_mask shape be \(M = \{0,1\}^{B\times S}\) ; the merged attention_mask becomes:  

\[M_{i n} = [M_{0:1};\mathbf{1}^{B};M_{1:S}]\in \{0,1\}^{B\times (1 + S)} \quad (19)\]