Machine learning (ML) techniques, particularly deep learning and reinforcement learning, have demonstrated significant potential in addressing CO problems [7]. As many CO problems arising from similar application domains exhibit inherent structural patterns, ML methods can leverage these patterns to reduce computational complexity in traditional CO approaches through data- driven strategies. The integration of ML and CO frameworks can be broadly categorized into three paradigms[7]: (1) End- to- end neural solvers [8–10], which train ML models to directly output CO solutions; (2) ML- augmented algorithm configuration [11–13], leveraging ML models to predict optimal algorithmic configurations for specific CO algorithm to solve associated problems; and (3) Hybrid methods [14–16], embedding ML models as critical decision modules within traditional CO solvers. However, real- world adoption of above approaches remains limited due to reliance on training distributional alignment for generalization and inherently insufficient interpretability [17].  

Building on the explosive advancements in large language models (LLMs) over recent years, combinatorial optimization tasks have shown potential for delegation to LLMs, particularly owing to their improved accountability and interpretability compared to conventional ML approaches. Capitalizing on their extensive world- knowledge priors and advanced code generation capabilities, numerous LLM- based approaches for CO problems have emerged. These methods bifurcate into two classes: (1) direct utilization of LLMs to solve CO problems [18, 19], and (2) employing LLMs to discover high- quality algorithm (in the form of solver- specific codes) for traditional solvers [20–22]. For example, Yang et al. [18] present Optimization by PROmpting (OPRO), which harnesses LLMs as black- box optimizers through iterative solution refinement via natural- language problem specifications and optimization trajectories. In contrast, Romera- Paredes et al. [20] proposes FunSearch that integrates a pre- trained LLM into an evolutionary algorithm to incrementally generate solver code, achieving state- of- the- art results on the cap set problem and discovering novel heuristics for online bin packing. Additionally, Huang et al. [19] demonstrates that multi- modal fusion of textual and visual prompts enhances LLM- driven optimization performance, as evidenced in capacitated vehicle routing problems.  

Despite these advances, current LLM- based approaches for solving CO problems remain in their infancy. As illustrated in Figure 1, they primarily rely on textual (or occasionally visual) prompting mechanisms to interface with LLMs, failing to effectively exploit the inherent topological structures of CO problems. Historically, human experts have successfully leveraged structural priors of CO problems to design sophisticated and efficient algorithms [4–6]. Furthermore, existing LLM- based methods [20, 21, 18] often require multiple iterations to produce high- quality solutions or algorithm implementations, primarily due to their lack of integration with CO- specific structural priors. As highlighted in Yang's insightful analysis [23], pre- training language model establishes strong priors for conversational tasks but weaker priors for structured domains like computer controls or video games — challenges exacerbated in solving CO problems, as these domains differ significantly from Internet text distributions. Given these challenges, developing a generative model that effectively integrates structural priors for CO problem solving becomes particularly compelling. As shown in Figure 1, the distinct insight lies in constructing a generative model that generates solver- specific code while respecting the intrinsic topological structure of CO problems. Theoretically, this approach aligns with the rising research directions of multi- modal generative model. Practically, such integration offers substantial benefits, dramatically reducing inference iterations while enhancing solution quality in solving CO problem.  

To tackle this challenge, we present STRcMP 1, a novel structure- prior- aware LLM- based algorithm discovery framework for solving CO problems. To the best of our knowledge, STRcMP is the first framework to explicitly integrate structural priors of CO problems into LLM- driven algorithm discovery, jointly improving solution quality and computational efficiency. Our framework first constructs a composite architecture combining a graph neural network (GNN) and an LLM. The GNN extracts structural embeddings from input CO instances, which serves as inductive biases for subsequent algorithm discovery via code generation. The LLM then produces solver- specific code conditioned on these structural priors, ensuring adherence to solver syntax, preservation of CO problems' topological characteristics, and alignment with natural language optimization objectives. This composite model is further embedded within an evolutionary algorithm- based refinement process to iteratively enhance solution quality. Besides, we provide the theoretical analysis why fusing the structure prior into LLM can benefit solving CO problem from the perspective of information theory.