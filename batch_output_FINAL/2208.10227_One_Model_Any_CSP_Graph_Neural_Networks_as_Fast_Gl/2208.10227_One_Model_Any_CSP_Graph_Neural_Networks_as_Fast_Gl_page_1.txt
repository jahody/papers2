# One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction  

Jan Tönshoff \(^{1}\) , Berke Kisin \(^{2}\) , Jakob Lindner \(^{3}\) , Martin Grohe \(^{4}\)  

\(^{1}\) RWTH Aachen, toenshoff@informatik.rwth- aachen.de \(^{2}\) RWTH Aachen, berke.kisin@rwth- aachen.de \(^{3}\) RWTH Aachen, jakob.lindner@rwth- aachen.de \(^{4}\) RWTH Aachen, grohe@informatik.rwth- aachen.de  

## Abstract  

We propose a universal Graph Neural Network architecture which can be trained as an end- 2- end search heuristic for any Constraint Satisfaction Problem (CSP). Our architecture can be trained unsupervised with policy gradient descent to generate problem specific heuristics for any CSP in a purely data driven manner. The approach is based on a novel graph representation for CSPs that is both generic and compact and enables us to process every possible CSP instance with one GNN, regardless of constraint arity, relations or domain size. Unlike previous RL- based methods, we operate on a global search action space and allow our GNN to modify any number of variables in every step of the stochastic search. This enables our method to properly leverage the inherent parallelism of GNNs. We perform a thorough empirical evaluation where we learn heuristics for well known and important CSPs from random data, including graph coloring, MAXCUT, 3- SAT and MAX- \(k\) - SAT. Our approach outperforms prior approaches for neural combinatorial optimization by a substantial margin. It can compete with, and even improve upon, conventional search heuristics on test instances that are several orders of magnitude larger and structurally more complex than those seen during training.  

## 1 Introduction  

Constraint Satisfaction Problems (CSP) are a ubiquitous framework for specifying combinatorial search and optimization problems. They include many of the best- known NP- hard problems such as Boolean satisfiability (SAT), graph coloring (COL) and maximum cut (MAXCUT) and can flexibly adapted to model specific application dependent problems. CSP solution strategies range from general solvers based on methods such as constraint propagation or local search (see Russell et al. (2020), Chapter 6) to specialized solvers for individual problems like SAT (see Biere et al. (2021)). In recent years, there is a growing interest in applying deep learning methods to combinatorial problems including many CSPs (e.g. Khalil et al. (2017), Selsam et al. (2018), Tönshoff et al. (2021)). The main motivation for these approaches is to learn novel heuristics from data rather than crafting them by hand.  

Graph Neural Networks (see Gilmer et al. 2017) have emerged as an effective tool for learning powerful, permutation invariant functions on graphs using deep neural networks, and they have become one of the main architectures in the field of neural combinatorial optimization. Problem instances are modelled as graphs and then mapped to approximate solutions with GNNs. However, most methods use graph reductions and GNN architectures that are problem specific, and transferring them across combinatorial tasks requires considerable engineering, limiting their use cases. Designing a generic neural network architecture and training procedure for the general CSP formalism offers a powerful alternative. Then learning heuristics for any specific CSP becomes a purely data driven process requiring no specialized graph reduction or architecture search.   

We propose a novel GNN based reinforcement learning approach to general constraint satisfaction. The main contributions of our method called \(\mathrm{ANYCSP}^{1}\) can be summarized as follows: We define a new graph representation for general CSP instances which is generic and well suited as an input for recurrent GNNs. It allows us to directly process all CSPs with one unified architecture and no prior reduction to more specific CSPs, such as SAT. In particular, one \(\mathrm{ANYCSP}\) model can take every CSP instance as input, even those with domain sizes, constraint arities, or relations not seen during training. Training is unsupervised using policy gradient ascent with a carefully tailored reward scheme that encourages exploration and prevents the search to get stuck in local maxima. During inference, a trained \(\mathrm{ANYCSP}\) model iteratively searches the space of assignments to the variables of the CSP instance for an optimal solution satisfying the maximum number of constraints. Crucially, the search is global; it allows transitions between any two assignments in a single step. To enable this global search we use policy gradient methods to handle the exponential action spaces efficiently. This design choice speeds up the search substantially, especially on large instances. We thereby overcome a primary bottleneck of previous neural approaches based on local search, which only flips the values of a single or a few variables in each step. GNN based local search tends to scale poorly to large instances as one GNN forward pass takes significantly more time than one step of classical local search heuristics. \(\mathrm{ANYCSP}\) accounts for this and exploits the GNNs inherent parallelism to refine the solution globally in each step.  

We evaluate \(\mathrm{ANYCSP}\) by learning heuristics for a range of important CSPs: COL, SAT, MAXCUT and gen