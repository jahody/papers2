Table 5: Selected Hyperparameters for each considered CSP   

<table><tr><td></td><td>MODEL RB</td><td>k-COL</td><td>3-SAT</td><td>Max-k-SAT</td><td>MAXCUT</td></tr><tr><td>d</td><td>128</td><td>128</td><td>128</td><td>128</td><td>128</td></tr><tr><td>⊕</td><td>MAX</td><td>MAX</td><td>MAX</td><td>MEAN</td><td>SUM</td></tr><tr><td>λ</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td></tr><tr><td>Ttrain</td><td>40</td><td>40</td><td>40</td><td>40</td><td>40</td></tr><tr><td>batch size</td><td>25</td><td>25</td><td>25</td><td>25</td><td>25</td></tr><tr><td>lr</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td></tr></table>  

cessed is usually determined by the memory required for the message passes between values and constraints. Let \(\mathcal{F} = (\mathcal{X},\mathcal{C},\mathcal{D})\) be a CSP instance with constraints of arity \(k\) and domains of uniform size \(\ell\) . Then the constraint value graph will contain \(|\mathcal{C}|\cdot k\cdot \ell\) constraint edges. Constraint edges are represented with a sparse matrix. For each non- zero entry of the sparse matrix (edge), we store the row (outgoing node) and the column (incoming node). Thus the space complexity of storing constraint edges is \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell)\) . During message passing from values to constraints, each value generates two messages with length d. Those messages are stacked along the first dimension, resulting in a dense matrix with \(2\cdot |\mathcal{X}|\cdot \ell \cdot d\) entries. To pass messages, we use sparse dense matrix multiplication (SPMM) between the sparse matrix of the edges and the dense matrix of the messages generated from values. Alternatively, one can also use the scatter operation from the PyTorch Scatter library, but the scatter operation requires the construction of an intermediate tensor stacking the messages send along each edge. This allocates extra memory of size \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell \cdot d)\) . In contrast, SPMM only allocates memory for the result of the aggregation, but no intermediate tensor is build. As a result, the space complexity of the SPMM operation is \(\mathcal{O}(|\mathcal{C}|\cdot d)\) for all aggregation types. Combining all of the terms, we get a space complexity of \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell +|\mathcal{X}|\cdot \ell \cdot d + |\mathcal{C}|\cdot d)\) for the message passing from values to constraints. The message passing from constraints to values uses the same operations that are used for values to constraints, therefore, the space complexity is the same for both directions.  

PyTorch Sparse supports generalized SPMM only in CSR format \(^6\) . Their implementation requires the expensive conversion of the sparse matrix from COO to CSR. The adjacency matrix between the stack of generated messages and the constraints is re- wired in every iteration \(t\) according to the new edge labels. Converting a new large matrix into CSR format in every step would be prohibitively expensive. To avoid that, we implement generalized sparse dense matrix multiplication in COO format with CUDA. With our newly implemented function we can pass messages memory efficiently and faster.  

We also experimented with more advanced attention- based aggregation, namely GAT (Veličković et al. 2017). However, it did not improve the performance but made the construction of large, intermediate edge- level tensors in the message passes unavoidable. Due to this, we restrict our focus on the  

three basic aggregations of element- wise SUM, MEAN and MAX.  

## A.5 Relabeling Constraint Value Graphs  

One critical requirement for ANYCSP is a fast subroutine for recomputing the edge labels \(L_{E}\) given the newly sampled assignment \(\alpha^{(t)}\) in step \(t\) . Our implementation of this relabeling procedure is based entirely on PyTorch and is GPU accelerated to maximize performance. Here, we will briefly discuss how this implementation works.  

Let \(\mathcal{F} = (\mathcal{X},\mathcal{D},\mathcal{C})\) be a CSP instances and let \(\alpha\) be the newly sampled assignment for which we have to compute the edge labels \(L_{E}\) . We first compute the node labels \(L_{V}\) which are a simple binary encoding of \(\alpha\) . Let \(C \in \mathcal{C}\) be some constraint with scope \(s^{C} = (X_{1}, \ldots , X_{k})\) , relation \(R^{C} \in \mathcal{D}(X_{1}) \times \dots \times \mathcal{D}(X_{k})\) and arity \(k\) . For each tuple \(r \in R^{C}\) we compute a score that counts how many of the values occurring in \(t\) are currently chosen by \(\alpha\) :  

\[s(r) = \sum_{i = 1}^{k}L_{V}(r_{i}) \quad (19)\]  

For each value \(\boldsymbol {v}\in \mathcal{V}_{X_{i}}\) of each variable \(X_{i}\) in the scope of \(C\) we then compute the maximum of \(s(t)\) over all tuples \(r\in R^{C}\) with \(v\in r\) ..  

\[m(C,\boldsymbol {v}) = \max_{r\in R^{C},\boldsymbol {v}\in r}s(r) \quad (20)\]  

For each value \(\boldsymbol{v}\) we observe that \(m(C,\boldsymbol {v}) - L_{V}(\boldsymbol {v}) = k - 1\) if and only if there exists some tuple \(r\in R^{C}\) with \(\boldsymbol {v}\in r\) such that for all \(\boldsymbol{v}^{\prime}\in r,\boldsymbol{v}^{\prime}\neq \boldsymbol{v}\) we already have \(\boldsymbol{v}^{\prime}\in \alpha\) . This is equivalent to our original definition of \(L_{E}\) and we can use this case distinction to obtain the edge labels:  

\[L_{E}(C,\boldsymbol {v}) = \left\{ \begin{array}{ll}1 & \mathrm{if} m(C,\boldsymbol {v}) - L_{V}(\boldsymbol {v}) + 1 = k,\\ 0 & \mathrm{otherwise}. \end{array} \right. \quad (21)\]  

Our implementation maintains edge lists which connect values and constraint edges to their respective tuples across all constraints. With these edge lists, Equations 19 and 20 are simply scatter operations and Equation 21 is carried out with standard torch arithmetic. These functions are fully based on the GPU and allow us to rapidly update all edge labels in parallel. We can optimize this further by allowing relations to be specified in terms of the disallowed tuples (conflicts), rather than the allowed ones. In this case, we can carry out the exact same procedure, except that we swap the labels in the case distinction of Equation 21. This optimization is very