eral CSP benchmark instances. We demonstrate that our method achieves a substantial increase in performance over prior GNN approaches and can compete with conventional algorithms. ANYCSP models trained on small random graph coloring problems are on par with state- of- the- art coloring heuristics on structured benchmark instances. On MAX- \(k\) - SAT, our method scales to test instances 100 times larger than the training data, where it finds better assignments than state- of- the- art conventional search heuristics despite performing 1000 times fewer search iterations.  

## 2 Related Work  

For a comprehensive overview on applying GNNs to combinatorial problems, we refer to Cappart et al. (2021). In this paper, we are primarily interested in end- 2- end approaches which seek to directly predict approximate solutions for combinatorial problems with trainable neural networks. Early work in this area was done by Bello et al. (2016), who learned TSP heuristics with Pointer Networks (Vinyals, Fortunato, and Jaitly 2015) and policy gradient descent. Several extensions of these ideas have since been proposed based on attention (Kool, van Hoof, and Welling 2018) and GNNs (Joshi et al. 2020). Khalil et al. (2017) propose a general method for graph problems, such as MAXCUT or Minimum Vertex Cover. They model the expansion of partial solutions as a reinforcement learning task and train a GNN with Q- learning to iteratively construct approximate solutions.  

A related group of approaches models local modifications to complete solutions as actions of a reinforcement learning problem. A GNN is then trained as a local search heuristic that iteratively improves candidate solutions through local changes. Methods following this concept are RLSAT (Yolcu and Póczos 2019) for SAT, ECO- DQN (Barrett et al. 2020) for MAXCUT, LS- DQN (Yao, Cai, and Wang 2021) for graph partitioning problems and TSP as well as BiHyb (Wang et al. 2021) for graph problems based on selecting and modifying edges. Like conventional search heuristics, these architectures can be applied for any number of search iterations to refine the solution. A shared drawback on large instances is the relatively high computational cost of GNNs, which slows down the search substantially when compared to classical algorithms. ECORD (Barrett, Parsonson, and Laterre 2022) addresses this issue for MAXCUT by applying a GNN only once before the local search, which is carried out by a faster GRU- based architecture without costly message passes. We address the same problem, but not by iterating faster, but by allowing global modifications in each iteration.  

A fundamentally different approach considers soft relaxations of the underlying problems which can optimized directly with SGD. Examples of this concept are PDP (Amizadeh, Matusevych, and Weimer 2019) for SAT and RUNCSP (Tönshoff et al. 2021) for all binary CSPs with fixed constraint language. These architectures can predict completely new solutions in each iteration but the relaxed differentiable objectives used for training typically do not capture the full hardness of the discrete problem.  

## 3 Preliminaries  

A CSP instance is a triple \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) , where \(\mathcal{X}\) is a finite set of variables, \(\mathcal{D}\) assigns to each variable \(X\in \mathcal{X}\) a finite set \(\mathcal{D}(X)\) , the domain of \(X\) , and \(\mathcal{C}\) is a set of constraints \(C = (s^C,R^C)\) , where for some \(k\geq 1\) , the scope \(s^C = (X_1,\ldots ,X_k)\in \mathcal{X}^k\) is a tuple of variables and \(R^C\subseteq \mathcal{D}(X_1)\times \ldots \times \mathcal{D}(X_k)\) is a \(k\) - ary relation over the corresponding domains. We always assume that the variables in the scope \(s^C\) of a constraint \(C\) are mutually distinct; we can easily transform an instance not satisfying this condition into one that does by adapting the relation \(R^C\) accordingly.  

Slightly abusing terminology, we call a pair \((X,d)\) where \(X\in \mathcal{X}\) and \(d\in \mathcal{D}(X)\) a value for variable \(X\) . For all \(X\in \mathcal{X}\) we let \(\mathcal{V}_X = \{X\} \times \mathcal{D}(X)\) be the set of all values for \(X\) , and we let \(\mathcal{V} = \bigcup_{X\in \mathcal{X}}\mathcal{V}_X\) be the set of all values. We usually denote values by \(\mathcal{U}\) . Working with these values instead of domain elements is convenient because the sets \(\mathcal{V}_X\) , for \(X\in \mathcal{X}\) , are mutually disjoint, whereas the domains \(\mathcal{D}(X)\) are not necessarily.  

An assignment for a CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) is a mapping \(\alpha\) that assigns a domain element \(\alpha (X)\in \mathcal{D}(X)\) to each variable \(X\) . Alternatively, we may view an assignment as a subset \(\alpha \subseteq \mathcal{V}\) that contains exactly one value from each \(\mathcal{V}_X\) . Depending on the context, we use either view, and we synonymously write \(\alpha (X) = d\) or \((X,d)\in \alpha\) . An assignment \(\alpha\) satisfies a constraint \(C = ((X_1,\ldots ,X_k),R)\) (we write \(\alpha \models C\) ) if \((\alpha (X_1),\ldots ,\alpha (X_k))\in R\) , and \(\alpha\) satisfies \(\mathcal{I}\) , or is a solution to \(\mathcal{I}\) , if it satisfies all constraints in \(\mathcal{C}\) . The objective of a CSP is to decide if a given instance has a satisfying assignment and to find one if it does. To distinguish this problem from the maximization version introduced below, we sometimes speak of the decision version. Specific CSPs such as Boolean satisfiability or graph coloring problems are obtained by restricting the instances considered.  

We define the quality \(Q_{\mathcal{I}}(\alpha)\) of an assignment \(\alpha\) to be the fraction of constraints in \(\mathcal{C}\) satisfied by \(\alpha\) : \(Q_{\mathcal{I}}(\alpha) = |\{C|C\in \mathcal{C},\alpha \models C\} | / |\mathcal{C}|\) . An assignment \(\alpha\) is optimal if it maximizes \(Q_{\mathcal{I}}(\alpha)\) for the instance \(\mathcal{I}\) . The goal of the maximisation problem MAXCSP is to find an optimal assignment for a given instance.  

A soft assignment for a CSP instance \(\mathcal{I}\) is a mapping \(\phi :\mathcal{V}\to [0,1]\) such that \(\sum_{\nu \in \mathcal{V}_X}\phi (\nu) = 1\) for all \(X\in \mathcal{X}\) . We interpret the numbers \(\phi (\nu)\) as probabilities and say that an assignment \(\alpha\) is sampled from a soft assignment \(\phi\) (we write \(\alpha \sim \phi\) ) if for each variable \(X\in \mathcal{X}\) we independently draw a value \(\nu \in \mathcal{V}_X\) with probability \(\phi (\nu)\) .  

## 4 Method  

With every CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) we associate a tripartite graph with vertex set \(\mathcal{X}\cup \mathcal{V}\cup \mathcal{C}\) , where \(\mathcal{V}\) is the set of values defined in the previous section, and two kinds of edges: variable edges \((X,\nu)\) for all \(X\in \mathcal{X}\) and \(\nu \in \mathcal{V}_X\) , and constraint edges \((C,\nu)\) for all \(C\in \mathcal{C}\) and \(\nu \in \mathcal{V}_X\) for some \(X\) in the scope of \(C\) .  

This graph representation is more or less standard; one slightly unusual feature is that we introduce edges from constraints directly to the values and not to the variables. This will be important in the next step, where information about