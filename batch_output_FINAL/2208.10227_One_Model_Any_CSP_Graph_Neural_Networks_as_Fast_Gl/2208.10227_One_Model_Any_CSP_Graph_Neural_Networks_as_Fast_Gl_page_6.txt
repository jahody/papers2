Table 2: MAxCUT results on Gset graphs. The graphs are grouped by their vertex counts and we provide the mean deviation from the best known cut size.   

<table><tr><td>METHOD</td><td>|V|=800</td><td>|V|=1K</td><td>|V|=2K</td><td>|V|≥3K</td></tr><tr><td>GREEDY</td><td>411.44</td><td>359.11</td><td>737.00</td><td>774.25</td></tr><tr><td>SDP</td><td>245.44</td><td>229.22</td><td>-</td><td>-</td></tr><tr><td>RUNCSP</td><td>185.89</td><td>156.56</td><td>357.33</td><td>401.00</td></tr><tr><td>ECO-DQN</td><td>65.11</td><td>54.67</td><td>157.00</td><td>428.25</td></tr><tr><td>ECORD</td><td>8.67</td><td>8.78</td><td>39.22</td><td>187.75</td></tr><tr><td>ANYCSP</td><td>1.22</td><td>2.44</td><td>13.11</td><td>51.63</td></tr></table>  

the learned policy generalizes to searches that are over 10K times longer than those seen during training.  

Graph Coloring We consider the problem of finding a conflict- free vertex coloring given a graph \(G\) and number of colors \(k\) . The corresponding CSP instance has variables for each vertex, domains containing the \(k\) colors and one binary " \(\neq\) "- constraint for each edge. We train on a distribution \(\Omega_{\mathrm{COL}}\) of graph coloring instances for random graphs with 50 vertices. We mix Erdős- Rényi, Barabási- Albert and random geometric graphs in equal parts. The number of colors is chosen to be in [3, 10]. As test instances we use 100 structured benchmark graphs with known chromatic number \(\mathcal{X}(G)\) . The instances are obtained from a collection of hard coloring instances commonly used to benchmark heuristics<sup>4</sup>. They are highly structured and come from a wide range of synthetic and real problems. We divide the test graphs into two sets with 50 graphs each: \(\mathrm{COL}_{< 10}\) contains graphs with \(\mathcal{X}(G) < 10\) and \(\mathrm{COL}_{> 10}\) contains graphs with \(\mathcal{X}(G) \geq 10\) . The graphs in \(\mathrm{COL}_{> 10}\) have up to 1K vertices, 19K edges and a chromatic number of up to 73. This experiment tests generalization to larger domains and more complex structures.  

We compare the performance to three problem specific heuristics: a simple greedy algorithm, the classic heuristic DSATUR (Brélaz 1979) and the state- of- the- art heuristic HybridEA (Galinier and Hao 1999), all implemented efficiently by Lewis et al. (2012); Lewis (2015). We also evaluate the best two CSP solvers from the MODEL RB experiment. The neural baseline RUNCSP is also tested on \(\mathrm{COL}_{< 10}\) . Unlike ANYCSP, RUNCSP requires us to fix a domain size before training. Therefore, we must train one RUNCSP model for each tested chromatic number \(4 \leq \mathcal{X}(G) \leq 9\) and omit testing on \(\mathrm{COL}_{> 10}\) . We use the same training data as Tönshoff et al. (2021) for their experiments on structured coloring benchmarks.  

Table 1 provides the number of solved \(k\) - COL instances from both splits. ANYCSP is on par with HybridEA which solves the most instances of all baselines. RUNCSP solves significantly fewer instances than ANYCSP on \(\mathrm{COL}_{< 10}\) and outperforms only the simple greedy approach. ANYCSP solves 40 out of the 50 instances in \(\mathrm{COL}_{> 10}\) . The optimally colored graphs include the largest instance with 73 colors. Since ANYCSP trains with 3 to 10 colors the trained model  

Table 3: Number of solved 3-SAT benchmark instances from SATLIB. For each number of variables there are 100 satisfiable test instances.   

<table><tr><td>METHOD</td><td>SL50</td><td>SL100</td><td>SL150</td><td>SL200</td><td>SL250</td></tr><tr><td>RLSAT</td><td>100</td><td>87</td><td>67</td><td>27</td><td>12</td></tr><tr><td>PDP</td><td>93</td><td>79</td><td>72</td><td>57</td><td>61</td></tr><tr><td>WALK SAT</td><td>100</td><td>100</td><td>97</td><td>93</td><td>87</td></tr><tr><td>PROB SAT</td><td>100</td><td>100</td><td>97</td><td>87</td><td>92</td></tr><tr><td>ANYCSP</td><td>100</td><td>100</td><td>100</td><td>97</td><td>99</td></tr></table>  

is able to generalize to significantly larger domains.  

MAXCUT For MAXCUT we train on the distribution \(\Omega_{\mathrm{MCUT}}\) of random unweighted Erdős- Rényi graphs with 100 vertices and an edge probability \(p \in [0.05, 0.3]\) . Our test data is Gset (Ye 2003), a collection of commonly used MAXCUT benchmarks of varying structure with 800 to 10K vertices. We evaluate three neural baselines: RUNCSP, ECO- DQN (Barrett et al. 2020) and ECORD (Barrett, Personson, and Laterre 2022). RUNCSP is also trained on \(\Omega_{\mathrm{MCUT}}\) . We train and validate ECO- DQN and ECORD models with the same data that Barrett, Personson, and Laterre (2022) used for their Gset experiments. We omit S2V- DQN (Khalil et al. 2017) since ECO- DQN and ECORD have been shown to yield substantially better cuts. We adopt the evaluation setup of ECORD and run the neural methods with 20 parallel runs and a timeout of 180s on all unweighted instances of Gset. The results of a standard greedy construction algorithm and the well- known SDP based approximation algorithm by Goemans and Williamson (1995) are also included as classical baselines. Both are implemented by Mehta (2019). SDP runs with a 3 hour timeout for graphs with up to 1K vertices.  

Table 2 provides results for Gset. We divide the test graphs into groups by the number of vertices (8- 9 graphs per group) and report the mean deviation from the best- known cuts obtained by Benlic and Hao (2013) for each method. ANYCSP outperforms all baselines across all graph sizes by a large margin. Recall that RUNCSP trains on a soft relaxation of MAXCUT while ECO- DQN and ECORD are both neural local search approaches. Neither concept matches the results of our global search approach trained with policy gradients.  

3- SAT For 3- SAT we choose the training distribution \(\Omega_{\mathrm{3SAT}}\) as uniform random 3- SAT instances with 100 variables. The ratio of clauses to variables is drawn uniformly from the interval [4, 5]. For 3- SAT we test on commonly used benchmark instances for uniform 3- SAT from SATLIB<sup>5</sup>. The test set \(\mathrm{SLN}\) contains 100 instances with \(N \in \{50, 100, 150, 200, 250\}\) variables each. The density of these formulas is at the threshold of satisfiability. We evaluate two neural baselines: RLSAT (Yolcu and Póczos 2019) and PDP (Amizadeh, Matusevych, and Weimer 2019). PDP is also trained on \(\Omega_{\mathrm{3SAT}}\) . We train RLSAT with the curriculum learning dataset for 3- SAT provided by its authors, since its reward scheme is incompatible with our partially unsatisfi