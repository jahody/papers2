Table 4: Results on Max- \(k\) -SAT instances with 10K variables. For each \(k\in \{3,4,5\}\) we provide the mean number of unsatisfied clauses over 50 random instances.   

<table><tr><td>METHOD</td><td>3CNF</td><td>4CNF</td><td>5CNF</td></tr><tr><td>WALKSAT</td><td>2145.28</td><td>1556.68</td><td>1685.10</td></tr><tr><td>CCLS</td><td>1567.24</td><td>1323.14</td><td>1315.96</td></tr><tr><td>SATLIKE</td><td>1595.86</td><td>1188.56</td><td>1152.88</td></tr><tr><td>ANYCSP</td><td>1537.46</td><td>1126.44</td><td>1103.14</td></tr></table>  

able training instances. We also adopt the experimental setup of RLSAT, which limits the evaluation run by the number of search steps instead of a timeout. The provided code for both PDP and RLSAT is comparatively slow and a timeout would compare implementation details rather than the capability of the learned algorithms. We also evaluate two conventional local search heuristics: The classical WalkSAT algorithm (Selman et al. 1993) based on random walks and a modern probabilistic approach called probSAT (Balint and Schöning 2018). Like Yolcu and Póczos (2019), we apply stochastic boosting and run each method 10 times for 10K steps on every instance. PDP is deterministic and only applied once to each formula.  

Table 3 provides the number of solved instances for each tested size. All compared approaches do reasonably well on small instances with 50 variables. However, the performance of the two neural baselines drops significantly as the number of variables increases. ANYCSP does not suffer from this issue and even outperforms the classical local search algorithms on the three largest instance sizes considered here.  

MAX- \(k\) - SAT We train on the distribution \(\Omega_{\mathrm{MSAT}}\) of uniform random MAX- \(k\) - SAT instances with 100 variables and \(k\in \{3,4\}\) . Here, the clause/variable ratio is chosen from [5, 8] and [10, 16] for \(k = 3\) and \(k = 4\) , respectively. These formulas are denser than those of \(\Omega_{3\mathrm{SAT}}\) since we aim to train for the maximization task. Our test data for MAX- \(k\) - SAT consists of uniform random \(k\) - CNF formulas generated by us. For each \(k\in \{3,4,5\}\) we generate 50 instances with 10K variables each. The number of clauses is chosen as 75K for \(k = 3\) , 150K for \(k = 4\) and 300K for \(k = 5\) . These formulas are therefore 100 times larger than the training data and aim to test the generalization to significantly larger instances as well as unseen arities, since \(k = 5\) is not used for training. Neural baselines for SAT focus primarily on decision problems. For MAX- \(k\) - SAT we therefore compare ANYCSP only to conventional search heuristics: the classical (Max- )WalkSAT (Selman et al. 1993) and two state- of- the- art MAX- SAT local search heuristics CCLS (Luo et al. 2015) and SATLike (Cai and Lei 2020). Table 4 provides a comparison. We provide the mean number of unsatisfied clauses after processing each instance with a 20 Minute timeout. Remarkably, ANYCSP outperforms all classical baselines by a significant margin.  

We point out that the conventional search heuristics all perform over 100M search steps in the 20 Minute timeout. ANYCSP performs less than 100K steps on each instance in this experiment. The GNN cannot match the speed with  

<center>Figure 4: Detailed results for MAX-5-SAT. For each test instance and each method we plot the number of unsatisfied clauses in the best found solution against the search step in which it was found. </center>  

which classical algorithms iterate, even though it is accelerated by a GPU. Despite this, ANYCSP consistently finds the best solutions. Figure 4 evaluates this surprising observation further. We plot the number of unsatisfied clauses in the best found solution against the search step in which the solution was found (Steps to Opt.) for all methods and all instances of our MAX- 5- SAT test data. We also provide the results of a modified ANYCSP version (ANYCSP Local defined in Appendix C) that is only allowed to change one variable at a time and is therefore a local search heuristic. Note that the \(x\) - axis is logarithmic as there is a clear dichotomy separating neural and classical approaches: Compared to conventional heuristics ANYCSP performs roughly three orders of magnitude fewer search steps in the same amount of time. When restricted to local search, ANYCSP is unable to overcome this deficit and yields worse results than strong heuristics such as SATLike. However, when ANYCSP leverages global search to parallelize refinements across the whole instance it can find solutions in 100K steps that elude state- of- the- art local search heuristics after well over 100M iterations.  

## 6 Conclusion  

We have introduced ANYCSP, a novel method for neural combinatorial optimization to learn heuristics for any CSP through a purely data- driven process. Our experiments demonstrate how the generic architecture of our method can learn effective search algorithms for a wide range of problems. We also observe that standard policy gradient descent methods like REINFORCE are capable of learning on an exponentially sized action space to obtain global search heuristics for NP- hard problems. This is a critical advantage when processing large problem instances.  

Directions for future work include widening the scope of the architecture even further: Weighted and partial CSPs are a natural extension of the CSP formalism and could be incorporated through node features and adjustments to the reward scheme. Variables with real- valued domains may be