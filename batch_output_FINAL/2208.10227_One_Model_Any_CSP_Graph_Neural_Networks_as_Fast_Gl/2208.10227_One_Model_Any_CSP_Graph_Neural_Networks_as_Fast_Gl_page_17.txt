in the XCSP3 format. As for ANYCSP, we model the decision variant of graph coloring and fix the known chromatic number of each graph as the domain size. The solvers then have to find a satisfiable assignment for these instances within the 20- minute timeout.  

For HybridEA we optimize the recombination strategy and number of tabu search steps. We choose "nPoint" for recombination and a factor of 64 for the number of tabu search steps per cycle. The greedy algorithm and DSATUR each run in their default configuration.  

Table 8 and 9 contain detailed instance- level results of all compared methods for \(\mathrm{COL}_{< 10}\) and \(\mathrm{COL}_{\geq 10}\) , respectively.  

## B.3 MAXCUT  

In our MAXCUT experiment, we only consider instances with positive edge weights. In this case, MAXCUT is identical to maximum 2- Colorability. Let \(G = (V,E)\) be an input graph. We can model the MAXCUT problem for \(G\) as a CSP instance by using the same reduction we use for vertex coloring in Section B.2 but with the number of colors fixed at \(k = 2\) .  

Data Our training distribution \(\Omega_{\mathrm{MCUT}}\) consists if Erdős- Rényi graphs with \(n = 100\) vertices and an edge probability sampled uniformly from \(p\in [0.05,0.3]\) . Our validation data uses the same distribution for \(p\) but with \(n = 500\) vertices.  

Baselines Our classical baselines are a constructive greedy algorithm and a well- known approximation algorithm based on SDP (Goemans and Williamson 1995). For both, we use the implementation of (Mehta 2019) and we run SDP with a 3- hour timeout. We were unable to obtain results for SDP for graphs with over 1000 vertices within this timeout.  

We consider three neural approaches as baselines: RUNCSP (Tönshoff et al. 2021), ECORD (Barrett et al. 2020) and ECO- DQN (Barrett, Parsonson, and Laterre 2022). RUNCSP is also trained on \(\Omega_{\mathrm{MCUT}}\) . We train ECO- DQN and ECORD on the same data distributions used by Barrett, Parsonson, and Laterre (2022) in their Gset experiments. More specifically, ECO- DQN and ECORD train on Erdős- Rényi graphs with 200 and 500 vertices, respectively. Both methods were validated with the "ER500" validation dataset from their own experiments, which contains Erdős- Rényi graphs with 500 vertices. Note that we select a single model for each method. Barrett, Parsonson, and Laterre (2022) suggest selecting different models with different validation datasets modeled after each group of graphs in the Gset test data. We do not adopt this procedure and select a single model using Erdős- Rényi graphs for validation. The goal of our setup is to test the generalization of one model to new sizes and structures not seen during training and validation.  

We also experimented with training and validating ECO- DQN and ECORD on our data distributions but found the data chosen by Barrett, Parsonson, and Laterre (2022) to be better for their methods. Table 7 provides extended results of the MAXCUT experiment across all used Gset graphs.  

## B.4 3-SAT  

Modeling a Boolean CNF formula \(f\) as a CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{E})\) is straightforward. The set of variables \(\mathcal{X}\) in \(\mathcal{I}\) is  

simply the set of variables in \(f\) and all domains are given by \(\mathcal{D}(X) = \{0,1\}\) . For each clause \(c\) in \(f\) we add one constraint \(C\) to \(\mathcal{I}\) with the same scope of variables as \(c\) and the relation \(R^{C} = \{0,1\}^{k}\backslash \{t_{c}\}\) . Here, \(k\) is the arity of \(c\) and \(t_{c}\in \{0,1\}^{k}\) is the one combination of values that does not satisfy \(c\) .  

Data We train on the distribution \(\Omega_{\mathrm{3SAT}}\) of random uniform 3SAT instances with 100 variables and a clause/variable ratio sampled uniformly between 4 and 5. This density is roughly where the threshold of satisfiability for random 3- SAT is. Many of these instances are unsatisfiable. For validation, we use formulas with 200 variables and the same density distribution.  

Baselines PDP is also trained on \(\Omega_{\mathrm{3SAT}}\) . We used the default PDP configuration for all other options. RLSAT can not be trained on our distribution, since its reward expects all training instances to be satisfiable. Furthermore, their training procedure is relatively sensitive and requires a Curriculum Learning to train well. We, therefore, use the curriculum for 3- SAT provided by the authors of RLSAT. Training is performed on a sequence of data sets with increasing variable counts 5, 10, 25, 50 and finally 100. This training data is generated by their generators.  

We also evaluate the conventional algorithms WalkSAT and probSAT. For WalkSAT we tuned the "walk probability" and "noise" parameters but found the default configuration to perform best. We do run probSAT in its default configuration as well since it has internal heuristics that choose parameters based on the arity of the instance.  

We run all methods for 10K steps on each instance. We adopt the experimental setting from RLSAT and run each method 10 times on every instance and take the best attempt as the output. PDP is deterministic and we run it only once.  

## B.5 MAX-k-SAT  

For MAX- \(k\) - SAT we can use the same reduction from Boolean CNF formulas to CSPs that we use for 3- SAT. The considered formulas are simply denser.  

Data Our training distribution \(\Omega_{\mathrm{MSAT}}\) contains \(k\) - CNF formulas with 100 variables and an arity of \(k\in \{3,4\}\) . The clause/variable ratio is sampled uniformly from [5, 8] and [10, 16] for \(k = 3\) and \(k = 4\) , respectively. We validate on formulas with 200 variables and identical density distribution. No 5- CNF formulas are used for training and model selection.  

Baselines In this experiment, we use MaxWalkSAT, a version of the classical WalkSAT algorithm optimized for MAXSAT. The noise parameter is tuned on our validation data to a value of \(10^{- 3}\) . The implementations of CCLS and SATLike do not have command line options and run in their default configurations.  

## B.6 Cross-Comparison of Trained Models  

Recall that every ANYCSP model can take any CSP instance as input. We train on specific distributions of CSPs to obtain problem specific heuristics. However, a model trained on graph coloring instances can still process 3- SAT formulas