Table 6: Cross-Comparison of training distributions on different test datasets.   

<table><tr><td>Ω</td><td>RB50</td><td>COL&amp;lt;10</td><td>Gset800</td><td>SL250</td><td>MAX-5-CNF</td></tr><tr><td>ΩRB</td><td>42</td><td>50</td><td>655.56</td><td>98</td><td>6192.18</td></tr><tr><td>ΩCOL</td><td>15</td><td>50</td><td>868.22</td><td>96</td><td>5076.16</td></tr><tr><td>ΩMCUT</td><td>0</td><td>0</td><td>1.22</td><td>0</td><td>9048.64</td></tr><tr><td>ΩSAT</td><td>0</td><td>19</td><td>1213.11</td><td>99</td><td>5001.72</td></tr><tr><td>ΩMSAT</td><td>0</td><td>15</td><td>1217.67</td><td>66</td><td>1103.14</td></tr></table>  

and vice versa. Naturally, we expect each model to perform best on the distribution it is trained on, but the universality of our architecture does raise interesting questions of how well models trained on one CSP perform on an entirely different CSP. In this section we aim to study this transferability of learned heuristics across different CSPs.  

Table 6 compares all models from our main experiments on each other's test data. For each test dataset, we use the same evaluation metric as the original experiments. First of all, each model does indeed achieve the best results on the test data of the CSP used for training. However, the degree to which each model generalizes to other problems varies substantially. There seems to be a significant compatibility between MODEL RB and graph coloring. The model trained on MODEL RB instances solves all coloring instances in \(\mathrm{COL}_{< 10}\) while the model trained on graph coloring problems solves 15 MODEL RB benchmarks instances. Both of these models also perform very well on decision 3- SAT instances. Curiously, they even outperform the model trained for MAX- \(k\) - SAT on decision 3- SAT. The training distributions \(\Omega_{3\mathrm{SAT}}\) and \(\Omega_{\mathrm{MSAT}}\) are closely related and one would expected our MAX- \(k\) - SAT model to do well on 3- SAT as well. While it does solve 66 of the 100 instances, it does not come close to the 96 and 98 instances solved by the policies trained on \(\Omega_{\mathrm{COL}}\) and \(\Omega_{\mathrm{RB}}\) , respectively. This surprising observation can most likely be attributed to the choice of the aggregation function. Like the model trained on \(\Omega_{3\mathrm{SAT}}\) , the MODEL RB and graph coloring policies both use MAX- aggregation. The MAX- \(k\) - SAT policy uses MEAN- aggregation. In this comparison, choosing the right aggregation function appears to be at least as important as training on similar data. A similar observation also holds for the MAXCUT model. It performs as well as random guessing on all problems other than MAX- CUT. Note that this model uses SUM- aggregation since this performed best in validation for the MAXCUT problem. However, SUM- aggregation is not very robust towards changes in the distribution of inputs. The learned functions are not able to handle larger domains, arities or degrees than those seen during training and the policy is highly specialized towards MAXCUT. On the other hand, no other model achieves competitive results on MAXCUT, indicating that this problem may require a higher degree of specialization.  

Overall, we can conclude that some of our trained models generalize well beyond their training distribution to entirely different CSPs. However, this is strongly dependent on the specific instance distributions and the robustness of the used aggregation functions.