ture iterations. The CDCL algorithm concludes with nonchronological backtracking/backjumping, which does not merely revert the most recent decision but backjumps to a point preceding the conflict, result in assigning a lower place in the decision- making queue or a lower decision- score to these variables involved in conflict. This allows a more rapid exploration of different solution spaces and aids in the avoidance of past mistakes. Two critical stages of this process are conflict analysis (clause learning) and variable decision, which collaboratively construct the decision queue for variable branching. Ideally, a swift identification of variables within the UNSAT- core would expedite the CDCL solver's resolution of UNSAT problems. Conversely, prematurely including variables outside UNSAT- core within the decision queue could slow down the solver. Thus we should try to guide the decision queue to optimize the performance. Specifically, we would instruct the decision queue and decision score, the internal variable used within variable decision and conflict analysis.  

## Queue & Score Based Branching  

Against above backdrop, we propose a novel methodology for interaction with the CDCL solver. Before attempting to solve a SAT problem, our trained network undertakes an inferential analysis of the problem, thereby calculating the probability of each variable being within the UNSAT- core. These probabilities guide the solver in establishing a queue as well as a set of scores for solver branch decisions. Probabilities output are first used as the initialization for decision scores. Then they are employed to rank variables in descending queue, which is adopted as the initial value for decision queue. As predicted decision queue and decision score are based on the probability of variables within UNSAT- core, solver can deal with those variables that highly related to UNSAT- core first to address the unsatisfiability.  

These decision values and decision queue, now based on our neural network inference, are imported as the solver's initial values. The solver then can commence its operations on problem. This importing process doesn't undermine the solver's capacity to comprehensively address UNSAT problems, as the basic algorithm in place still centers around identifying the UNSAT- core.  

The added advantage here is that our inference performs only once for each problem, thus keeping the computational burden and associated time minimal. The processing time of the whole pipeline of IB- Net (including graph formulation, inference and solver initialization) can be controled within 1.5 second, which is regarded as a small cost comparing to the solving time. It's worth noting that our neural network is efficient enough to run on CPUs. Consequently, our method maintains the solver's completeness while significantly enhancing its efficiency and speed.  

## 4 Experimental Settings  

### 4.1 Datasets  

We utilize two datasets to train and evaluate our framework and previous approaches. The first one is the industrial LEC circuit SAT instances from real- life chip design in 2023. They  

Table 1: Detail statistic of LEC circuit dataset and SAT Competition dataset. Noted that over \(99\%\) of instance in LEC dataset is result in UNSAT.   

<table><tr><td>Data</td><td>#CNF</td><td>Avg#Var</td><td>Avg#Clause</td></tr><tr><td rowspan="4">LEC</td><td>Circuit 1</td><td>22,050</td><td>1,002</td></tr><tr><td>Circuit 2</td><td>14,840</td><td>952</td></tr><tr><td>Circuit 3</td><td>25,591</td><td>1,563</td></tr><tr><td>All</td><td>62,481</td><td>1,220</td></tr><tr><td rowspan="4">SAT Comp</td><td>SAT</td><td>1,115</td><td>3,065</td></tr><tr><td>UNSAT</td><td>985</td><td>3,784</td></tr><tr><td>Halted</td><td>1,171</td><td>3,623</td></tr><tr><td>All</td><td>3,271</td><td>3,481</td></tr></table>  

are extracted from the real chip development process to reflect the need for LEC process. To fit the data into GPU easily and model the real- life production constraint, we first run state- of- the- art SAT solver (Kissat [Biere and Fleury, 2022]) on these problems and filter the instance that solver can solve within 1,000 seconds (the instances that solver cannot solve within 1,000 seconds will be regarded as hard cases and sent to redesign in real- life). Apart from the industrial dataset, we also prepare the SAT Competition dataset as previous works are mostly target open datasets. We adopt the anniversary track of SAT Competition in 2022 [Balyo et al., 2022]. The anniversary track is comprised of all benchmark instances used in previous SAT Competitions to ensure the coverage of problems. We also filter the anniversary track dataset in the same way with LEC dataset. The details of filtered datasets can be found in Table 1. Halted here means instances unsolved within given time. After preparing the datasets, we use Kissat and Drat- trim [Wetzler et al., 2014] to find variable assignments for SAT problems and UNSAT- core for UNSAT problems, which serves as the supervision of our model training. When Kissat try to solve a UNSAT instance, it will produce a proof for the unsatisfiability, which will be taken by Drat- trim to produce the UNSAT- core variable. Though the UNSAT- core found by Drat- trim is not the minimal core, it still helps in solving UNSAT instances, so we will take these UNSAT- core as all the UNSAT- core refered in following sections. The supervision production just cost the same time of solving given dataset by Kissat, which can consider as easy to follow and efficient to produce.  

### 4.2 Experimental Setup  

Our experiment design commences by segregating the two previously mentioned datasets into distinct training and testing subsets, meticulously avoiding any overlaps. We allocate \(80\%\) of the data to training, reserving the remaining \(20\%\) for testing. We benchmark against several baselines: (1) The NeuroSAT [Selsam et al., 2019] model based on GNN and Literal- clause graph. We follow the official implementation and used the score they set for variable as our proposed probality. (2) Modified version of NeuroSAT: NeuroCore [Selsam and Bj√∏rner, 2019], that target UNSAT problem. We follow the official implementation and interaction periodically with solvers. (3) NLocalSAT [Zhang et al., 2021] is based on NeuroSAT and interacts with SLS solver.