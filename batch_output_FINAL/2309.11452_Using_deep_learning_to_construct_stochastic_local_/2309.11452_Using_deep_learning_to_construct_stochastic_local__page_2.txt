<center>Figure 1: Illustration of the general idea of this work. Left: A simple SLS solver finds a solution to a SAT instance by repeatedly and randomly updating a small subset of the variables. Middle: An oracle-based SLS solver uses samples from an oracle \(O\) that is provided as part of the input to update the variables at each iteration. Right: We use a deep learning model to train an oracle factory \(F_{\theta}\) that maps an incoming instance to an oracle which is then fed into an oracle-based. This approach is motivated by results that provide sufficient conditions for an oracle-based SLS solver to find a solution efficiently, based on properties of the oracle. </center>  

The main motivation for the approach presented in this paper is a series of breakthrough results from theoretical computer science that imply sufficient conditions for an SLS solver with access to a suitable oracle to efficiently find a solution to a SAT instance from Refs. [12, 13, 14, 15].  

### 1.1 Contributions  

This work provides several contributions:  

- We establish the promise of application-specific solvers with performance guarantees, by connecting "hands-on" research from machine learning with theoretical results from computer science.- We introduce a new, theoretically motivated loss function, which we call the Lovász Local loss, which rewards an oracle's exploitation of the local structure of SAT instances. We are not aware of any work in the field whose loss is motivated by known bounds on the resulting solver performance.- We construct and train two oracle-based SLS solvers using an Interaction network [16].- We empirically investigate the ability of these solvers to solve random 3-SAT at varying levels of difficulty (as measured by the common \(\alpha\) -ratio between clauses and variables). Our experiments show significant boosts in performance, with the ML-based solvers solving instances that have, on average, a \(17\%\) higher value of \(\alpha\) and doing so in \(35\%\) fewer steps, with a larger than \(8x\) improvement in the median number of steps.- We show that, in our experiments, providing continuous access to an oracle produces significantly better results (both in number of steps needed and number of instances solved) while an algorithm that only uses the oracle to initialise a candidate, only solves instances in fewer steps and is not able to solve more instances.  

### 1.2 Related Work  

#### 1.2.1 Deep learning based SAT solving  

Ref. [3] provides a recent review paper on machine learning- based SAT solvers. Focussing here on the literature on SLS solvers, in Ref. [11] the authors train a Graph Neural Network to generate an initial candidate for various SLS solvers. However, they don't use this GNN as an oracle in the course of running these solvers. Ref. [10] uses a GNN to learn a variable selection heuristic in the course of a WalkSAT type SLS solver. However, their model is based on a learned policy and hence doesn't act as an oracle in the sense of this work. The cross- entropy loss with respect to a Gibbs distribution that we use in this work is also used in Ref. [17] in the context of Mixed Integer Program solving.  

#### 1.2.2 SLS algorithms with solution guarantees  

As described above, one core contribution of this work is to choose a solver and loss function for training such that we are guaranteed by theoretical results that a smaller loss will lead to better performance. The body of work we have in mind here is that around the seminal Lovász Local Lemma (LLL) [18]. In particular, Ref. [12, 13] proved the results that underlie our Proposition 2.1. Ref. [19] explicitly introduces the notion of an oracle- based SLS algorithm, even though their resampling oracles are special cases of the oracles we consider here. Ref. [14] proves the results stated below as Proposition 2.2 and that we consider as crucial for motivating our work in settings where an oracle does not satisfy the conditions of the LLL.