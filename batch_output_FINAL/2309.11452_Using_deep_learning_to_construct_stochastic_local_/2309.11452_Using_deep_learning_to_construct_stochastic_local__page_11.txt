## 5 Conclusion and future work  

In this work, we have presented an approach to use deep learning to construct SLS- based SAT solvers. The main contribution is that we design the algorithms and loss functions in such a way that the training process is guaranteed, by theoretical results, to, in expectation, produce a stronger solver. The hope is that with this approach we can create solvers for SAT and constraint optimization more generally, that are highly performant on for specific applications, while also being provably robust in terms of their average performance.  

We have empirically investigated this promise on a random 3- SAT dataset, with promising results. However, the practical potential of our approach remains largely unchartered, as more thorough experiments on larger and application- oriented datasets, more research on suitable deep learning architectures, as well as the application to more mature and sophisticated SLS solvers, needs to be carried out, since the resulting solvers are currently far from being competitive with state of the art solvers.  

We leave these experiments for future research and hope that the preliminary results presented in this paper nevertheless establish the promise of the idea.  

## Acknowledgments  

P.B. thanks Mahdi Manesh for stimulating discussions. Both authors thank Jens Eisert for introducing them to the results of the Moser- Tardos algorithm and related works, which instilled the idea for this work. They thank Porsche Digital GmbH for the possibility to work on this research project. M.K. thanks the BMBF (Hybrid) and the BMWK (EniQmA) for their support.  

## References  

[1] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. Association for Computing Machinery. [2] L. A. Levin. Problems of information transmission. 1973. [3] Wenxuan Guo, Junchi Yan, Hui- Ling Zhen, Xijun Li, Mingxuan Yuan, and Yaohui Jin. Machine learning methods in solving the boolean satisfiability problem, 2022. [4] Benedikt Bunz and Matthew Lamm. Graph neural networks and boolean satisfiability, 2017. [5] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a sat solver from single- bit supervision, 2019. [6] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, jul 2022. [7] Daniel Selsam and Nikolaj Bjorner. Guiding high- performance sat solvers with unsat- core predictions, 2019. [8] Sebastian Jaszczur, Michal Łuszczyk, and Henryk Michalewski. Neural heuristics for sat solving, 2020. [9] Jesse Michael Han. Enhancing sat solvers with glue variable predictions, 2020. [10] Emre Yolcu and Barnabas Poczos. Learning local search heuristics for boolean satisfiability. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [11] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. NLocalSAT: Boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, jul 2020. [12] Robin A. Moser. A constructive proof of the lovasz local lemma, 2008. [13] Robin A. Moser and Gábor Tardos. A constructive proof of the general lovasz local lemma, 2009. [14] David G. Harris and Aravind Srinivasan. Algorithmic and enumerative aspects of the moser- tardos distribution, 2017. [15] Dimitris Achlioptas, Fotis Iliopoulos, and Alistair Sinclair. Beyond the lovasz local lemma: Point to set correlations and their algorithmic applications, 2020.