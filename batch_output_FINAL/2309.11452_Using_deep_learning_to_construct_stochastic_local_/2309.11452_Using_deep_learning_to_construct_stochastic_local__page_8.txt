<center>Figure 3: Simple illustration of the “hardness regimes” for 3-SAT: For \(\alpha < 0.55\) the uniform MT algorithm is guaranteed to solve instances efficiently (green), while in practice, the uniform algorithm is found to solve instances efficiently up until around 2.5 to 2.7, but not guaranteed to do so (yellow) [28]. Our experiments indicate that the oracle-boosted MT algorithm solves instances until around 3 to 3.2 efficiently (blue). Finally, it is an open question whether this cutoff can be increased to 4.27 using better oracle factories (orange), at which point a phase transition occurs and instances are typically unsatisfiable (red) [29]. </center>  

<table><tr><td>Variant</td><td>#</td><td>M1/2(#)</td><td>%</td><td>α</td></tr><tr><td>Uniform MT</td><td>36e3</td><td>1163</td><td>69.1</td><td>1.50</td></tr><tr><td>Hybrid MT</td><td>36e3</td><td>801</td><td>68.6</td><td>1.49</td></tr><tr><td>Boosted MT</td><td>22e3</td><td>134</td><td>80.3</td><td>1.90</td></tr><tr><td>Uniform WalkSAT</td><td>23e3</td><td>462</td><td>81.2</td><td>1.96</td></tr><tr><td>Hybrid WalkSAT</td><td>23e3</td><td>161</td><td>82.2</td><td>1.96</td></tr><tr><td>Boosted WalkSAT</td><td>16e3</td><td>93</td><td>87.0</td><td>2.12</td></tr></table>  

Table 1: Results from our experiments. The best performing algorithm variant is highlighted. \(\#\) is the average number of steps before finding a solution, across all instances and runs and \(M_{1 / 2}(\#)\) is the outer median (across instances) over the inner median (across runs) of steps before finding a solution. Furthermore, we report the total fraction \(\%\) of solved instances and the average value \(\bar{\alpha}\) of \(\alpha\) for which an algorithm was able to find a solution.  

We trained our model on a dataset of 396 satisfiable instances equally distributed between \(n \in [100, 200, 300]\) and \(1 \leq \alpha \leq 4.82\) . We generated these instances with the python library CNFgen (see Ref. [30]) and post- selected on satisfiable instances. For generating the solutions, we used the Glucose3 solver [31] implemented in the Pysat library in Ref. [32]. As it turns out, this number of training instances is already enough for our model to learn the important characteristics of these SAT- instances. For the Interaction network, we used a number of \(l = 5\) message passing steps and 2 layers of dimension \(d = 200\) for every fully connected layer, in every message passing step. As hyperparameters for the loss, we used \(z = 2\) , \(\beta = 1\) , \(\gamma_{1} = 1\) , \(\gamma_{2} = 1\) . For the optimization, we used the ADAM- optimizer with a learning rate of \(10^{- 3}\) , a batchsize of 1, and trained for 50 epochs.  

While experimenting with hyperparameters, we have learned that the model leads to the best results when using a batchsize of 1. We have not experimented too much with changing the learning rate. However, we have included in our code the possibility to use a dynamic learning rate with an exponential decay since we have seen that this was beneficial for overfitting highly structured small scale problems. We are convinced that by tuning the hyperparameters one can further boost the performance of the pre- trained model. It seems to be the case that the learning rate has to be adapted when using another dataset.  

To evaluate the resulting model, we ran the oracle- based WalkSAT and MT algorithms on an evaluation set of 2052 instances, also containing an equal number of instances across \(n \in [100, 200, 300]\) and \(1 \leq \alpha \leq 4.82\) . We ran each algorithm for up to \(10^{6}\) steps and a total of 5 runs per test instance. Following Refs. [10, 33], we evaluated each algorithm using three metrics: i) the average number of steps \(\#\) before finding a solution, across all instances and runs, ii) the outer median (across instances) over the inner median (across runs) of steps before finding a solution, \(M_{1 / 2}(\#)\) , iii) the total fraction \(\%\) of solved instances, where we considered an instance solved if any of the runs had returned by the time of the cutoff. As an additional metric, we measure the average value \(\bar{\alpha}\) of \(\alpha\) for which an algorithm was able to find a solution.  

We implemented our experiments in Python, using the JAX- framework [34] and its graph extension JRAPH [35] for the GNN and optimization. We implemented the oracle- based SLS algorithms in Rust. We ran the experiments on an AWS g4dn.4xlarge instance, using a Deep Learning AMI for Ubuntu 20.04 (ami- 094950f08c57b4f62). All code and datasets used are made available as part of this publication in a GitHub repository.