more important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted MaxSAT problem.  

The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the HyperSAT network. Its form is given by  

\[\mathcal{L}_{\mathrm{shared}} = \left\| \pmb{L}_{+}^{(T - 1)} + \pmb{L}_{-}^{(T - 1)}\right\|_{F}^{2}, \quad (10)\]  

where \(\left\| \cdot \right\|_{F}\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space.  

It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted MaxSAT problem. On the one hand, a Weighted MaxSAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted MaxSAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances.  

### 3.3. Probabilistic Mapping  

The neural network module takes the hypergraph as input and produces a probability vector \(\mathbf{Y}\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \(\mathbf{Y}\) into a Boolean assignment, we transform \(\mathbf{Y}\) into \(n\) Bernoulli distributions, where the \(i\) - th distribution \(B(y_{i})\) is over the discrete values \(\{0,1\}\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions.  

## 4. Experiment  

### 4.1. Experimental Settings  

We compare the performance of HyperSAT against GNN- based methods for solving Weighted MaxSAT problems.  

Baseline Algorithms. We compare our proposed HyperSAT against GNN- based methods. The following algorithms are considered as baselines: (i) HypOp (Heydaribeni et al., 2024): an advanced unsupervised learning framework  

that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted MaxSAT problems.  

Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St√ºtzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted MaxSAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \([1,10]\) uniformly at random.  

Table 1. The parameters of the datasets.   

<table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table>  

Model Settings. HyperGCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \(7 \times 10^{- 2}\) . The balancing hyperparameter of loss function \(\lambda\) is \(2 \times 10^{- 3}\) . The FFN consists of two linear transformations and a ReLU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \(10^{- 4}\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained.  

Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and PyTorch 1.13.0.