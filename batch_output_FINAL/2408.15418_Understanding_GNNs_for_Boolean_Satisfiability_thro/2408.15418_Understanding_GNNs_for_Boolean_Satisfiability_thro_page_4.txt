allows one to encode complex relations and sparsity structures. GNNs then allow to encode inductive biases such as invariance to various transformations [4]. For these reasons, GNNs are frequently used in applications of machine learning to combinatorial optimization [6, 11, 19] where optimization problems are often amenable to graph- based representations.  

Typically, a GNN would enhance a manually designed solver by replacing various decision heuristics with their predictions after being trained either in a supervised or reinforcement learning mode [1, 11]. Another area of research focuses on end- to- end approaches where the GNN is trained to produce the final answer [29]. From a practical point of view, these end- to- end approaches are interesting because they can potentially find more efficient solutions than those proposed by algorithm designers [31].  

As other data- driven algorithms, GNNs used for combinatorial optimization make a trade- off between performance on some restricted subset of inputs and generalization to the whole domain of possible inputs. In the extreme case, the input distribution may be skewed to the extent that the GNN only needs to recognize superficial features of the input graph.  

In this work, we focus on the end- to- end approaches. We demonstrate these improvements with the popular NeuroSAT architecture [29], which has demonstrated the ability to exhibit nontrivial behavior resembling a search in a continuous space, rather than mere classification based on superficial statistics.  

The NeuroSAT Architecture. We demonstrate our enhancement using the NeuroSAT architecture with several simplifications. NeuroSAT is a GNN that operates on an undirected bipartite graph of literals and clauses. In this graph, each literal is connected to clauses that contain this literal. The MP process alternates between two steps that update the representations of clauses and literals, respectively. The embeddings of literals and clauses are updated by two independent LSTMs. The messages from clause to literals are produced with a 3- layer MLP that takes the embeddings of a clause as an input, and similarly in the opposite direction. After several rounds of MP iterations, the vector representation of each literal is passed into another MLP used to produce a vote for the satisfiability of the formula. These votes are averaged across all literals to produce the final prediction. A more detailed description is provided in the Supplementary material S.2.  

## 3 Curriculum for Training GNNs  

An important feature of the NeuroSAT architecture is that the number of MP iterations does not need to be fixed because each round of MP is realized with the same function. In the original paper, the authors demonstrated that the model trained with 26 MP iterations on problems with up to 40 variables was able to generalize to much larger problems with up to 200 variables. This is achieved just by iterating MP for more steps (hundreds or even thousands). Therefore, we can view the architecture as an iterative algorithm with an adaptive number of steps that could depend on the difficulty of the problem. During training, the number of iterations needs to be fixed so that the problems can be solved in batches, but during inference, each problem can run for a different number of steps.  

As was already shown in the original paper, when the problem is satisfiable and the model correctly predicts it, the vectors of literals form two well- separated clusters. Empirically, once the vectors form two well- separate clusters, subsequent updates do not change the vectors significantly. Informally speaking, MP iterations can be viewed as optimization steps of an implicit energy function of the trained model [14]. Unsatisfied clauses should increase the energy and the minimum energy should be achieved when the maximum number of clauses are satisfied. For satisfiable formulas, this occurs when the vectors form two well- separated clusters, which makes the whole process qualitatively similar to the optimization of the SDP relaxation described in Section 2.3. In the experimental section 5 (and in the Supplementary material S.5), we further verify the connection to SDP by visualizing the evolution of the SDP objective evaluated on the NeuroSAT embeddings after each MP round. Figure 4 shows that this objective function increases until it reaches a fixed point.  

Therefore, we can set up a stopping criterion that stops the MP process once the vectors stop to change significantly. This could be viewed as an analog of a convergence threshold of iterative solvers for continuous problems or BP.  

As mentioned in Section 2.3, the number of iterations required is well correlated with the difficulty of the problem. This motivates our curriculum training procedure, which trains the model by incrementally enlarging the training set with bigger problems and increasing the number of MP operations. For each new problem size, the model is trained until a certain accuracy is reached, and after that, larger problems are added to the training set and the number of MP rounds is incremented accordingly. With this procedure and several simplifications of the original model, we achieve almost an order of magnitude faster convergence to the same accuracy as reported in the original paper (85%).  

A similar observation was recently made by Garg et al. [9] in a study of the in- context learning capabilities of a trained transformer. The authors observe that the trained model is able to perform optimization of a loss function (a high- dimensional regression) as the input passes through individual layers. They also experimentally demonstrated that it is possible to significantly accelerate the emergence of this capability if the model is trained incrementally by increasing the dimensionality of the regression. In our case, we also incrementally increase the number of MP iterations together with the number of variables within the formula, which speeds up the training even further.  

In the experimental section 5 (and in the Supplementary material S.6), we also describe an experiment in which we trained NeuroSAT with an SDP- like loss function instead of the classification loss. The trained model reached a smaller accuracy but was not as dependent on the curriculum as the original model, because the MAX- SAT objective gives more information than the 1- bit supervision of SAT.  

## 4 Sampling and Decimation  

Selsam et al. [29] observe that for formulas that the model correctly classified as satisfiable, the embeddings of literals form two well- separated clusters. In Figure 1 and in Supplementary Figure 6, we recapitulate their visualization of embeddings with UMAP instead of PCA (the final clusters are more distinct when visualized with