Table 1: Improvements obtained due to the sampling and decimation procedure. We test how many satisfiable problems could be solved with the model. SR(40) is the test set with random problems; the other problems are describe in the Supplementary meterial S3.2. For each problem type, we include the average number of variables. For larger problems, we run the model for more MP iterations. When decimation is used, we count the number of problems solved during the first and the second pass separately. #samples refers to the number of different initializations of literal embeddings.   

<table><tr><td>Problem type</td><td>#SAT problems</td><td>Avg. #var</td><td>First pass</td><td>Second pass</td><td>#MP iterations</td><td>#samples</td><td>Decimation</td><td>Solved</td></tr><tr><td rowspan="3">SR(40)</td><td rowspan="3">5000</td><td rowspan="3">40</td><td>4442</td><td>274</td><td rowspan="3">100</td><td>16</td><td>Yes</td><td>94 %</td></tr><tr><td>3990</td><td>-</td><td>1</td><td>No</td><td>80 %</td></tr><tr><td>4457</td><td>-</td><td>32</td><td>No</td><td>89.1 %</td></tr><tr><td rowspan="3">Latin Squares 9x9</td><td rowspan="3">200</td><td rowspan="3">196.9</td><td>186</td><td>14</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>100 %</td></tr><tr><td>95</td><td>-</td><td>1</td><td>No</td><td>47.5 %</td></tr><tr><td>192</td><td>-</td><td>32</td><td>No</td><td>96 %</td></tr><tr><td rowspan="3">Latin Squares 8x8</td><td rowspan="3">200</td><td rowspan="3">133.5</td><td>196</td><td>1</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>98.5 %</td></tr><tr><td>113</td><td>-</td><td>1</td><td>No</td><td>56.5 %</td></tr><tr><td>197</td><td>-</td><td>32</td><td>No</td><td>98.5 %</td></tr><tr><td rowspan="3">Logical Circuits</td><td rowspan="3">344</td><td rowspan="3">131.1</td><td>319</td><td>0</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>92.7 %</td></tr><tr><td>293</td><td>-</td><td>1</td><td>No</td><td>85.2 %</td></tr><tr><td>319</td><td>-</td><td>32</td><td>No</td><td>92.73 %</td></tr><tr><td rowspan="3">Sudoku 9x9</td><td rowspan="3">200</td><td rowspan="3">245.6</td><td>92</td><td>11</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>51.5 %</td></tr><tr><td>35</td><td>-</td><td>1</td><td>No</td><td>17.5 %</td></tr><tr><td>94</td><td>-</td><td>32</td><td>No</td><td>47 %</td></tr></table>  

GNN did not solve the formula, we fix the variables whose distances to the average vectors were below a threshold, simplify the formula, and then process it with the GNN again. For the second pass, we used only one initialization sample for each decimated formula. Therefore, if the first pass uses 16 samples, the second pass can also produce a maximum of 16 samples. To see the effects of decimation, we show results of runs with the same number of samples in total (32) but without decimation.  

We included results with 3 passes and comparison to BP in the Supplementary materials S.7 and S.8. We note, as should be obvious, that our method cannot certify unsatisfiability.  

## 6 Related Work  

Most of the related work was already mentioned in Section 2. In this section, we describe related work in the context of GNNs and Boolean Satisfiability.  

In the domain of Boolean Satisfiability, applications of GNNs can be divided into hybrid or end- to- end approaches. In the hybrid approaches, the GNN is used to guide a discrete search. We can further distinguish between applications where the GNN guides a simple heuristic and applications where the predictions of the GNN are used inside an industrial SAT solver. For the case of heuristics, Yolcu and Poczos [33] use GNNs trained by Reinforcement Learning to select variables and values in a local search. Zhang et al. [35] also use GNN for local search, but train it with supervised learning. For the case of SAT solvers, Kurin et al. [17] introduce a branching heuristic for SAT solvers trained using value- based reinforcement learning (RL) with GNNs for function approximation. They incorporate the heuristic with the MiniSat solver and manage to reduce the number of iterations required to solve SAT problems by 2- 3X.  

Similarly, Wang et al. [32] use GNN as a variable selection heuristic and manage to improve MiniSat in terms of the number of solved problems on the SATCOMP- 2021 competition problem set.  

On the end- to- end front, the most relevant work is the one by Selsam et al. [29] who introduced the NeuroSAT architecture, which was our starting point. Similar to NeuroSAT were the models introduced by Cameron et al. [5] who used different GNN architecture and Shi et al. [30] who used a Transformer. Freivalds and Kozlovics [8] use a Denoising Diffusion model to learn to sample multiple solutions and Ozolins et al. [21] propose an approach in which the GNN can take feedback from solution trials.  

Apart from work focused on Boolean Satisfiability, we also mention the work by Kuck et al. [16] who use GNN to improve Belief Propagation.  

## 7 Conclusion  

We uncover a connection between GNNs trained on combinatorial problems and two well- known approximation algorithms, SDP and BP. Using this connection, we enhance their training and inference procedure. In particular, we focus on the well- known NP- complete problem of Boolean Satisfiability (SAT). We introduce a curriculum training procedure, which enables a significantly faster iteration over experiments. Further, we apply a decimation procedure and initial- value sampling, which significantly increase the number of solved problems. For a problem to be considered solved, we not only require the correct prediction whether it is satisfiable or not, but we also require the GNN to produce a satisfying assignment for satisfiable problem instances.  

Even though the enhancements were presented in the domain of Boolean Satisfiability, we believe that they can easily be generalized to other domains where these approximation algorithms are used.