Like NeuroSAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration.  

## 4 EXPERIMENTAL RESULTS  

Dataset and training details. To train and evaluate the models we use a class of SAT problems \(SR(n)\) introduced and described in detail in [SLB \(^{+18}\) ]. It is parametrized only by \(n\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \(SR(n)\) samples has two labels (see Section 3): sat indicating whether the formula \(\Phi\) is satisfiable and policy indicating for each literal \(l\) whether \(\Phi \wedge l\) is satisfiable. We generate each of those numbers by running MiniSat 2.2 [ES03]. Sample random \(SR(30)\) formulas are solved by MiniSAT 2.2 in 0.007 seconds, while \(SR(110)\) takes 0.137 second and \(SR(150)\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism.  

Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2.   

<table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178±0.672</td><td>0.084±0.004</td><td>0.050±0.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024±0.555</td><td>0.233±0.017</td><td>0.105±0.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010±0.482</td><td>0.266±0.033</td><td>0.110±0.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227±0.127</td><td>0.319±0.007</td><td>0.123±0.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table>  

Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \(SR(n)\) samples. JW- OS proved to be the best on average classes of problems: \(SR(50)\) and \(SR(70)\) , whereas neural guidance- based algorithms proved to be the best on large problems: \(SR(90)\) and \(SR(110)\) .  

<center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \(x\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \(SR(x)\) formulas. The \(y\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center>  

Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \(SR(50)\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \(0.3^{2}\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).