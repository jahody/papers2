<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center>  

Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \(SR(30)\) , level 20 attention degraded the model performance. For \(SR(50)\) , level 40 the metrics with and without attention stayed within the standard deviation of each other.  

Reproducibility. For each set of hyperparameters (e.g. \(SR(30)\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on TensorFlow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access MiniSat through PySAT interface [IMM18].  

<center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center>  

5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.