general problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \(a \wedge \neg a\) ). However, the formulas solved by EqNet have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\(^+\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\(^+\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers.  

## 3 ARCHITECTURE  

We use a message- passing graph- based neural network architecture similar to NeuroSAT introduced in [SLB\(^+\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula.  

<center>Figure 2: Left: A graph representation of formula \((A \vee \neg C \vee B) \wedge (\neg B \vee C)\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \((CCL)\) , connection between literals and their negations \((CLL)\) , literal embeddings from previous iteration \((LE_{t-1})\) , and clause embeddings from previous iteration \((CE_{t-1})\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center>  

We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \(V\) (and a vector \(K\) if needed) based on its embedding, to every connected node. \(V\) and \(K\) are generated with a three- layer MLP with LeakyReLU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with LeakyReLU activation after each hidden layer and sigmoid activation after the last layer.  

We explore two different aggregation methods. The first is the average of received \(V\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \(V\) , we send two vectors, \(V\) and \(K\) . Receiving node generates one vector \(Q\) based on its embedding, and the result of aggregation is \(\sum_{i} V_{i} \text{ sigmoid } (K_{i} \cdot Q)\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \(K\) and \(Q\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\(^+\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.